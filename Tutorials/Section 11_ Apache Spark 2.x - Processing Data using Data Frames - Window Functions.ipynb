{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11: Apache Spark 2.x - Processing Data using Data Frames - Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Window Function - APIs\n",
    "- Problem Statement\n",
    "- Creating Window Spec\n",
    "- Performing Aggregation\n",
    "- Using Windowing Functions\n",
    "- Ranking Functions\n",
    "- Development Life Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames - Window Functions APIs - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Main package pyspark.sql.window \n",
    "* It has classes such as Window and WindowSpec \n",
    "* Window have APIs such as partitionBy, orderBy etc \n",
    "* These APIs (such as partitionBy) return WindowSpec object.\n",
    "* We can pass WindowSpec object to over on functions such as rank(), dense_rank(), sum() etc \n",
    "* Syntax: rank().over(spec) where spec = Window.partitionBy(toLumnNamet) \n",
    "    * Aggregations - sum, avg, min, max etc \n",
    "    * Ranking - rank, dense_rank, row_number etc \n",
    "    * Windowing - Lead, Lag etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+----------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity_id|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+----------------------+-------------------+------------------------+\n",
      "|1            |1                  |957                  |1                     |299.98             |299.98                  |\n",
      "|2            |2                  |1073                 |1                     |199.99             |199.99                  |\n",
      "|3            |2                  |502                  |5                     |250.0              |50.0                    |\n",
      "|4            |2                  |403                  |1                     |129.99             |129.99                  |\n",
      "|5            |4                  |897                  |2                     |49.98              |24.99                   |\n",
      "|6            |4                  |365                  |5                     |299.95             |59.99                   |\n",
      "|7            |4                  |502                  |3                     |150.0              |50.0                    |\n",
      "|8            |4                  |1014                 |4                     |199.92             |49.98                   |\n",
      "|9            |5                  |957                  |1                     |299.98             |299.98                  |\n",
      "|10           |5                  |365                  |5                     |299.95             |59.99                   |\n",
      "+-------------+-------------------+---------------------+----------------------+-------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load order_items\n",
    "orderItems = spark.read.format(\"csv\"). \\\n",
    "            schema(\"order_item_id int,order_item_order_id int,order_item_product_id int,order_item_quantity_id int,order_item_subtotal float,order_item_product_price float\"). \\\n",
    "            load(\"/user/pi/retail_db/order_items\")\n",
    "orderItems.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.window.WindowSpec"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get total oder revenue correspondes to each order item record\n",
    "from pyspark.sql.window import *\n",
    "#step 1 - create window spec object\n",
    "spec = Window.partitionBy(\"order_item_order_id\")\n",
    "type(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------+-------------+\n",
      "|order_item_id|order_item_order_id|order_item_subtotal|order_revenue|\n",
      "+-------------+-------------------+-------------------+-------------+\n",
      "|348          |148                |100.0              |479.99       |\n",
      "|349          |148                |250.0              |479.99       |\n",
      "|350          |148                |129.99             |479.99       |\n",
      "|1129         |463                |239.96             |829.92       |\n",
      "|1130         |463                |250.0              |829.92       |\n",
      "|1131         |463                |39.99              |829.92       |\n",
      "|1132         |463                |299.97             |829.92       |\n",
      "|1153         |471                |39.99              |169.98       |\n",
      "|1154         |471                |129.99             |169.98       |\n",
      "|1223         |496                |59.99              |441.95       |\n",
      "+-------------+-------------------+-------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,round\n",
    "#step2:over function\n",
    "orderItems.withColumn('order_revenue',round(sum('order_item_subtotal').over(spec),2)). \\\n",
    "            select('order_item_id','order_item_order_id','order_item_subtotal','order_revenue'). \\\n",
    "            show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Problem Statement - Get Top N Daily Products\n",
    "* Problem Statement - Get top N products Per day\n",
    "* Get daily product revenue code from the previous topic\n",
    "* Using ranking functions and get the ran associated based on revenue for each day\n",
    "* Once we get rank,let us filter for top n products\n",
    "\n",
    "* use daily product revenue code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders=spark.read.format('csv').schema('order_id int,order_date string,customer_id int,order_status string'). \\\n",
    "                               load('/user/pi/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems = spark.read.format('csv'). \\\n",
    "schema('order_item_id int,order_item_order_id int,order_item_product_id int,order_item_quantity_id int,order_item_subtotal float,order_item_product_price float'). \\\n",
    "load('/user/pi/retail_db/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+--------+\n",
      "|order_date           |order_item_product_id|revenue |\n",
      "+---------------------+---------------------+--------+\n",
      "|2013-07-25 00:00:00.0|1004                 |10799.46|\n",
      "|2013-07-25 00:00:00.0|957                  |9599.36 |\n",
      "|2013-07-25 00:00:00.0|191                  |8499.15 |\n",
      "|2013-07-25 00:00:00.0|365                  |7558.74 |\n",
      "|2013-07-25 00:00:00.0|1073                 |6999.65 |\n",
      "|2013-07-25 00:00:00.0|1014                 |6397.44 |\n",
      "|2013-07-25 00:00:00.0|403                  |5589.57 |\n",
      "|2013-07-25 00:00:00.0|502                  |5100.0  |\n",
      "|2013-07-25 00:00:00.0|627                  |2879.28 |\n",
      "|2013-07-25 00:00:00.0|226                  |599.99  |\n",
      "+---------------------+---------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyProductRevenue = orders.select('order_id','order_date'). \\\n",
    "    join(orderItems,orders.order_id==orderItems.order_item_order_id,'inner'). \\\n",
    "    groupby('order_date','order_item_product_id'). \\\n",
    "    agg(round(sum('order_item_subtotal'),2).alias('revenue')). \\\n",
    "    sort(['order_date','revenue'],ascending=[1,0])\n",
    "dailyProductRevenue.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Operations - Creating Window Spec\n",
    "       * get daily order count part of each order record\n",
    "       \n",
    "***\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spec = Window.partitionBy('ColumnName1').orderBy('ColumnName2')\n",
    "\n",
    "***\n",
    "*For aggregate functions no need to mention orderBy*\n",
    "\n",
    "*Spec created can only be used over dataframe which has columns mentioned in Window Spec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+---------------+\n",
      "|order_id|order_date           |customer_id|order_status   |\n",
      "+--------+---------------------+-----------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599      |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256        |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111      |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827       |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318      |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130       |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530       |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911       |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657       |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648       |PENDING_PAYMENT|\n",
      "+--------+---------------------+-----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get daily order count part of each order record\n",
    "orders = spark.read.\\\n",
    "         csv(\"/user/pi/retail_db/orders\", \\\n",
    "         schema=\"order_id int,order_date string,customer_id int,order_status string\")\n",
    "orders.show(10,False)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+---------------+------------+\n",
      "|order_id|order_date           |customer_id|order_status   |daily_orders|\n",
      "+--------+---------------------+-----------+---------------+------------+\n",
      "|3378    |2013-08-13 00:00:00.0|3155       |PROCESSING     |73          |\n",
      "|3379    |2013-08-13 00:00:00.0|5437       |COMPLETE       |73          |\n",
      "|3380    |2013-08-13 00:00:00.0|3519       |CLOSED         |73          |\n",
      "|3381    |2013-08-13 00:00:00.0|10023      |ON_HOLD        |73          |\n",
      "|3382    |2013-08-13 00:00:00.0|7856       |PENDING        |73          |\n",
      "|3383    |2013-08-13 00:00:00.0|1523       |PENDING_PAYMENT|73          |\n",
      "|3384    |2013-08-13 00:00:00.0|12398      |PENDING_PAYMENT|73          |\n",
      "|3385    |2013-08-13 00:00:00.0|132        |COMPLETE       |73          |\n",
      "|3386    |2013-08-13 00:00:00.0|2128       |PENDING        |73          |\n",
      "|3387    |2013-08-13 00:00:00.0|2735       |COMPLETE       |73          |\n",
      "|3388    |2013-08-13 00:00:00.0|12319      |COMPLETE       |73          |\n",
      "|3389    |2013-08-13 00:00:00.0|7024       |COMPLETE       |73          |\n",
      "|3390    |2013-08-13 00:00:00.0|5012       |CLOSED         |73          |\n",
      "|3391    |2013-08-13 00:00:00.0|11076      |PROCESSING     |73          |\n",
      "|3392    |2013-08-13 00:00:00.0|8989       |CLOSED         |73          |\n",
      "|3393    |2013-08-13 00:00:00.0|11247      |COMPLETE       |73          |\n",
      "|3394    |2013-08-13 00:00:00.0|890        |PENDING_PAYMENT|73          |\n",
      "|3395    |2013-08-13 00:00:00.0|1618       |PENDING        |73          |\n",
      "|3396    |2013-08-13 00:00:00.0|9491       |PENDING_PAYMENT|73          |\n",
      "|3397    |2013-08-13 00:00:00.0|6722       |PENDING_PAYMENT|73          |\n",
      "|3398    |2013-08-13 00:00:00.0|5212       |COMPLETE       |73          |\n",
      "|3399    |2013-08-13 00:00:00.0|7704       |COMPLETE       |73          |\n",
      "|3400    |2013-08-13 00:00:00.0|8529       |PROCESSING     |73          |\n",
      "|3401    |2013-08-13 00:00:00.0|3038       |COMPLETE       |73          |\n",
      "|3402    |2013-08-13 00:00:00.0|9735       |COMPLETE       |73          |\n",
      "|3403    |2013-08-13 00:00:00.0|6397       |PENDING        |73          |\n",
      "|3404    |2013-08-13 00:00:00.0|6377       |COMPLETE       |73          |\n",
      "|3405    |2013-08-13 00:00:00.0|11236      |COMPLETE       |73          |\n",
      "|3406    |2013-08-13 00:00:00.0|4844       |PROCESSING     |73          |\n",
      "|3407    |2013-08-13 00:00:00.0|6947       |PROCESSING     |73          |\n",
      "|3408    |2013-08-13 00:00:00.0|11510      |COMPLETE       |73          |\n",
      "|3409    |2013-08-13 00:00:00.0|10050      |COMPLETE       |73          |\n",
      "|3410    |2013-08-13 00:00:00.0|9376       |PROCESSING     |73          |\n",
      "|3411    |2013-08-13 00:00:00.0|7683       |COMPLETE       |73          |\n",
      "|3412    |2013-08-13 00:00:00.0|1137       |COMPLETE       |73          |\n",
      "|3413    |2013-08-13 00:00:00.0|214        |CANCELED       |73          |\n",
      "|3414    |2013-08-13 00:00:00.0|4861       |PENDING        |73          |\n",
      "|3415    |2013-08-13 00:00:00.0|7333       |PENDING        |73          |\n",
      "|3416    |2013-08-13 00:00:00.0|9838       |PENDING_PAYMENT|73          |\n",
      "|3417    |2013-08-13 00:00:00.0|3354       |COMPLETE       |73          |\n",
      "|3418    |2013-08-13 00:00:00.0|10568      |CLOSED         |73          |\n",
      "|3419    |2013-08-13 00:00:00.0|3950       |CLOSED         |73          |\n",
      "|3420    |2013-08-13 00:00:00.0|10873      |ON_HOLD        |73          |\n",
      "|3421    |2013-08-13 00:00:00.0|4008       |COMPLETE       |73          |\n",
      "|3422    |2013-08-13 00:00:00.0|705        |COMPLETE       |73          |\n",
      "|3423    |2013-08-13 00:00:00.0|6999       |PENDING_PAYMENT|73          |\n",
      "|3424    |2013-08-13 00:00:00.0|8710       |PROCESSING     |73          |\n",
      "|3425    |2013-08-13 00:00:00.0|9580       |PENDING_PAYMENT|73          |\n",
      "|3426    |2013-08-13 00:00:00.0|703        |PENDING        |73          |\n",
      "|3427    |2013-08-13 00:00:00.0|11781      |PENDING_PAYMENT|73          |\n",
      "|3428    |2013-08-13 00:00:00.0|9687       |PROCESSING     |73          |\n",
      "|3429    |2013-08-13 00:00:00.0|5159       |COMPLETE       |73          |\n",
      "|3430    |2013-08-13 00:00:00.0|3896       |PENDING_PAYMENT|73          |\n",
      "|3431    |2013-08-13 00:00:00.0|11014      |CLOSED         |73          |\n",
      "|3432    |2013-08-13 00:00:00.0|9338       |PENDING_PAYMENT|73          |\n",
      "|3433    |2013-08-13 00:00:00.0|11929      |COMPLETE       |73          |\n",
      "|3434    |2013-08-13 00:00:00.0|9454       |PROCESSING     |73          |\n",
      "|3435    |2013-08-13 00:00:00.0|2467       |CLOSED         |73          |\n",
      "|3436    |2013-08-13 00:00:00.0|4758       |PROCESSING     |73          |\n",
      "|3437    |2013-08-13 00:00:00.0|10837      |COMPLETE       |73          |\n",
      "|3438    |2013-08-13 00:00:00.0|9387       |COMPLETE       |73          |\n",
      "|58239   |2013-08-13 00:00:00.0|10166      |PENDING_PAYMENT|73          |\n",
      "|58240   |2013-08-13 00:00:00.0|7646       |COMPLETE       |73          |\n",
      "|58241   |2013-08-13 00:00:00.0|355        |COMPLETE       |73          |\n",
      "|58242   |2013-08-13 00:00:00.0|1352       |PROCESSING     |73          |\n",
      "|58243   |2013-08-13 00:00:00.0|4081       |COMPLETE       |73          |\n",
      "|58244   |2013-08-13 00:00:00.0|2316       |CLOSED         |73          |\n",
      "|58245   |2013-08-13 00:00:00.0|1489       |PENDING        |73          |\n",
      "|58246   |2013-08-13 00:00:00.0|5629       |COMPLETE       |73          |\n",
      "|58247   |2013-08-13 00:00:00.0|254        |PENDING        |73          |\n",
      "|58248   |2013-08-13 00:00:00.0|2889       |PROCESSING     |73          |\n",
      "|58249   |2013-08-13 00:00:00.0|2127       |COMPLETE       |73          |\n",
      "|67487   |2013-08-13 00:00:00.0|6874       |PENDING        |73          |\n",
      "|12827   |2013-10-12 00:00:00.0|542        |PENDING_PAYMENT|162         |\n",
      "|12828   |2013-10-12 00:00:00.0|9          |COMPLETE       |162         |\n",
      "|12829   |2013-10-12 00:00:00.0|12230      |COMPLETE       |162         |\n",
      "|12830   |2013-10-12 00:00:00.0|11455      |PENDING        |162         |\n",
      "|12831   |2013-10-12 00:00:00.0|3511       |COMPLETE       |162         |\n",
      "|12832   |2013-10-12 00:00:00.0|5217       |PENDING_PAYMENT|162         |\n",
      "|12833   |2013-10-12 00:00:00.0|1102       |PENDING_PAYMENT|162         |\n",
      "|12834   |2013-10-12 00:00:00.0|7282       |PROCESSING     |162         |\n",
      "|12835   |2013-10-12 00:00:00.0|7891       |CLOSED         |162         |\n",
      "|12836   |2013-10-12 00:00:00.0|9040       |COMPLETE       |162         |\n",
      "|12837   |2013-10-12 00:00:00.0|6581       |PENDING_PAYMENT|162         |\n",
      "|12838   |2013-10-12 00:00:00.0|8597       |COMPLETE       |162         |\n",
      "|12839   |2013-10-12 00:00:00.0|6196       |CLOSED         |162         |\n",
      "|12840   |2013-10-12 00:00:00.0|10653      |PENDING        |162         |\n",
      "|12841   |2013-10-12 00:00:00.0|860        |ON_HOLD        |162         |\n",
      "|12842   |2013-10-12 00:00:00.0|5364       |COMPLETE       |162         |\n",
      "|12843   |2013-10-12 00:00:00.0|7198       |PENDING_PAYMENT|162         |\n",
      "|12844   |2013-10-12 00:00:00.0|11139      |PENDING_PAYMENT|162         |\n",
      "|12845   |2013-10-12 00:00:00.0|7576       |PENDING_PAYMENT|162         |\n",
      "|12846   |2013-10-12 00:00:00.0|4965       |COMPLETE       |162         |\n",
      "|12847   |2013-10-12 00:00:00.0|3998       |PENDING_PAYMENT|162         |\n",
      "|12848   |2013-10-12 00:00:00.0|8305       |PENDING_PAYMENT|162         |\n",
      "|12849   |2013-10-12 00:00:00.0|6847       |COMPLETE       |162         |\n",
      "|12850   |2013-10-12 00:00:00.0|6985       |PENDING        |162         |\n",
      "|12851   |2013-10-12 00:00:00.0|1335       |COMPLETE       |162         |\n",
      "|12852   |2013-10-12 00:00:00.0|5402       |ON_HOLD        |162         |\n",
      "|12853   |2013-10-12 00:00:00.0|3825       |COMPLETE       |162         |\n",
      "+--------+---------------------+-----------+---------------+------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "spec = Window.partitionBy('order_date')\n",
    "orders. \\\n",
    "    withColumn('daily_orders',count('order_id').over(spec)). \\\n",
    "    show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Operations - Performing Aggregations using sum, avg etc\n",
    "*For aggregate functions no need to mention orderBy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get daily product revenue - get average revenue,minimum and maximum revenue\n",
    "#col function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+--------+\n",
      "|order_date           |order_item_product_id|revenue |\n",
      "+---------------------+---------------------+--------+\n",
      "|2013-07-25 00:00:00.0|1004                 |10799.46|\n",
      "|2013-07-25 00:00:00.0|957                  |9599.36 |\n",
      "|2013-07-25 00:00:00.0|191                  |8499.15 |\n",
      "|2013-07-25 00:00:00.0|365                  |7558.74 |\n",
      "|2013-07-25 00:00:00.0|1073                 |6999.65 |\n",
      "|2013-07-25 00:00:00.0|1014                 |6397.44 |\n",
      "|2013-07-25 00:00:00.0|403                  |5589.57 |\n",
      "|2013-07-25 00:00:00.0|502                  |5100.0  |\n",
      "|2013-07-25 00:00:00.0|627                  |2879.28 |\n",
      "|2013-07-25 00:00:00.0|226                  |599.99  |\n",
      "+---------------------+---------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyProductRevenue.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+------------+-----------------+---------------+---------------+---------------+\n",
      "|order_date           |revenue |DailyRevenue|PercentageRevenue|DailyAvgRevenue|DailyMinRevenue|DailyMaxRevenue|\n",
      "+---------------------+--------+------------+-----------------+---------------+---------------+---------------+\n",
      "|2013-07-25 00:00:00.0|599.99  |68153.83    |0.88             |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|6397.44 |68153.83    |9.39             |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|2879.28 |68153.83    |4.22             |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|8499.15 |68153.83    |12.47            |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|6999.65 |68153.83    |10.27            |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|5589.57 |68153.83    |8.2              |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|5100.0  |68153.83    |7.48             |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|10799.46|68153.83    |15.85            |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|9599.36 |68153.83    |14.08            |1842.0         |19.98          |10799.46       |\n",
      "|2013-07-25 00:00:00.0|7558.74 |68153.83    |11.09            |1842.0         |19.98          |10799.46       |\n",
      "+---------------------+--------+------------+-----------------+---------------+---------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count,avg,min,max,sum,round\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "spec = Window.partitionBy('order_date')\n",
    "dailyProductRevenue. \\\n",
    "    select('order_date','revenue'). \\\n",
    "    withColumn('DailyRevenue',round(sum('revenue').over(spec),2)). \\\n",
    "    withColumn('PercentageRevenue',round((dailyProductRevenue.revenue/col('DailyRevenue'))*100,2)). \\\n",
    "    withColumn('DailyAvgRevenue',round(avg('revenue').over(spec),2)). \\\n",
    "    withColumn('DailyMinRevenue',min('revenue').over(spec)). \\\n",
    "    withColumn('DailyMaxRevenue',max('revenue').over(spec)). \\\n",
    "    sort('order_date','DailyRevenue'). \\\n",
    "    show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col\n",
    "#col function is used when we need to refer analytic column part of same withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Operations - Time Series Functions such as Lead, Lag etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------------------+-----+---------------+--------------------+--------------------+\n",
      "|ANNUAL_RT|DEPTID|               DESCR|EMPID|        HIRE_DT|            JOBTITLE|                NAME|\n",
      "+---------+------+--------------------+-----+---------------+--------------------+--------------------+\n",
      "|    32470|A50550|DPW-Water & Waste...| 1001| 8/27/2018 0:00|Utilities Inst Re...|      Aaron Kareem D|\n",
      "|    60200|A03031|OED-Employment De...| 1002|10/24/1979 0:00|Facilities/Office...|    Aaron Patricia G|\n",
      "|    64823|A02002|  City Council (002)| 1003|12/12/2016 0:00|  Council Technician|       Abadir Adam O|\n",
      "|    53640|A99094|Police Department...| 1004| 4/17/2018 0:00|      Police Officer|Abaku Aigbolosimu...|\n",
      "|    68562|A29011|States Attorneys ...| 1005| 5/22/2017 0:00|Assistant State's...|       Abbeduto Mack|\n",
      "|    33280|A68002|     R&P-Parks (002)| 1006| 4/11/2018 0:00|Recreation Arts I...|      Abbott Ethan N|\n",
      "|    75110|A90005| TRANS-Traffic (005)| 1007|11/28/2014 0:00|Operations Office...|Abbott-Cole Michelle|\n",
      "|    69595|A64120|Fire Department (...| 1008| 3/30/2011 0:00|Fire Pump Operato...|  Abdal-Rahim Naim A|\n",
      "|    93284|A99160|Police Department...| 1009| 6/14/2007 0:00|     Police Sergeant|      Abdi Ezekiel W|\n",
      "|    50079|A38410|Sheriff's Office ...| 1010|  9/2/1999 0:00|Radio Dispatcher ...| Abdul Adl Attrice A|\n",
      "|    28554|P04002|R&P-Recreation (p...| 1011|  6/1/2017 0:00|Swimming Pool Ope...|   Abdul Aziz Hajr E|\n",
      "|    28554|P04002|R&P-Recreation (p...| 1012|  6/1/2017 0:00|Swimming Pool Ope...|  Abdul Aziz Yaqub M|\n",
      "|    57857|A99393|Police Department...| 1013| 4/13/1998 0:00|           Paralegal| Abdul Saboor Dana N|\n",
      "|    64505|A50101|DPW-Water & Waste...| 1014| 7/17/2017 0:00|          Engineer I|         Abdul Jalil|\n",
      "|    46395|A65028|HLTH-Health Depar...| 1015| 4/14/2008 0:00|Social Service Co...|Abdul-Jabbar Bush...|\n",
      "|    32131|A04005|R&P-Recreation (005)| 1016|  6/6/2019 0:00|Recreation Leader II|  Abdul-Khaliq Amahl|\n",
      "|    41757|A06004|Housing & Communi...| 1017| 12/1/1986 0:00|Office Support Sp...|  Abdullah Beverly A|\n",
      "|    56322|A64604|Fire Department (...| 1018| 6/10/2004 0:00|        911 Operator|  Abdullahi Sharon M|\n",
      "|    78000|A85001|General Services ...| 1019|  5/9/2019 0:00|Supt of Public Bl...|Abdullateef Muham...|\n",
      "|    53512|A99416|Police Department...| 1020|12/28/2018 0:00|Police Officer Tr...|Abdulrahman Musta...|\n",
      "+---------+------+--------------------+-----+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read employee data\n",
    "from pyspark.sql import Row\n",
    "employeeRaw = open('/home/pi/shared/datasets/employee.csv').read().splitlines()\n",
    "employeeRdd = sc.parallelize(employeeRaw)\n",
    "employee = employeeRdd.filter(lambda rw:rw.split(',')[0]!='EMPID'). \\\n",
    "        map(lambda r:Row(EMPID=r.split(',')[0],NAME=r.split(',')[1],JOBTITLE=r.split(',')[2],DEPTID=r.split(',')[3], \\\n",
    "                         DESCR=r.split(',')[4],HIRE_DT=r.split(',')[5],ANNUAL_RT=r.split(',')[6])).toDF()\n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.write.csv('/user/pi/employee_data/employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-05 20:03:25,556 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 17 items\n",
      "-rw-r--r--   2 pi supergroup          0 2020-06-05 20:03 /user/pi/employee_data/employee/_SUCCESS\n",
      "-rw-r--r--   2 pi supergroup      83031 2020-06-05 20:03 /user/pi/employee_data/employee/part-00000-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      82511 2020-06-05 20:03 /user/pi/employee_data/employee/part-00001-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83589 2020-06-05 20:03 /user/pi/employee_data/employee/part-00002-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      82781 2020-06-05 20:03 /user/pi/employee_data/employee/part-00003-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83280 2020-06-05 20:03 /user/pi/employee_data/employee/part-00004-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      82406 2020-06-05 20:03 /user/pi/employee_data/employee/part-00005-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      82991 2020-06-05 20:03 /user/pi/employee_data/employee/part-00006-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      82686 2020-06-05 20:03 /user/pi/employee_data/employee/part-00007-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      82138 2020-06-05 20:03 /user/pi/employee_data/employee/part-00008-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83463 2020-06-05 20:03 /user/pi/employee_data/employee/part-00009-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83008 2020-06-05 20:03 /user/pi/employee_data/employee/part-00010-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83593 2020-06-05 20:03 /user/pi/employee_data/employee/part-00011-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83671 2020-06-05 20:03 /user/pi/employee_data/employee/part-00012-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83695 2020-06-05 20:03 /user/pi/employee_data/employee/part-00013-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      83697 2020-06-05 20:03 /user/pi/employee_data/employee/part-00014-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n",
      "-rw-r--r--   2 pi supergroup      84063 2020-06-05 20:03 /user/pi/employee_data/employee/part-00015-d4884824-842e-4853-af5d-08abe7308656-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/pi/employee_data/employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+-----+---------------+--------------------+-----------------+\n",
      "|  SALARY|DEPTID|               DESCR|EMPID|        HIRE_DT|            JOBTITLE|             NAME|\n",
      "+--------+------+--------------------+-----+---------------+--------------------+-----------------+\n",
      "| 38926.0|A29009|States Attorneys ...|13945| 5/20/2019 0:00|       Law Clerk SAO|    Webb Edward J|\n",
      "| 39102.0|A02002|  City Council (002)|13946|  1/4/2017 0:00|   Council Assistant|   Webb Frances M|\n",
      "|131438.0|A99190|Police Department...|13947|  5/8/1992 0:00|        Police Major|      Webb John O|\n",
      "| 33680.0|A68009|     R&P-Parks (009)|13948|  4/8/2004 0:00|        Utility Aide|      Webb Kern A|\n",
      "| 36202.0|A49507|TRANS-Highways (507)|13949|  7/1/1991 0:00|      Laborer Hourly|   Webb Michael E|\n",
      "| 38002.0|A65116|HLTH-Health Dept....|13950| 6/25/2008 0:00|Medical Office As...|  Webb Rochelle M|\n",
      "| 33192.0|A65016|HLTH-Health Depar...|13951| 11/7/1994 0:00|Medical Office As...|  Webb Rochelle M|\n",
      "| 86000.0|A30001|Law Department (001)|13952| 2/10/2014 0:00| Assistant Solicitor|    Webb Thomas P|\n",
      "| 11303.0|C90786|TRANS-Crossing Gu...|13953|  5/4/2006 0:00|      Crossing Guard|  Webber Stella M|\n",
      "| 23941.0|C90SUM|TRANS-Cross Guard...|13954| 6/25/2007 0:00|      Crossing Guard|  Webber Stella M|\n",
      "| 63065.0|A29002|States Attorneys ...|13955|  4/5/1999 0:00|    Investigator SAO|     Weber John D|\n",
      "| 51479.0|A85302|General Services ...|13956|10/22/1979 0:00| Automotive Mechanic|  Weber Michael J|\n",
      "| 77500.0|A65004|HLTH-Health Depar...|13957| 9/20/2018 0:00|City Planner Supe...|   Webster Lara K|\n",
      "| 57040.0|A26001|M-R Labor Commiss...|13958| 11/8/1993 0:00|     ADM Coordinator|Webster Lavetta Y|\n",
      "| 37401.0|A75060|Enoch Pratt Free ...|13959| 2/27/2006 0:00|Office Assistant III|  Webster Merri T|\n",
      "| 43285.0|A75082|Enoch Pratt Free ...|13960| 7/31/2006 0:00|Library Security ...|Wedington Lennard|\n",
      "| 37435.0|B70412|DPW-Solid Waste (...|13961| 3/29/1993 0:00|  Solid Waste Worker|  Weeks Jr Rollin|\n",
      "| 33441.0|A70415|DPW-Solid Waste (...|13962| 5/13/2010 0:00|  Solid Waste Worker|     Weeks Alan M|\n",
      "| 39101.0|A12002|FIN-Acct & Payrol...|13963| 11/6/2017 0:00|Accounting Assist...|  Weeks Carolyn D|\n",
      "| 29162.0|A65099|HLTH-Health Depar...|13964| 8/21/2006 0:00|School Health Aid...|    Weeks Latrice|\n",
      "+--------+------+--------------------+-----+---------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee = spark.read.csv(\"/user/pi/employee_data/employee\", \\\n",
    "                          schema='SALARY float,DEPTID string,DESCR string,EMPID int,HIRE_DT string,JOBTITLE string,NAME string')\n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spec = Window.partitionBy('DEPTID'). \\\n",
    "            orderBy(employee.SALARY.desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----------+-----------+----------+\n",
      "|EMPID|DEPTID|SALARY |NextLowSal|NextHighSal|HighestSal|\n",
      "+-----+------+-------+----------+-----------+----------+\n",
      "|1067 |A30003|83856.0|67830.0   |null       |83856.0   |\n",
      "|3976 |A30003|67830.0|65000.0   |83856.0    |67830.0   |\n",
      "|5098 |A30003|65000.0|50927.0   |67830.0    |65000.0   |\n",
      "|3030 |A30003|50927.0|44061.0   |65000.0    |50927.0   |\n",
      "|3720 |A30003|44061.0|null      |50927.0    |44061.0   |\n",
      "|2505 |A64466|94303.0|84784.0   |null       |94303.0   |\n",
      "|6661 |A64466|84784.0|84177.0   |94303.0    |84784.0   |\n",
      "|14785|A64466|84177.0|83338.0   |84784.0    |84177.0   |\n",
      "|7280 |A64466|83338.0|78721.0   |84177.0    |83338.0   |\n",
      "|8125 |A64466|78721.0|78721.0   |83338.0    |78721.0   |\n",
      "+-----+------+-------+----------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead,lag,first,last\n",
    "employee.select('EMPID','DEPTID','SALARY'). \\\n",
    "    withColumn('NextLowSal',lead('SALARY',1).over(spec)). \\\n",
    "    withColumn('NextHighSal',lag('SALARY',1).over(spec)). \\\n",
    "    withColumn('HighestSal',first('SALARY').over(spec)). \\\n",
    "    withColumn('HighestSal',last('SALARY').over(spec)). \\\n",
    "    show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - As per above result last() is not giving expected result\n",
    " - So we need to modify Window spec to add rangeBetween()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Window in module pyspark.sql.window:\n",
      "\n",
      "class Window(builtins.object)\n",
      " |  Utility functions for defining window in DataFrames.\n",
      " |  \n",
      " |  For example:\n",
      " |  \n",
      " |  >>> # ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      " |  >>> window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |  \n",
      " |  >>> # PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING\n",
      " |  >>> window = Window.orderBy(\"date\").partitionBy(\"country\").rangeBetween(-3, 3)\n",
      " |  \n",
      " |  .. note:: When ordering is not defined, an unbounded window frame (rowFrame,\n",
      " |       unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined,\n",
      " |       a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.\n",
      " |  \n",
      " |  .. note:: Experimental\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  orderBy(*cols)\n",
      " |      Creates a :class:`WindowSpec` with the ordering defined.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  partitionBy(*cols)\n",
      " |      Creates a :class:`WindowSpec` with the partitioning defined.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rangeBetween(start, end)\n",
      " |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      " |      from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative from the current row. For example,\n",
      " |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      " |      and \"5\" means the five off after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      :param start: boundary start, inclusive.\n",
      " |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |                    any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      " |      :param end: boundary end, inclusive.\n",
      " |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |                  any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  rowsBetween(start, end)\n",
      " |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      " |      from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative positions from the current row.\n",
      " |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      " |      the current row, and \"5\" means the fifth row after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      :param start: boundary start, inclusive.\n",
      " |                    The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |                    any value less than or equal to -9223372036854775808.\n",
      " |      :param end: boundary end, inclusive.\n",
      " |                  The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |                  any value greater than or equal to 9223372036854775807.\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  currentRow = 0\n",
      " |  \n",
      " |  unboundedFollowing = 9223372036854775807\n",
      " |  \n",
      " |  unboundedPreceding = -9223372036854775808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spec = Window.partitionBy('DEPTID'). \\\n",
    "            orderBy(employee.SALARY.desc()). \\\n",
    "            rangeBetween(Window.unboundedPreceding,Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+---------+\n",
      "|EMPID|DEPTID|SALARY |LowestSal|\n",
      "+-----+------+-------+---------+\n",
      "|1067 |A30003|83856.0|44061.0  |\n",
      "|3976 |A30003|67830.0|44061.0  |\n",
      "|5098 |A30003|65000.0|44061.0  |\n",
      "|3030 |A30003|50927.0|44061.0  |\n",
      "|3720 |A30003|44061.0|44061.0  |\n",
      "|2505 |A64466|94303.0|34298.0  |\n",
      "|6661 |A64466|84784.0|34298.0  |\n",
      "|14785|A64466|84177.0|34298.0  |\n",
      "|7280 |A64466|83338.0|34298.0  |\n",
      "|8125 |A64466|78721.0|34298.0  |\n",
      "+-----+------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead,lag,first,last\n",
    "employee.select('EMPID','DEPTID','SALARY'). \\\n",
    "    withColumn('LowestSal',last('SALARY',False).over(spec)). \\\n",
    "    show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Operations - Ranking Functions - rank,dense_rank, row_number etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+-----+---------------+--------------------+-----------------+\n",
      "|  SALARY|DEPTID|               DESCR|EMPID|        HIRE_DT|            JOBTITLE|             NAME|\n",
      "+--------+------+--------------------+-----+---------------+--------------------+-----------------+\n",
      "| 38926.0|A29009|States Attorneys ...|13945| 5/20/2019 0:00|       Law Clerk SAO|    Webb Edward J|\n",
      "| 39102.0|A02002|  City Council (002)|13946|  1/4/2017 0:00|   Council Assistant|   Webb Frances M|\n",
      "|131438.0|A99190|Police Department...|13947|  5/8/1992 0:00|        Police Major|      Webb John O|\n",
      "| 33680.0|A68009|     R&P-Parks (009)|13948|  4/8/2004 0:00|        Utility Aide|      Webb Kern A|\n",
      "| 36202.0|A49507|TRANS-Highways (507)|13949|  7/1/1991 0:00|      Laborer Hourly|   Webb Michael E|\n",
      "| 38002.0|A65116|HLTH-Health Dept....|13950| 6/25/2008 0:00|Medical Office As...|  Webb Rochelle M|\n",
      "| 33192.0|A65016|HLTH-Health Depar...|13951| 11/7/1994 0:00|Medical Office As...|  Webb Rochelle M|\n",
      "| 86000.0|A30001|Law Department (001)|13952| 2/10/2014 0:00| Assistant Solicitor|    Webb Thomas P|\n",
      "| 11303.0|C90786|TRANS-Crossing Gu...|13953|  5/4/2006 0:00|      Crossing Guard|  Webber Stella M|\n",
      "| 23941.0|C90SUM|TRANS-Cross Guard...|13954| 6/25/2007 0:00|      Crossing Guard|  Webber Stella M|\n",
      "| 63065.0|A29002|States Attorneys ...|13955|  4/5/1999 0:00|    Investigator SAO|     Weber John D|\n",
      "| 51479.0|A85302|General Services ...|13956|10/22/1979 0:00| Automotive Mechanic|  Weber Michael J|\n",
      "| 77500.0|A65004|HLTH-Health Depar...|13957| 9/20/2018 0:00|City Planner Supe...|   Webster Lara K|\n",
      "| 57040.0|A26001|M-R Labor Commiss...|13958| 11/8/1993 0:00|     ADM Coordinator|Webster Lavetta Y|\n",
      "| 37401.0|A75060|Enoch Pratt Free ...|13959| 2/27/2006 0:00|Office Assistant III|  Webster Merri T|\n",
      "| 43285.0|A75082|Enoch Pratt Free ...|13960| 7/31/2006 0:00|Library Security ...|Wedington Lennard|\n",
      "| 37435.0|B70412|DPW-Solid Waste (...|13961| 3/29/1993 0:00|  Solid Waste Worker|  Weeks Jr Rollin|\n",
      "| 33441.0|A70415|DPW-Solid Waste (...|13962| 5/13/2010 0:00|  Solid Waste Worker|     Weeks Alan M|\n",
      "| 39101.0|A12002|FIN-Acct & Payrol...|13963| 11/6/2017 0:00|Accounting Assist...|  Weeks Carolyn D|\n",
      "| 29162.0|A65099|HLTH-Health Depar...|13964| 8/21/2006 0:00|School Health Aid...|    Weeks Latrice|\n",
      "+--------+------+--------------------+-----+---------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assign rank to employees based on salary within in each department\n",
    "employee = spark.read.csv(\"/user/pi/employee_data/employee\", \\\n",
    "                          schema='SALARY float,DEPTID string,DESCR string,EMPID int,HIRE_DT string,JOBTITLE string,NAME string')\n",
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----+---------+---------+\n",
      "|EMPID|DEPTID| SALARY|Rank|DenseRank|RowNumber|\n",
      "+-----+------+-------+----+---------+---------+\n",
      "| 1067|A30003|83856.0|   1|        1|        1|\n",
      "| 3976|A30003|67830.0|   2|        2|        2|\n",
      "| 5098|A30003|65000.0|   3|        3|        3|\n",
      "| 3030|A30003|50927.0|   4|        4|        4|\n",
      "| 3720|A30003|44061.0|   5|        5|        5|\n",
      "| 2505|A64466|94303.0|   1|        1|        1|\n",
      "| 6661|A64466|84784.0|   2|        2|        2|\n",
      "|14785|A64466|84177.0|   3|        3|        3|\n",
      "| 7280|A64466|83338.0|   4|        4|        4|\n",
      "| 7122|A64466|78721.0|   5|        5|        5|\n",
      "| 8125|A64466|78721.0|   5|        5|        6|\n",
      "|14567|A64466|78265.0|   7|        6|        7|\n",
      "| 3539|A64466|78265.0|   7|        6|        8|\n",
      "|10842|A64466|75054.0|   9|        7|        9|\n",
      "| 1573|A64466|75054.0|   9|        7|       10|\n",
      "| 7685|A64466|75054.0|   9|        7|       11|\n",
      "| 8542|A64466|75054.0|   9|        7|       12|\n",
      "|13504|A64466|75054.0|   9|        7|       13|\n",
      "|12212|A64466|72622.0|  14|        8|       14|\n",
      "|11247|A64466|72622.0|  14|        8|       15|\n",
      "+-----+------+-------+----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,dense_rank,row_number\n",
    "spec = Window.partitionBy('DEPTID').orderBy(employee.SALARY.desc())\n",
    "\n",
    "employee.select('EMPID','DEPTID','SALARY'). \\\n",
    "        withColumn('Rank',rank().over(spec)). \\\n",
    "        withColumn('DenseRank',dense_rank().over(spec)). \\\n",
    "        withColumn('RowNumber',row_number().over(spec)). \\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top N=5 purchased products Per day\n",
    "orders=spark.read.format('csv').schema('order_id int,order_date string,customer_id int,order_status string'). \\\n",
    "                               load('/user/pi/retail_db/orders')\n",
    "#load order_items\n",
    "orderItems = spark.read.format(\"csv\"). \\\n",
    "            schema(\"order_item_id int,order_item_order_id int,order_item_product_id int,order_item_quantity_id int,order_item_subtotal float,order_item_product_price float\"). \\\n",
    "            load(\"/user/pi/retail_db/order_items\")\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "dailyProducts = orders.select('order_id','order_date').join(orderItems,orders.order_id==orderItems.order_item_order_id). \\\n",
    "                        groupBy('order_date','order_item_product_id'). \\\n",
    "                        agg(count('order_item_product_id').alias(\"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---+---------------+\n",
      "|          order_date|order_item_product_id|cnt|ProductFreqRank|\n",
      "+--------------------+---------------------+---+---------------+\n",
      "|2013-08-13 00:00:...|                  365| 33|              1|\n",
      "|2013-08-13 00:00:...|                  403| 32|              2|\n",
      "|2013-08-13 00:00:...|                  502| 29|              3|\n",
      "|2013-08-13 00:00:...|                 1073| 20|              4|\n",
      "|2013-08-13 00:00:...|                  957| 18|              5|\n",
      "|2013-10-12 00:00:...|                  502| 58|              1|\n",
      "|2013-10-12 00:00:...|                  403| 54|              2|\n",
      "|2013-10-12 00:00:...|                  365| 50|              3|\n",
      "|2013-10-12 00:00:...|                 1014| 43|              4|\n",
      "|2013-10-12 00:00:...|                 1073| 41|              5|\n",
      "|2013-11-15 00:00:...|                  403| 51|              1|\n",
      "|2013-11-15 00:00:...|                  502| 48|              2|\n",
      "|2013-11-15 00:00:...|                  365| 45|              3|\n",
      "|2013-11-15 00:00:...|                 1014| 38|              4|\n",
      "|2013-11-15 00:00:...|                 1004| 32|              5|\n",
      "|2014-03-19 00:00:...|                  365| 56|              1|\n",
      "|2014-03-19 00:00:...|                  403| 44|              2|\n",
      "|2014-03-19 00:00:...|                 1014| 43|              3|\n",
      "|2014-03-19 00:00:...|                  502| 39|              4|\n",
      "|2014-03-19 00:00:...|                  191| 27|              5|\n",
      "+--------------------+---------------------+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,col\n",
    "spec = Window.partitionBy('order_date').orderBy(dailyProducts.cnt.desc())\n",
    "topNProducts = dailyProducts. \\\n",
    "    withColumn('ProductFreqRank',row_number().over(spec)). \\\n",
    "    where('ProductFreqRank<=5')\n",
    "topNProducts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|product_id|       products_desc|\n",
      "+----------+--------------------+\n",
      "|         1|Quest Q64 10 FT. ...|\n",
      "|         2|Under Armour Men'...|\n",
      "|         3|Under Armour Men'...|\n",
      "|         4|Under Armour Men'...|\n",
      "|         5|Riddell Youth Rev...|\n",
      "|         6|Jordan Men's VI R...|\n",
      "|         7|Schutt Youth Recr...|\n",
      "|         8|Nike Men's Vapor ...|\n",
      "|         9|Nike Adult Vapor ...|\n",
      "|        10|Under Armour Men'...|\n",
      "|        11|Fitness Gear 300 ...|\n",
      "|        12|Under Armour Men'...|\n",
      "|        13|Under Armour Men'...|\n",
      "|        14|Quik Shade Summit...|\n",
      "|        15|Under Armour Kids...|\n",
      "|        16|Riddell Youth 360...|\n",
      "|        17|Under Armour Men'...|\n",
      "|        18|Reebok Men's Full...|\n",
      "|        19|Nike Men's Finger...|\n",
      "|        20|Under Armour Men'...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productsCSV = spark.read.format('csv').load('/user/pi/retail_db/products').toDF('product_id','col1','products_desc','col2','col3','col4')\n",
    "products = productsCSV. \\\n",
    "            select('product_id','products_desc'). \\\n",
    "            withColumn('product_id',productsCSV.product_id.cast('int'))\n",
    "products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------------------------------+---+\n",
      "|order_date           |products_desc                                |cnt|\n",
      "+---------------------+---------------------------------------------+---+\n",
      "|2014-07-24 00:00:00.0|Perfect Fitness Perfect Rip Deck             |72 |\n",
      "|2014-07-24 00:00:00.0|Nike Men's CJ Elite 2 TD Football Cleat      |61 |\n",
      "|2014-07-24 00:00:00.0|Nike Men's Dri-FIT Victory Golf Polo         |56 |\n",
      "|2014-07-24 00:00:00.0|O'Brien Men's Neoprene Life Vest             |51 |\n",
      "|2014-07-24 00:00:00.0|Field & Stream Sportsman 16 Gun Fire Safe    |48 |\n",
      "|2014-07-23 00:00:00.0|Perfect Fitness Perfect Rip Deck             |62 |\n",
      "|2014-07-23 00:00:00.0|Nike Men's Dri-FIT Victory Golf Polo         |49 |\n",
      "|2014-07-23 00:00:00.0|Field & Stream Sportsman 16 Gun Fire Safe    |48 |\n",
      "|2014-07-23 00:00:00.0|O'Brien Men's Neoprene Life Vest             |47 |\n",
      "|2014-07-23 00:00:00.0|Nike Men's CJ Elite 2 TD Football Cleat      |47 |\n",
      "|2014-07-22 00:00:00.0|Nike Men's CJ Elite 2 TD Football Cleat      |50 |\n",
      "|2014-07-22 00:00:00.0|Nike Men's Dri-FIT Victory Golf Polo         |44 |\n",
      "|2014-07-22 00:00:00.0|Perfect Fitness Perfect Rip Deck             |43 |\n",
      "|2014-07-22 00:00:00.0|Diamondback Women's Serene Classic Comfort Bi|39 |\n",
      "|2014-07-22 00:00:00.0|Field & Stream Sportsman 16 Gun Fire Safe    |34 |\n",
      "|2014-07-21 00:00:00.0|Perfect Fitness Perfect Rip Deck             |94 |\n",
      "|2014-07-21 00:00:00.0|Pelican Sunstream 100 Kayak                  |75 |\n",
      "|2014-07-21 00:00:00.0|Nike Men's CJ Elite 2 TD Football Cleat      |69 |\n",
      "|2014-07-21 00:00:00.0|O'Brien Men's Neoprene Life Vest             |63 |\n",
      "|2014-07-21 00:00:00.0|Nike Men's Dri-FIT Victory Golf Polo         |61 |\n",
      "+---------------------+---------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topNProducts.join(products,topNProducts.order_item_product_id==products.product_id). \\\n",
    "            select('order_date','products_desc','cnt'). \\\n",
    "            orderBy(['order_date','cnt'],ascending=[0,0]). \\\n",
    "            show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
