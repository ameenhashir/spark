{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "***\n",
    "\n",
    "Convert snappy compressed avro data-files stored at hdfs location /user/cloudera/practice1/question3  into parquet file.\n",
    "\n",
    "sqoop import --connect \"jdbc:mysql://localhost/retail_db\" --password cloudera --username root --table orders --target-dir /user/cloudera/practice1/question3 --compress --compression-codec snappy --as-avrodatafile \n",
    "\n",
    "Instructions:\n",
    "\n",
    "Convert snappy compressed avro data-files stored at hdfs location /user/cloudera/practice1/question3  into parquet file.\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Result should be saved in /user/cloudera/practice1/question3/output/\n",
    "\n",
    "Output should consist of only order_id,order_status\n",
    "\n",
    "Output file should be saved as Parquet file in gzip Compression.\n",
    "\n",
    "=================================================================\n",
    "\n",
    "Important Information & Tips\n",
    "\n",
    "1. Please look at the answer only once you solve the question.\n",
    "\n",
    "2. Actual Exam is hands on exam and no answer is provided in exam.Below answer is  just provided to guide you\n",
    "\n",
    "3. To work with avro files on local cloudera vm, open spark-shell using\n",
    "\n",
    "spark-shell --packages org.apache.spark:spark-avro_2.11:2.4.4\n",
    "\n",
    "But exam environment is already setup with avro library, so in exam you just need to open spark-shell without --packages.\n",
    "\n",
    "4. By default, parquet files are stored in snappy compression,\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 17:17:38,673 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   2 pi supergroup          0 2020-06-10 19:29 /user/cloudera/practice1/question3/_SUCCESS\n",
      "drwxr-xr-x   - pi supergroup          0 2020-06-10 19:37 /user/cloudera/practice1/question3/output\n",
      "-rw-r--r--   2 pi supergroup     911596 2020-06-10 19:29 /user/cloudera/practice1/question3/part-00000-ed93b711-8df7-4eba-a11f-ce54cbe1393d-c000.avro\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls  /user/cloudera/practice1/question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "|_c0|                 _c1|  _c2|            _c3|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
      "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
      "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
      "|  6|2013-07-25 00:00:...| 7130|       COMPLETE|\n",
      "|  7|2013-07-25 00:00:...| 4530|       COMPLETE|\n",
      "|  8|2013-07-25 00:00:...| 2911|     PROCESSING|\n",
      "|  9|2013-07-25 00:00:...| 5657|PENDING_PAYMENT|\n",
      "| 10|2013-07-25 00:00:...| 5648|PENDING_PAYMENT|\n",
      "| 11|2013-07-25 00:00:...|  918| PAYMENT_REVIEW|\n",
      "| 12|2013-07-25 00:00:...| 1837|         CLOSED|\n",
      "| 13|2013-07-25 00:00:...| 9149|PENDING_PAYMENT|\n",
      "| 14|2013-07-25 00:00:...| 9842|     PROCESSING|\n",
      "| 15|2013-07-25 00:00:...| 2568|       COMPLETE|\n",
      "| 16|2013-07-25 00:00:...| 7276|PENDING_PAYMENT|\n",
      "| 17|2013-07-25 00:00:...| 2667|       COMPLETE|\n",
      "| 18|2013-07-25 00:00:...| 1205|         CLOSED|\n",
      "| 19|2013-07-25 00:00:...| 9488|PENDING_PAYMENT|\n",
      "| 20|2013-07-25 00:00:...| 9198|     PROCESSING|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.read.format(\"com.databricks.spark.avro\").load('/user/cloudera/practice1/question3')\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "orders.select(col('_c0').cast('int').alias('order_id'),col('_c1').alias('order_status')). \\\n",
    "    write.mode('overwrite').\\\n",
    "    parquet(\"/user/cloudera/practice1/question3/output\",compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|order_id|        order_status|\n",
      "+--------+--------------------+\n",
      "|       1|2013-07-25 00:00:...|\n",
      "|       2|2013-07-25 00:00:...|\n",
      "|       3|2013-07-25 00:00:...|\n",
      "|       4|2013-07-25 00:00:...|\n",
      "|       5|2013-07-25 00:00:...|\n",
      "|       6|2013-07-25 00:00:...|\n",
      "|       7|2013-07-25 00:00:...|\n",
      "|       8|2013-07-25 00:00:...|\n",
      "|       9|2013-07-25 00:00:...|\n",
      "|      10|2013-07-25 00:00:...|\n",
      "|      11|2013-07-25 00:00:...|\n",
      "|      12|2013-07-25 00:00:...|\n",
      "|      13|2013-07-25 00:00:...|\n",
      "|      14|2013-07-25 00:00:...|\n",
      "|      15|2013-07-25 00:00:...|\n",
      "|      16|2013-07-25 00:00:...|\n",
      "|      17|2013-07-25 00:00:...|\n",
      "|      18|2013-07-25 00:00:...|\n",
      "|      19|2013-07-25 00:00:...|\n",
      "|      20|2013-07-25 00:00:...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('/user/cloudera/practice1/question3/output').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"com\">//Load the avro file</span></li><li class=\"L1\"><span class=\"pln\">val dataFile </span><span class=\"pun\">=</span><span class=\"pln\"> spark</span></li><li class=\"L2\"><span class=\"pun\">.</span><span class=\"pln\">read</span></li><li class=\"L3\"><span class=\"pun\">.</span><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"avro\"</span><span class=\"pun\">)</span></li><li class=\"L4\"><span class=\"pun\">.</span><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/question3\"</span><span class=\"pun\">)</span></li><li class=\"L5\"><span class=\"pln\">&nbsp;</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;</span></li><li class=\"L7\"><span class=\"com\">//select order_id/status and save in gzip compressed parquet file</span></li><li class=\"L8\"><span class=\"pln\">dataFile</span></li><li class=\"L9\"><span class=\"pun\">.</span><span class=\"kwd\">select</span><span class=\"pun\">(</span><span class=\"str\">\"order_id\"</span><span class=\"pun\">,</span><span class=\"str\">\"order_status\"</span><span class=\"pun\">)</span></li><li class=\"L0\"><span class=\"pun\">.</span><span class=\"pln\">write</span></li><li class=\"L1\"><span class=\"pun\">.</span><span class=\"pln\">option</span><span class=\"pun\">(</span><span class=\"str\">\"compression\"</span><span class=\"pun\">,</span><span class=\"str\">\"gzip\"</span><span class=\"pun\">)</span></li><li class=\"L2\"><span class=\"pun\">.</span><span class=\"pln\">parquet</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/question3/output/\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Join the comma separated file located at hdfs location /user/cloudera/p1/p4/orders & /user/cloudera/p1/p4/customers to find out  customers who have placed more than 4 orders.\n",
    "\n",
    "Schema for customer File \n",
    "customer_id,customer_fname,......................................................\n",
    " \n",
    "Schema for Order File \n",
    "order_id,order_date,order_customer_id,order_status\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Order status should be COMPLETE\n",
    "\n",
    "Output should have customer_id,customer_fname,count\n",
    "\n",
    "Save the results in json format.\n",
    "\n",
    "Result should be order by count of orders in ascending fashion.\n",
    "\n",
    "Result should be saved in /user/cloudera/p1/p4/output \n",
    "\n",
    "=================================================================\n",
    "\n",
    "Important Information & Tips\n",
    "\n",
    "1. All Solutions provided in multiple choice are correct\n",
    "\n",
    "2. You will not be provided with any answer in actual exam.Below answers are just provided to guide you\n",
    "\n",
    "3. Schema of files are optional, you might or might not be given schema explicitly in exam.If in case schema are not explicitly given, you have to open the file and find out the schema.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|order_customer_id|count|\n",
      "+-----------------+-----+\n",
      "|             4900|    4|\n",
      "|            11458|    4|\n",
      "|            11748|    5|\n",
      "|             3175|    4|\n",
      "|              463|    4|\n",
      "|             5518|    4|\n",
      "|            10623|    4|\n",
      "|             2999|    5|\n",
      "|             7417|    4|\n",
      "|             2811|    4|\n",
      "|             1990|    5|\n",
      "|              392|    4|\n",
      "|             6393|    4|\n",
      "|              737|    5|\n",
      "|            11710|    4|\n",
      "|             2387|    4|\n",
      "|             1025|    4|\n",
      "|             1650|    4|\n",
      "|             3213|    4|\n",
      "|             6622|    4|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count,col\n",
    "orders = spark.read.format(\"csv\"). \\\n",
    "        schema('order_id int,order_date string,order_customer_id int,order_status string'). \\\n",
    "        load(\"/user/pi/retail_db/orders\"). \\\n",
    "        where('order_status == \"COMPLETE\"'). \\\n",
    "        groupBy('order_customer_id'). \\\n",
    "        agg(count('order_id').alias('count')). \\\n",
    "        where('count >= 4')\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = spark.read.csv(\"/user/pi/retail_db/customers\"). \\\n",
    "        select(col('_c0').cast('int').alias('customer_id'),col('_c1').alias('customer_fname'))\n",
    "orders.join(customers,orders.order_customer_id==customers.customer_id). \\\n",
    "    select('customer_id','customer_fname','count'). \\\n",
    "    orderBy('count'). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    json(\"/user/cloudera/p1/p4/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-----------+\n",
      "|count|customer_fname|customer_id|\n",
      "+-----+--------------+-----------+\n",
      "|    4|         James|       4900|\n",
      "|    4|        Rachel|      11458|\n",
      "|    4|          Mary|       3175|\n",
      "|    4|         Harry|        463|\n",
      "|    4|      Patricia|       5518|\n",
      "|    4|        Joshua|      10623|\n",
      "|    4|          Mary|       7417|\n",
      "|    4|        Thomas|       2811|\n",
      "|    4|       Beverly|        392|\n",
      "|    4|          Mary|       6393|\n",
      "|    4|         Jacob|      11710|\n",
      "|    4|          Mary|       2387|\n",
      "|    4|         Megan|       1025|\n",
      "|    4|          Mary|       1650|\n",
      "|    4|          Mary|       3213|\n",
      "|    4|          Mary|       6622|\n",
      "|    4|         Harry|       4391|\n",
      "|    4|          Mary|       5155|\n",
      "|    4|          Mary|         85|\n",
      "|    4|          Mary|       3561|\n",
      "+-----+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(\"/user/cloudera/p1/p4/output\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"com\">//Load the customer data and name the required columns</span></li><li class=\"L1\"><span class=\"pln\">val cusDS</span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span></li><li class=\"L2\"><span class=\"pln\">read</span><span class=\"pun\">.</span></li><li class=\"L3\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"csv\"</span><span class=\"pun\">).</span></li><li class=\"L4\"><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/p1/p4/customers\"</span><span class=\"pun\">).</span></li><li class=\"L5\"><span class=\"kwd\">select</span><span class=\"pun\">(</span><span class=\"pln\">col</span><span class=\"pun\">(</span><span class=\"str\">\"_c0\"</span><span class=\"pun\">).</span><span class=\"kwd\">as</span><span class=\"pun\">(</span><span class=\"str\">\"customer_id\"</span><span class=\"pun\">),</span><span class=\"pln\">col</span><span class=\"pun\">(</span><span class=\"str\">\"_c1\"</span><span class=\"pun\">).</span><span class=\"kwd\">as</span><span class=\"pun\">(</span><span class=\"str\">\"customer_fname\"</span><span class=\"pun\">))</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;</span></li><li class=\"L7\"><span class=\"com\">//Load,rename,filter,group,join,sort and write json</span></li><li class=\"L8\"><span class=\"pln\">val ordDS </span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span></li><li class=\"L9\"><span class=\"pln\">read</span><span class=\"pun\">.</span></li><li class=\"L0\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"csv\"</span><span class=\"pun\">).</span></li><li class=\"L1\"><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/p1/p4/orders\"</span><span class=\"pun\">).</span></li><li class=\"L2\"><span class=\"kwd\">select</span><span class=\"pun\">(</span><span class=\"pln\">col</span><span class=\"pun\">(</span><span class=\"str\">\"_c2\"</span><span class=\"pun\">).</span><span class=\"kwd\">as</span><span class=\"pun\">(</span><span class=\"str\">\"customer_id\"</span><span class=\"pun\">),</span><span class=\"pln\">col</span><span class=\"pun\">(</span><span class=\"str\">\"_c3\"</span><span class=\"pun\">).</span><span class=\"kwd\">as</span><span class=\"pun\">(</span><span class=\"str\">\"status\"</span><span class=\"pun\">)).</span><span class=\"pln\">   </span></li><li class=\"L3\"><span class=\"pln\">filter</span><span class=\"pun\">(</span><span class=\"pln\">$</span><span class=\"str\">\"status\"</span><span class=\"pun\">===</span><span class=\"str\">\"COMPLETE\"</span><span class=\"pun\">).</span></li><li class=\"L4\"><span class=\"pln\">groupBy</span><span class=\"pun\">(</span><span class=\"str\">\"customer_id\"</span><span class=\"pun\">).</span></li><li class=\"L5\"><span class=\"pln\">count</span><span class=\"pun\">.</span><span class=\"kwd\">where</span><span class=\"pun\">(</span><span class=\"str\">\"count &gt; 4\"</span><span class=\"pun\">).</span></li><li class=\"L6\"><span class=\"pln\">join</span><span class=\"pun\">(</span><span class=\"pln\">cusDS</span><span class=\"pun\">,</span><span class=\"str\">\"customer_id\"</span><span class=\"pun\">).</span></li><li class=\"L7\"><span class=\"pln\">sort</span><span class=\"pun\">(</span><span class=\"str\">\"count\"</span><span class=\"pun\">).</span></li><li class=\"L8\"><span class=\"pln\">write</span><span class=\"pun\">.</span></li><li class=\"L9\"><span class=\"pln\">mode</span><span class=\"pun\">(</span><span class=\"str\">\"overwrite\"</span><span class=\"pun\">).</span></li><li class=\"L0\"><span class=\"pln\">json</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/p1/p4/output\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Instructions:\n",
    "\n",
    "From provided parquet files located at hdfs location :\n",
    "\n",
    "/user/cloudera/practice1/problem5\n",
    "\n",
    "Get maximum product_price in each product_category\n",
    "\n",
    "order the results by maximum price descending.\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Final output should be saved in below hdfs location:\n",
    "\n",
    "/user/cloudera/practice1/problem5/output\n",
    "\n",
    "Final output should have product_category_id and max_price separated by pipe delimiter\n",
    "\n",
    "Ouput should be saved in text format with Gzip compression\n",
    "\n",
    "Output should be stored in a single file.\n",
    "\n",
    "=================================================================\n",
    "\n",
    "Important Information & Tips\n",
    "\n",
    "[You will not be provided with any answer choice in actual exam.Below answers are just provided to guide you]\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark.read.csv(\"/user/pi/retail_db/products\",schema= 'product_id int,product_catagory_id int, \\\n",
    "                             product_name string,product_description string,product_price float, \\\n",
    "                             product_image string')\n",
    "order_items.write.mode('overwrite').parquet(\"/user/cloudera/practice1/problem5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max,concat,lit\n",
    "spark.read.parquet(\"/user/cloudera/practice1/problem5\"). \\\n",
    "    groupBy('product_catagory_id'). \\\n",
    "    agg(max('product_price').alias('max_product_price')). \\\n",
    "    orderBy(col('max_product_price').desc()). \\\n",
    "    select(concat('product_catagory_id',lit('|'),'max_product_price')). \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    text('/user/cloudera/practice1/problem5/output',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     value|\n",
      "+----------+\n",
      "|10|1999.99|\n",
      "|22|1799.99|\n",
      "| 4|1799.99|\n",
      "|47|1099.99|\n",
      "| 32|999.99|\n",
      "| 31|899.99|\n",
      "| 48|799.99|\n",
      "| 11|799.99|\n",
      "| 45|599.99|\n",
      "| 38|599.99|\n",
      "| 46|549.99|\n",
      "|  5|499.99|\n",
      "|  9|499.99|\n",
      "| 43|449.99|\n",
      "| 44|399.99|\n",
      "|  6|399.99|\n",
      "| 41|399.99|\n",
      "| 17|399.99|\n",
      "|  8|399.99|\n",
      "| 49|399.99|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text('/user/cloudera/practice1/problem5/output').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">val productDF </span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span></li><li class=\"L1\"><span class=\"pln\">read</span><span class=\"pun\">.</span></li><li class=\"L2\"><span class=\"pln\">parquet</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem5\"</span><span class=\"pun\">).</span></li><li class=\"L3\"><span class=\"pln\">createOrReplaceTempView</span><span class=\"pun\">(</span><span class=\"str\">\"product\"</span><span class=\"pun\">)</span></li><li class=\"L4\"><span class=\"pln\">&nbsp;</span></li><li class=\"L5\"><span class=\"pln\">val maxDF</span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span><span class=\"pln\">sql</span><span class=\"pun\">(</span><span class=\"str\">\"select CONCAT(product_category_id,'|',max(product_price)) from product group by product_category_id order by max(product_price) desc\"</span><span class=\"pun\">)</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;</span></li><li class=\"L7\"><span class=\"pln\">maxDF</span><span class=\"pun\">.</span></li><li class=\"L8\"><span class=\"pln\">coalesce</span><span class=\"pun\">(</span><span class=\"lit\">1</span><span class=\"pun\">).</span></li><li class=\"L9\"><span class=\"pln\">write</span><span class=\"pun\">.</span></li><li class=\"L0\"><span class=\"pln\">mode</span><span class=\"pun\">(</span><span class=\"str\">\"overwrite\"</span><span class=\"pun\">).</span></li><li class=\"L1\"><span class=\"pln\">option</span><span class=\"pun\">(</span><span class=\"str\">\"compression\"</span><span class=\"pun\">,</span><span class=\"str\">\"gzip\"</span><span class=\"pun\">).</span></li><li class=\"L2\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"text\"</span><span class=\"pun\">).</span></li><li class=\"L3\"><span class=\"pln\">save</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem5/output\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Provided customer tab delimited files at below HDFS location.\n",
    "\n",
    "Input folder is  /user/cloudera/practice1/problem6\n",
    "\n",
    "Find all customers that lives 'Caguas' city.\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Result should be saved in /user/cloudera/practice1/problem6/output\n",
    "\n",
    "Output file should be saved in avro format in deflate compression.\n",
    "\n",
    "=================================================================\n",
    "\n",
    "Important Information:\n",
    "\n",
    "You will not be provided with any answer choice in actual exam.Below answers are just provided to guide you.\n",
    "\n",
    "You can check avro file meta information using avro-tools to check for compression.Copy the part file on your local and run below command.\n",
    "\n",
    "avro-tools getmeta <path to avro file>.avro\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare tab delimited file\n",
    "spark.read.csv(\"/user/pi/retail_db/customers\", \\\n",
    "              schema='customer_id int,customer_fname string,customer_lname string, \\\n",
    "              customer_email string,customer_password string,customer_street string,customer_city string, \\\n",
    "              customer_city_code string,customer_postal_code string'). \\\n",
    "    write.mode('overwrite').csv('/user/cloudera/practice1/problem6',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"/user/cloudera/practice1/problem6\",sep='\\t', \\\n",
    "              schema='customer_id int,customer_fname string,customer_lname string, \\\n",
    "              customer_email string,customer_password string,customer_street string,customer_city string, \\\n",
    "              customer_city_code string,customer_postal_code string'). \\\n",
    "            where('customer_city == \"Caguas\"'). \\\n",
    "            write.format('com.databricks.spark.avro'). \\\n",
    "            save('/user/cloudera/practice1/problem6/output',compression='deflate')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+------------------+--------------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_city_code|customer_postal_code|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+------------------+--------------------+\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|                PR|               00725|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|                PR|               00725|\n",
      "|          7|       Melissa|        Wilcox|     XXXXXXXXX|        XXXXXXXXX|9453 High Concession|       Caguas|                PR|               00725|\n",
      "|          9|          Mary|         Perez|     XXXXXXXXX|        XXXXXXXXX| 3616 Quaking Street|       Caguas|                PR|               00725|\n",
      "|         11|          Mary|       Huffman|     XXXXXXXXX|        XXXXXXXXX|    3169 Stony Woods|       Caguas|                PR|               00725|\n",
      "|         13|          Mary|       Baldwin|     XXXXXXXXX|        XXXXXXXXX|7922 Iron Oak Gar...|       Caguas|                PR|               00725|\n",
      "|         16|       Tiffany|         Smith|     XXXXXXXXX|        XXXXXXXXX|      6651 Iron Port|       Caguas|                PR|               00725|\n",
      "|         19|     Stephanie|      Mitchell|     XXXXXXXXX|        XXXXXXXXX|3543 Red Treasure...|       Caguas|                PR|               00725|\n",
      "|         21|       William|     Zimmerman|     XXXXXXXXX|        XXXXXXXXX|3323 Old Willow Mall|       Caguas|                PR|               00725|\n",
      "|         24|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX| 9417 Emerald Towers|       Caguas|                PR|               00725|\n",
      "|         27|          Mary|       Vincent|     XXXXXXXXX|        XXXXXXXXX|1768 Sleepy Zephy...|       Caguas|                PR|               00725|\n",
      "|         30|       Barbara|         Smith|     XXXXXXXXX|        XXXXXXXXX|   2455 Merry Hollow|       Caguas|                PR|               00725|\n",
      "|         32|         Alice|         Smith|     XXXXXXXXX|        XXXXXXXXX|   2082 Hidden Green|       Caguas|                PR|               00725|\n",
      "|         34|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|3330 Easy Berry R...|       Caguas|                PR|               00725|\n",
      "|         36|      Michelle|         Carey|     XXXXXXXXX|        XXXXXXXXX| 6336 Fallen Village|       Caguas|                PR|               00725|\n",
      "|         39|          Juan|      Mckinney|     XXXXXXXXX|        XXXXXXXXX|7274 Blue Wagon  ...|       Caguas|                PR|               00725|\n",
      "|         43|          Mary|       Herring|     XXXXXXXXX|        XXXXXXXXX|   4575 Thunder Dale|       Caguas|                PR|               00725|\n",
      "|         47|          Lori|        Fuller|     XXXXXXXXX|        XXXXXXXXX|      357 Noble Lane|       Caguas|                PR|               00725|\n",
      "|         49|        Martha|         Smith|     XXXXXXXXX|        XXXXXXXXX|    7449 Merry Chase|       Caguas|                PR|               00725|\n",
      "|         51|       Jessica|         Smith|     XXXXXXXXX|        XXXXXXXXX|8344 Dewy Fawn Farms|       Caguas|                PR|               00725|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('com.databricks.spark.avro').load('/user/cloudera/practice1/problem6/output').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">val schema </span><span class=\"pun\">=</span><span class=\"pln\"> </span><span class=\"typ\">Seq</span><span class=\"pun\">(</span><span class=\"str\">\"custId\"</span><span class=\"pun\">,</span><span class=\"str\">\"name\"</span><span class=\"pun\">,</span><span class=\"str\">\"city\"</span><span class=\"pun\">)</span></li><li class=\"L1\"><span class=\"kwd\">var</span><span class=\"pln\"> custData </span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span></li><li class=\"L2\"><span class=\"pln\">read</span><span class=\"pun\">.</span></li><li class=\"L3\"><span class=\"pln\">option</span><span class=\"pun\">(</span><span class=\"str\">\"sep\"</span><span class=\"pun\">,</span><span class=\"str\">\"\\t\"</span><span class=\"pun\">).</span></li><li class=\"L4\"><span class=\"pln\">csv</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem6\"</span><span class=\"pun\">).</span></li><li class=\"L5\"><span class=\"pln\">toDF</span><span class=\"pun\">(</span><span class=\"pln\">newColumns</span><span class=\"pun\">:</span><span class=\"pln\">_</span><span class=\"pun\">*)</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;</span></li><li class=\"L7\"><span class=\"pln\">custData</span><span class=\"pun\">.</span></li><li class=\"L8\"><span class=\"pln\">filter</span><span class=\"pun\">(</span><span class=\"str\">\"city='Caguas'\"</span><span class=\"pun\">).</span></li><li class=\"L9\"><span class=\"pln\">write</span><span class=\"pun\">.</span></li><li class=\"L0\"><span class=\"pln\">option</span><span class=\"pun\">(</span><span class=\"str\">\"compression\"</span><span class=\"pun\">,</span><span class=\"str\">\"deflate\"</span><span class=\"pun\">).</span></li><li class=\"L1\"><span class=\"pln\">save</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem6/output\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "*** \n",
    "\n",
    "Instructions:\n",
    "\n",
    "Convert avro data-files stored at hdfs location /user/cloudera/practice1/problem7/customer/avro  into tab delimited file\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Result should be saved in /user/cloudera/practice1/problem7/customer_text_bzip2\n",
    "\n",
    "Output file should be saved as tab delimited file in bzip2 Compression.\n",
    "\n",
    "Output should consist of customer_id   customer_name(only first three letter)   customer_lname\n",
    "\n",
    "Sample Output:\n",
    "\n",
    "21    And   Smith\n",
    "\n",
    "111    Mar    Jons\n",
    "\n",
    "=================================================================\n",
    "\n",
    "Important Informations & Tips\n",
    "\n",
    "[You will not be provided with any answer choice in actual exam.Below answers are just provided to guide you]\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare input\n",
    "spark.read.csv(\"/user/pi/retail_db/customers\", \\\n",
    "              schema='customer_id int,customer_fname string,customer_lname string, \\\n",
    "              customer_email string,customer_password string,customer_street string,customer_city string, \\\n",
    "              customer_city_code string,customer_postal_code string'). \\\n",
    "    write. \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    format(\"com.databricks.spark.avro\"). \\\n",
    "    save(\"/user/cloudera/practice1/problem7/customer/avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "spark.read.format(\"com.databricks.spark.avro\"). \\\n",
    "    load('/user/cloudera/practice1/problem7/customer/avro'). \\\n",
    "    select('customer_id',substring(col('customer_fname'),0,3).alias('customer_fname'),'customer_lname'). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv('/user/cloudera/practice1/problem7/customer_text_bzip2',sep='\\t',compression='bzip2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+\n",
      "|_c0|_c1|      _c2|\n",
      "+---+---+---------+\n",
      "|  1|Ric|Hernandez|\n",
      "|  2|Mar|  Barrett|\n",
      "|  3|Ann|    Smith|\n",
      "|  4|Mar|    Jones|\n",
      "|  5|Rob|   Hudson|\n",
      "|  6|Mar|    Smith|\n",
      "|  7|Mel|   Wilcox|\n",
      "|  8|Meg|    Smith|\n",
      "|  9|Mar|    Perez|\n",
      "| 10|Mel|    Smith|\n",
      "| 11|Mar|  Huffman|\n",
      "| 12|Chr|    Smith|\n",
      "| 13|Mar|  Baldwin|\n",
      "| 14|Kat|    Smith|\n",
      "| 15|Jan|     Luna|\n",
      "| 16|Tif|    Smith|\n",
      "| 17|Mar| Robinson|\n",
      "| 18|Rob|    Smith|\n",
      "| 19|Ste| Mitchell|\n",
      "| 20|Mar|    Ellis|\n",
      "+---+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/user/cloudera/practice1/problem7/customer_text_bzip2',sep='\\t').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">val custDF</span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span></li><li class=\"L1\"><span class=\"pln\">read</span><span class=\"pun\">.</span></li><li class=\"L2\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"avro\"</span><span class=\"pun\">).</span></li><li class=\"L3\"><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem7/customer/avro\"</span><span class=\"pun\">)</span></li><li class=\"L4\"><span class=\"pln\">&nbsp;</span></li><li class=\"L5\"><span class=\"pln\">custDF</span><span class=\"pun\">.</span></li><li class=\"L6\"><span class=\"pln\">withColumn</span><span class=\"pun\">(</span><span class=\"str\">\"customer_fname\"</span><span class=\"pun\">,</span><span class=\"pln\"> substring</span><span class=\"pun\">(</span><span class=\"pln\">col</span><span class=\"pun\">(</span><span class=\"str\">\"customer_fname\"</span><span class=\"pun\">),</span><span class=\"pln\"> </span><span class=\"lit\">0</span><span class=\"pun\">,</span><span class=\"pln\"> </span><span class=\"lit\">3</span><span class=\"pun\">)).</span></li><li class=\"L7\"><span class=\"pln\">map</span><span class=\"pun\">(</span><span class=\"pln\">x </span><span class=\"pun\">=&gt;</span><span class=\"pln\"> x</span><span class=\"pun\">.</span><span class=\"pln\">mkString</span><span class=\"pun\">(</span><span class=\"str\">\"\\t\"</span><span class=\"pun\">)).</span></li><li class=\"L8\"><span class=\"pln\">write</span><span class=\"pun\">.</span></li><li class=\"L9\"><span class=\"pln\">mode</span><span class=\"pun\">(</span><span class=\"str\">\"overwrite\"</span><span class=\"pun\">).</span></li><li class=\"L0\"><span class=\"pln\">option</span><span class=\"pun\">(</span><span class=\"str\">\"compression\"</span><span class=\"pun\">,</span><span class=\"str\">\"bzip2\"</span><span class=\"pun\">).</span></li><li class=\"L1\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"text\"</span><span class=\"pun\">).</span></li><li class=\"L2\"><span class=\"pln\">save</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem7/customer_text_bzip\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Get products from metastore table named \"product_replica\" whose price > 100 and save the results in HDFS in parquet format.\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Result should be saved in /user/cloudera/practice1/problem8/product/output as parquet file\n",
    "\n",
    "Files should be saved in uncompressed format.\n",
    "\n",
    "================================================================\n",
    "\n",
    "Important Information & Tips:\n",
    "\n",
    "1. You will not be provided with any answer choice in actual exam.Below answers are just provided to guide you\n",
    "\n",
    "2. In case you face table not found issue. Just check that SPARK_HOME/conf has hive_site.xml copied from /etc/hive/conf/hive_site.xml.\n",
    "\n",
    "3. If in case any derby lock issue occurs, delete SPARK_HOME/metastore_db/dbex.lck   to release the lock.\n",
    "\n",
    "4. By default, parquet files are compressed using snappy compression codec.\n",
    "\n",
    "5. You can check parquet file meta information using parquet tools.\n",
    "\n",
    "parquet-tools meta <path to file>.parquet\n",
    "\n",
    "Larger image\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----------+\n",
      "|     database|           tableName|isTemporary|\n",
      "+-------------+--------------------+-----------+\n",
      "|retail_db_txt|           customers|      false|\n",
      "|retail_db_txt|     daily_revenue_2|      false|\n",
      "|retail_db_txt|         order_items|      false|\n",
      "|retail_db_txt|              orders|      false|\n",
      "|retail_db_txt|top5_customers_pe...|      false|\n",
      "+-------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use retail_db_txt\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       databaseName|\n",
      "+-------------------+\n",
      "|ameen_daily_revenue|\n",
      "|ameen_retail_db_txt|\n",
      "|            default|\n",
      "|      retail_db_txt|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"/user/pi/retail_db/products\", \\\n",
    "                  schema = 'product_id int,product_catagory_id int,product_name string, \\\n",
    "                  product_description string,product_price float,product_image string'). \\\n",
    "            createOrReplaceTempView('prds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop table if exists retail_db_txt.product_replica')\n",
    "spark.sql('create table retail_db_txt.product_replica as select * from prds')\n",
    "spark.sql('truncate table retail_db_txt.product_replica')\n",
    "spark.sql('insert into retail_db_txt.product_replica select * from prds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from retail_db_txt.product_replica where product_price > 100'). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    parquet('/user/cloudera/practice1/problem8/product/output',compression='uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|product_id|product_catagory_id|        product_name|product_description|product_price|       product_image|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "|       689|                 31|Tour Edge XCG7 Ir...|               null|       599.99|http://images.acm...|\n",
      "|       690|                 31|Tour Edge XCG7 X-...|               null|       599.99|http://images.acm...|\n",
      "|       693|                 32|Top Flite Women's...|               null|       179.98|http://images.acm...|\n",
      "|       694|                 32|Callaway Women's ...|               null|       999.99|http://images.acm...|\n",
      "|       695|                 32|Callaway Women's ...|               null|       999.99|http://images.acm...|\n",
      "|       696|                 32|Wilson Women's Pr...|               null|       299.99|http://images.acm...|\n",
      "|       697|                 32|Tour Edge Women's...|               null|       599.99|http://images.acm...|\n",
      "|       698|                 32|Cleveland Women's...|               null|       699.99|http://images.acm...|\n",
      "|       699|                 32|Tour Edge Women's...|               null|       599.99|http://images.acm...|\n",
      "|       704|                 32|Cobra Women's AMP...|               null|       499.99|http://images.acm...|\n",
      "|       705|                 32|Cleveland Golf Wo...|               null|       119.99|http://images.acm...|\n",
      "|       708|                 32|Adams Golf Women'...|               null|       499.99|http://images.acm...|\n",
      "|       713|                 32|TaylorMade Women'...|               null|       199.99|http://images.acm...|\n",
      "|       714|                 32|TaylorMade Women'...|               null|       179.99|http://images.acm...|\n",
      "|       715|                 32|TaylorMade Women'...|               null|       129.99|http://images.acm...|\n",
      "|       716|                 32|TaylorMade Women'...|               null|       449.99|http://images.acm...|\n",
      "|       718|                 33|Nike Lunarwaverly...|               null|       139.99|http://images.acm...|\n",
      "|       725|                 33|LIJA Women's Butt...|               null|        108.0|http://images.acm...|\n",
      "|       739|                 33|Nike TW 14 Mesh G...|               null|       169.99|http://images.acm...|\n",
      "|       740|                 33|Nike TW 14 Mesh G...|               null|       169.99|http://images.acm...|\n",
      "+----------+-------------------+--------------------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('/user/cloudera/practice1/problem8/product/output').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">spark</span><span class=\"pun\">.</span></li><li class=\"L1\"><span class=\"pln\">sql</span><span class=\"pun\">(</span><span class=\"str\">\"select * from product_replica where product_price&gt;100\"</span><span class=\"pun\">).</span></li><li class=\"L2\"><span class=\"pln\">write</span><span class=\"pun\">.</span></li><li class=\"L3\"><span class=\"pln\">mode</span><span class=\"pun\">(</span><span class=\"str\">\"overwrite\"</span><span class=\"pun\">).</span></li><li class=\"L4\"><span class=\"pln\">option</span><span class=\"pun\">(</span><span class=\"str\">\"compression\"</span><span class=\"pun\">,</span><span class=\"str\">\"uncompressed\"</span><span class=\"pun\">).</span></li><li class=\"L5\"><span class=\"pln\">parquet</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/problem8/product/output\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Instructions:\n",
    "\n",
    "\n",
    "\n",
    "Find out all PENDING_PAYMENT orders in March 2014.\n",
    "\n",
    "\n",
    "\n",
    "order_date format is in unix_timestamp\n",
    "\n",
    "\n",
    "\n",
    "Input file is parquet file stored at hdfs location\n",
    "\n",
    "/user/cloudera/practice1/question8\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "\n",
    "\n",
    "Output should be date and total pending order for that date.\n",
    "\n",
    "\n",
    "\n",
    "Output should be saved at below hdfs location\n",
    "\n",
    "/user/cloudera/practice1/question8/output\n",
    "\n",
    "\n",
    "\n",
    "Output should be json file format.\n",
    "\n",
    "=================================================================\n",
    "\n",
    "\n",
    "\n",
    "Important Information & Tips\n",
    "\n",
    "1. Please look at the answer only once you solve the question.\n",
    "\n",
    "1. Actual Exam is hands on exam and no answer is provided in exam.Below answer is  just provided to guide you\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare input file\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "spark.read.csv(\"/user/pi/retail_db/orders\",schema=\"order_id int,order_date string,customer_id int,order_status string\"). \\\n",
    "    select('order_id',unix_timestamp('order_date').alias('order_date'),'customer_id','order_status'). \\\n",
    "    write.mode('overwrite'). \\\n",
    "    parquet(\"/user/cloudera/practice1/question8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|   order_status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|1374706800|      11599|         CLOSED|\n",
      "|       2|1374706800|        256|PENDING_PAYMENT|\n",
      "|       3|1374706800|      12111|       COMPLETE|\n",
      "|       4|1374706800|       8827|         CLOSED|\n",
      "|       5|1374706800|      11318|       COMPLETE|\n",
      "|       6|1374706800|       7130|       COMPLETE|\n",
      "|       7|1374706800|       4530|       COMPLETE|\n",
      "|       8|1374706800|       2911|     PROCESSING|\n",
      "|       9|1374706800|       5657|PENDING_PAYMENT|\n",
      "|      10|1374706800|       5648|PENDING_PAYMENT|\n",
      "|      11|1374706800|        918| PAYMENT_REVIEW|\n",
      "|      12|1374706800|       1837|         CLOSED|\n",
      "|      13|1374706800|       9149|PENDING_PAYMENT|\n",
      "|      14|1374706800|       9842|     PROCESSING|\n",
      "|      15|1374706800|       2568|       COMPLETE|\n",
      "|      16|1374706800|       7276|PENDING_PAYMENT|\n",
      "|      17|1374706800|       2667|       COMPLETE|\n",
      "|      18|1374706800|       1205|         CLOSED|\n",
      "|      19|1374706800|       9488|PENDING_PAYMENT|\n",
      "|      20|1374706800|       9198|     PROCESSING|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/user/cloudera/practice1/question8\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring,count,from_unixtime\n",
    "spark.read.parquet(\"/user/cloudera/practice1/question8\"). \\\n",
    "    withColumn('order_date',from_unixtime('order_date')). \\\n",
    "    where('order_status == \"PENDING_PAYMENT\" and order_date like \"2014-03%\"'). \\\n",
    "    groupBy('order_date'). \\\n",
    "    agg(count('order_id').alias('Total_Pending_order')). \\\n",
    "    orderBy('order_date'). \\\n",
    "    write.mode('overwrite'). \\\n",
    "    json(\"/user/cloudera/practice1/question8/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|Total_Pending_order|         order_date|\n",
      "+-------------------+-------------------+\n",
      "|                 52|2014-03-01 00:00:00|\n",
      "|                 27|2014-03-02 00:00:00|\n",
      "|                 62|2014-03-03 00:00:00|\n",
      "|                 43|2014-03-04 00:00:00|\n",
      "|                 44|2014-03-05 00:00:00|\n",
      "|                 50|2014-03-06 00:00:00|\n",
      "|                 34|2014-03-07 00:00:00|\n",
      "|                 14|2014-03-08 00:00:00|\n",
      "|                 64|2014-03-10 00:00:00|\n",
      "|                 29|2014-03-11 00:00:00|\n",
      "|                 52|2014-03-12 00:00:00|\n",
      "|                 40|2014-03-13 00:00:00|\n",
      "|                 41|2014-03-14 00:00:00|\n",
      "|                 49|2014-03-15 00:00:00|\n",
      "|                 45|2014-03-16 00:00:00|\n",
      "|                 19|2014-03-17 00:00:00|\n",
      "|                 49|2014-03-18 00:00:00|\n",
      "|                 26|2014-03-19 00:00:00|\n",
      "|                 34|2014-03-20 00:00:00|\n",
      "|                 49|2014-03-21 00:00:00|\n",
      "|                 48|2014-03-22 00:00:00|\n",
      "|                 34|2014-03-23 00:00:00|\n",
      "|                 31|2014-03-24 00:00:00|\n",
      "|                 28|2014-03-25 00:00:00|\n",
      "|                 61|2014-03-26 00:00:00|\n",
      "|                 26|2014-03-27 00:00:00|\n",
      "|                 40|2014-03-28 00:00:00|\n",
      "|                 18|2014-03-29 00:00:00|\n",
      "|                 57|2014-03-30 00:00:00|\n",
      "|                 67|2014-03-31 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('/user/cloudera/practice1/question8/output').show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"pln\">spark</span><span class=\"pun\">.</span></li><li class=\"L1\"><span class=\"pln\">read</span><span class=\"pun\">.</span></li><li class=\"L2\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"parquet\"</span><span class=\"pun\">).</span></li><li class=\"L3\"><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/question8\"</span><span class=\"pun\">).</span></li><li class=\"L4\"><span class=\"pln\">withColumn</span><span class=\"pun\">(</span><span class=\"str\">\"order_date\"</span><span class=\"pun\">,</span><span class=\"pln\">to_date</span><span class=\"pun\">(</span><span class=\"pln\">from_unixtime</span><span class=\"pun\">(</span><span class=\"pln\"></span><span class=\"str\">\"order_date\"</span><span class=\"pun\">/</span><span class=\"lit\">1000</span><span class=\"pun\">))).</span></li><li class=\"L5\"><span class=\"pln\">filter</span><span class=\"pun\">(</span><span class=\"str\">\"order_date LIKE '2014-03%' and order_status='PENDING_PAYMENT'\"</span><span class=\"pun\">).</span></li><li class=\"L6\"><span class=\"pln\">groupBy</span><span class=\"pun\">(</span><span class=\"pln\"></span><span class=\"str\">\"order_date\"</span><span class=\"pun\">).</span><span class=\"pln\">count</span><span class=\"pun\">.</span></li><li class=\"L7\"><span class=\"pln\">write</span><span class=\"pun\">.</span></li><li class=\"L8\"><span class=\"pln\">mode</span><span class=\"pun\">(</span><span class=\"str\">\"overwrite\"</span><span class=\"pun\">).</span></li><li class=\"L9\"><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"json\"</span><span class=\"pun\">).</span></li><li class=\"L0\"><span class=\"pln\">save</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice1/question8/output\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Find out total number of orders placed by each customers in year 2013.\n",
    "\n",
    "Order status should be COMPLETE\n",
    "\n",
    "order_date format is in unix_timestamp\n",
    "\n",
    "\n",
    "\n",
    "Input customer & order files are stored as avro file at below hdfs location\n",
    "\n",
    "/user/cloudera/practice2/question8/orders\n",
    "\n",
    "/user/cloudera/practice2/question8/customers\n",
    "\n",
    "\n",
    "\n",
    "Output Requirement:\n",
    "\n",
    "Output should be stored in a hive table named \"customer_order\" with three columns customer_fname,customer_lname and orders_count.\n",
    "\n",
    "Hive tables should be partitioned by customer_state.\n",
    "\n",
    "=================================================================\n",
    "\n",
    "Important Information & Tips\n",
    "1. To enable Hive partitioning, please run these command in your spark-shell\n",
    "\n",
    "spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n",
    "spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \n",
    "2. Please look at the answer only once you solve the question.\n",
    "\n",
    "3.  Actual Exam is hands on exam and no answer is provided in exam.Below answer is  just provided to guide you.\n",
    "\n",
    "4. Solution is very long so providing here only\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol class=\"linenums\"><li class=\"L0\"><span class=\"com\">//Creating Customer DataSet </span></li><li class=\"L1\"><span class=\"pln\">val cus </span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span><span class=\"pln\">read</span><span class=\"pun\">.</span><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"avro\"</span><span class=\"pun\">).</span><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice2/question8/customers\"</span><span class=\"pun\">)</span></li><li class=\"L2\"><span class=\"pln\">&nbsp;</span></li><li class=\"L3\"><span class=\"com\">//Creating Order DataSet </span></li><li class=\"L4\"><span class=\"pln\">val ord </span><span class=\"pun\">=</span><span class=\"pln\"> spark</span><span class=\"pun\">.</span><span class=\"pln\">read</span><span class=\"pun\">.</span><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"avro\"</span><span class=\"pun\">).</span><span class=\"pln\">load</span><span class=\"pun\">(</span><span class=\"str\">\"/user/cloudera/practice2/question8/orders\"</span><span class=\"pun\">).</span><span class=\"pln\">withColumn</span><span class=\"pun\">(</span><span class=\"str\">\"order_date\"</span><span class=\"pun\">,</span><span class=\"pln\">to_date</span><span class=\"pun\">(</span><span class=\"pln\">from_unixtime</span><span class=\"pun\">(</span><span class=\"pln\"></span><span class=\"str\">\"order_date\"</span><span class=\"pun\">/</span><span class=\"lit\">1000</span><span class=\"pun\">)))</span></li><li class=\"L5\"><span class=\"pln\">&nbsp;</span></li><li class=\"L6\"><span class=\"pln\">&nbsp;</span></li><li class=\"L7\"><span class=\"com\">//Filtering to find all orders in 2013 and whose status is Complete</span></li><li class=\"L8\"><span class=\"pln\">val filOrd </span><span class=\"pun\">=</span><span class=\"pln\"> ord</span><span class=\"pun\">.</span><span class=\"pln\">filter</span><span class=\"pun\">(</span><span class=\"str\">\"order_date LIKE '2013%' and order_status='COMPLETE'\"</span><span class=\"pun\">)</span></li><li class=\"L9\"><span class=\"pln\">&nbsp;</span></li><li class=\"L0\"><span class=\"com\">//Finding the total order placed by each customer in year 2013</span></li><li class=\"L1\"><span class=\"pln\">val groupDF</span><span class=\"pun\">=</span><span class=\"pln\">filOrd</span><span class=\"pun\">.</span><span class=\"pln\">groupBy</span><span class=\"pun\">(</span><span class=\"pln\"></span><span class=\"str\">\"order_customer_id\"</span><span class=\"pun\">).</span><span class=\"pln\">count</span></li><li class=\"L2\"><span class=\"pln\">&nbsp;</span></li><li class=\"L3\"><span class=\"com\">//Joining groupedDF created in last step with customer dataframe to get customer details</span></li><li class=\"L4\"><span class=\"pln\">val result </span><span class=\"pun\">=</span><span class=\"pln\"> groupDF</span><span class=\"pun\">.</span><span class=\"pln\">join</span><span class=\"pun\">(</span><span class=\"pln\">cus</span><span class=\"pun\">,</span><span class=\"pln\"> </span><span class=\"str\">\"order_customer_id\"</span><span class=\"pln\"> </span><span class=\"pun\">===</span><span class=\"pln\"> </span><span class=\"str\">\"customer_id\"</span><span class=\"pun\">).</span><span class=\"kwd\">select</span><span class=\"pun\">(</span><span class=\"pln\"></span><span class=\"str\">\"customer_fname\"</span><span class=\"pun\">,</span><span class=\"pln\"></span><span class=\"str\">\"customer_lname\"</span><span class=\"pun\">,</span><span class=\"pln\"></span><span class=\"str\">\"count\"</span><span class=\"pun\">,</span><span class=\"pln\"></span><span class=\"str\">\"customer_state\"</span><span class=\"pun\">)</span></li><li class=\"L5\"><span class=\"pln\">&nbsp;</span></li><li class=\"L6\"><span class=\"com\">//Setting spark configuration to enable Hive partitioning</span></li><li class=\"L7\"><span class=\"pln\">spark</span><span class=\"pun\">.</span><span class=\"pln\">sqlContext</span><span class=\"pun\">.</span><span class=\"pln\">setConf</span><span class=\"pun\">(</span><span class=\"str\">\"hive.exec.dynamic.partition\"</span><span class=\"pun\">,</span><span class=\"pln\"> </span><span class=\"str\">\"true\"</span><span class=\"pun\">)</span></li><li class=\"L8\"><span class=\"pln\">spark</span><span class=\"pun\">.</span><span class=\"pln\">sqlContext</span><span class=\"pun\">.</span><span class=\"pln\">setConf</span><span class=\"pun\">(</span><span class=\"str\">\"hive.exec.dynamic.partition.mode\"</span><span class=\"pun\">,</span><span class=\"pln\"> </span><span class=\"str\">\"nonstrict\"</span><span class=\"pun\">)</span></li><li class=\"L9\"><span class=\"pln\">&nbsp;</span></li><li class=\"L0\"><span class=\"com\">// Create a Hive partitioned table using DataFrame API and partition by customer_state</span></li><li class=\"L1\"><span class=\"pln\">result</span><span class=\"pun\">.</span><span class=\"pln\">write</span><span class=\"pun\">.</span><span class=\"pln\">partitionBy</span><span class=\"pun\">(</span><span class=\"str\">\"customer_state\"</span><span class=\"pun\">).</span><span class=\"pln\">format</span><span class=\"pun\">(</span><span class=\"str\">\"hive\"</span><span class=\"pun\">).</span><span class=\"pln\">saveAsTable</span><span class=\"pun\">(</span><span class=\"str\">\"customer_order\"</span><span class=\"pun\">)</span></li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
