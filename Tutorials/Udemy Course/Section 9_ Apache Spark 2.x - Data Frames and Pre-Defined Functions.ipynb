{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9: Apache Spark 2.x - Data Frames and Pre-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Frames - Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://192.168.1.109:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read a file into to dataframe\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xb021ca10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF = spark.read.csv(\"/user/pi/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ordersDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='1', _c1='2013-07-25 00:00:00.0', _c2='11599', _c3='CLOSED')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|                 _c1|\n",
      "+---+--------------------+\n",
      "|  1|2013-07-25 00:00:...|\n",
      "|  2|2013-07-25 00:00:...|\n",
      "|  3|2013-07-25 00:00:...|\n",
      "|  4|2013-07-25 00:00:...|\n",
      "|  5|2013-07-25 00:00:...|\n",
      "|  6|2013-07-25 00:00:...|\n",
      "|  7|2013-07-25 00:00:...|\n",
      "|  8|2013-07-25 00:00:...|\n",
      "|  9|2013-07-25 00:00:...|\n",
      "| 10|2013-07-25 00:00:...|\n",
      "| 11|2013-07-25 00:00:...|\n",
      "| 12|2013-07-25 00:00:...|\n",
      "| 13|2013-07-25 00:00:...|\n",
      "| 14|2013-07-25 00:00:...|\n",
      "| 15|2013-07-25 00:00:...|\n",
      "| 16|2013-07-25 00:00:...|\n",
      "| 17|2013-07-25 00:00:...|\n",
      "| 18|2013-07-25 00:00:...|\n",
      "| 19|2013-07-25 00:00:...|\n",
      "| 20|2013-07-25 00:00:...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.select('_c0','_c1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Data Frames is nothing but RDD with structure.</p>\n",
    "<ul>\n",
    "<li>Data Frame can be created on any data set which have structure associated with it.</li>\n",
    "<li>Attributes/columns in a data frame can be referred using names.</li>\n",
    "<li>One can create data frame using data from files, hive tables, relational tables over JDBC.</li>\n",
    "<li>Common functions on Data Frames\n",
    "<ul>\n",
    "<li>printSchema – to print the column names and data types of data frame</li>\n",
    "<li>show – to preview data (default 20 records)</li>\n",
    "<li>describe – to understand characteristics of data</li>\n",
    "<li>count – to get number of records</li>\n",
    "<li>collect – to convert data frame into Array</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Once data frame is created, we can process data using 2 approaches.\n",
    "<ul>\n",
    "<li>Native Data Frame APIs</li>\n",
    "<li>Register as temp table and run queries using spark.sql</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>To work with Data Frames as well as Spark SQL, we need to create object of type SparkSession</li>\n",
    "</ul>\n",
    "\n",
    "***\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    master('local'). \\\n",
    "    appName('Create Dataframe over JDBC'). \\\n",
    "    getOrCreate()\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to see the Dataframe look like - structure\n",
    "ordersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "|_c0|                 _c1|  _c2|            _c3|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
      "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
      "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
      "|  6|2013-07-25 00:00:...| 7130|       COMPLETE|\n",
      "|  7|2013-07-25 00:00:...| 4530|       COMPLETE|\n",
      "|  8|2013-07-25 00:00:...| 2911|     PROCESSING|\n",
      "|  9|2013-07-25 00:00:...| 5657|PENDING_PAYMENT|\n",
      "| 10|2013-07-25 00:00:...| 5648|PENDING_PAYMENT|\n",
      "| 11|2013-07-25 00:00:...|  918| PAYMENT_REVIEW|\n",
      "| 12|2013-07-25 00:00:...| 1837|         CLOSED|\n",
      "| 13|2013-07-25 00:00:...| 9149|PENDING_PAYMENT|\n",
      "| 14|2013-07-25 00:00:...| 9842|     PROCESSING|\n",
      "| 15|2013-07-25 00:00:...| 2568|       COMPLETE|\n",
      "| 16|2013-07-25 00:00:...| 7276|PENDING_PAYMENT|\n",
      "| 17|2013-07-25 00:00:...| 2667|       COMPLETE|\n",
      "| 18|2013-07-25 00:00:...| 1205|         CLOSED|\n",
      "| 19|2013-07-25 00:00:...| 9488|PENDING_PAYMENT|\n",
      "| 20|2013-07-25 00:00:...| 9198|     PROCESSING|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to preview data\n",
    "ordersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-----+---------------+\n",
      "|_c0|_c1                  |_c2  |_c3            |\n",
      "+---+---------------------+-----+---------------+\n",
      "|1  |2013-07-25 00:00:00.0|11599|CLOSED         |\n",
      "|2  |2013-07-25 00:00:00.0|256  |PENDING_PAYMENT|\n",
      "|3  |2013-07-25 00:00:00.0|12111|COMPLETE       |\n",
      "|4  |2013-07-25 00:00:00.0|8827 |CLOSED         |\n",
      "|5  |2013-07-25 00:00:00.0|11318|COMPLETE       |\n",
      "|6  |2013-07-25 00:00:00.0|7130 |COMPLETE       |\n",
      "|7  |2013-07-25 00:00:00.0|4530 |COMPLETE       |\n",
      "|8  |2013-07-25 00:00:00.0|2911 |PROCESSING     |\n",
      "|9  |2013-07-25 00:00:00.0|5657 |PENDING_PAYMENT|\n",
      "|10 |2013-07-25 00:00:00.0|5648 |PENDING_PAYMENT|\n",
      "|11 |2013-07-25 00:00:00.0|918  |PAYMENT_REVIEW |\n",
      "|12 |2013-07-25 00:00:00.0|1837 |CLOSED         |\n",
      "|13 |2013-07-25 00:00:00.0|9149 |PENDING_PAYMENT|\n",
      "|14 |2013-07-25 00:00:00.0|9842 |PROCESSING     |\n",
      "|15 |2013-07-25 00:00:00.0|2568 |COMPLETE       |\n",
      "|16 |2013-07-25 00:00:00.0|7276 |PENDING_PAYMENT|\n",
      "|17 |2013-07-25 00:00:00.0|2667 |COMPLETE       |\n",
      "|18 |2013-07-25 00:00:00.0|1205 |CLOSED         |\n",
      "|19 |2013-07-25 00:00:00.0|9488 |PENDING_PAYMENT|\n",
      "|20 |2013-07-25 00:00:00.0|9198 |PROCESSING     |\n",
      "+---+---------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to preview data - without data truncated\n",
    "ordersDF.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-----------------+---------------+\n",
      "|summary|               _c0|                 _c1|              _c2|            _c3|\n",
      "+-------+------------------+--------------------+-----------------+---------------+\n",
      "|  count|             68883|               68883|            68883|          68883|\n",
      "|   mean|           34442.0|                null|6216.571098819738|           null|\n",
      "| stddev|19884.953633337947|                null|3586.205241263963|           null|\n",
      "|    min|                 1|2013-07-25 00:00:...|                1|       CANCELED|\n",
      "|    max|              9999|2014-07-24 00:00:...|             9999|SUSPECTED_FRAUD|\n",
      "+-------+------------------+--------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to check teh characteristics of the data\n",
    "ordersDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get no of records in DF\n",
    "ordersDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get no of records in DF\n",
    "ordersDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert dataframe to python collection\n",
    "ordersLst=ordersDF.collect()\n",
    "type(ordersLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a json file\n",
    "ordersDF = spark.read.json(\"/user/pi/retail_db_json/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|                 _c1|\n",
      "+---+--------------------+\n",
      "|  1|2013-07-25 00:00:...|\n",
      "|  2|2013-07-25 00:00:...|\n",
      "|  3|2013-07-25 00:00:...|\n",
      "|  4|2013-07-25 00:00:...|\n",
      "|  5|2013-07-25 00:00:...|\n",
      "|  6|2013-07-25 00:00:...|\n",
      "|  7|2013-07-25 00:00:...|\n",
      "|  8|2013-07-25 00:00:...|\n",
      "|  9|2013-07-25 00:00:...|\n",
      "| 10|2013-07-25 00:00:...|\n",
      "| 11|2013-07-25 00:00:...|\n",
      "| 12|2013-07-25 00:00:...|\n",
      "| 13|2013-07-25 00:00:...|\n",
      "| 14|2013-07-25 00:00:...|\n",
      "| 15|2013-07-25 00:00:...|\n",
      "| 16|2013-07-25 00:00:...|\n",
      "| 17|2013-07-25 00:00:...|\n",
      "| 18|2013-07-25 00:00:...|\n",
      "| 19|2013-07-25 00:00:...|\n",
      "| 20|2013-07-25 00:00:...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.select(\"_c0\",\"_c1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data in form of SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.createTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "|_c0|                 _c1|  _c2|            _c3|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
      "|  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
      "|  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
      "|  6|2013-07-25 00:00:...| 7130|       COMPLETE|\n",
      "|  7|2013-07-25 00:00:...| 4530|       COMPLETE|\n",
      "|  8|2013-07-25 00:00:...| 2911|     PROCESSING|\n",
      "|  9|2013-07-25 00:00:...| 5657|PENDING_PAYMENT|\n",
      "| 10|2013-07-25 00:00:...| 5648|PENDING_PAYMENT|\n",
      "| 11|2013-07-25 00:00:...|  918| PAYMENT_REVIEW|\n",
      "| 12|2013-07-25 00:00:...| 1837|         CLOSED|\n",
      "| 13|2013-07-25 00:00:...| 9149|PENDING_PAYMENT|\n",
      "| 14|2013-07-25 00:00:...| 9842|     PROCESSING|\n",
      "| 15|2013-07-25 00:00:...| 2568|       COMPLETE|\n",
      "| 16|2013-07-25 00:00:...| 7276|PENDING_PAYMENT|\n",
      "| 17|2013-07-25 00:00:...| 2667|       COMPLETE|\n",
      "| 18|2013-07-25 00:00:...| 1205|         CLOSED|\n",
      "| 19|2013-07-25 00:00:...| 9488|PENDING_PAYMENT|\n",
      "| 20|2013-07-25 00:00:...| 9198|     PROCESSING|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Frames from Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let us see how we can read text data from files into data frame. spark.read also have APIs for other types of file formats, but we will get into those details later.</p>\n",
    "<ul>\n",
    "<li>We can use spark.read.csv or spark.read.text to read text data.</li>\n",
    "<li>spark.read.csv can be used for comma separated data. Default field names will be in the form of _c0, _c1 etc</li>\n",
    "<li>spark.read.text can be used to read fixed length data where there is no delimiter. Default field name is value.</li>\n",
    "<li>We can define attribute names using toDF function</li>\n",
    "<li>In either of the case data will be represented as strings</li>\n",
    "<li>We can covert data types by using cast function –  <code>df.select(df.field.cast(IntegerType()))</code>\n",
    "</li>\n",
    "<li>We will see all other functions soon, but let us perform the task of reading the data into data frame and represent it in their original format.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xb021ca10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"Reading Data from Text Files\").\\\n",
    "        master(\"spark://192.168.1.109:7077\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reading Data from Text Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xae86d4d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from text file using spark.read.csv\n",
    "orderDF = spark.read.csv(\"/user/pi/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='1', _c1='2013-07-25 00:00:00.0', _c2='11599', _c3='CLOSED')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview first record\n",
    "orderDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-----+---------------+\n",
      "|_c0|_c1                  |_c2  |_c3            |\n",
      "+---+---------------------+-----+---------------+\n",
      "|1  |2013-07-25 00:00:00.0|11599|CLOSED         |\n",
      "|2  |2013-07-25 00:00:00.0|256  |PENDING_PAYMENT|\n",
      "|3  |2013-07-25 00:00:00.0|12111|COMPLETE       |\n",
      "|4  |2013-07-25 00:00:00.0|8827 |CLOSED         |\n",
      "|5  |2013-07-25 00:00:00.0|11318|COMPLETE       |\n",
      "|6  |2013-07-25 00:00:00.0|7130 |COMPLETE       |\n",
      "|7  |2013-07-25 00:00:00.0|4530 |COMPLETE       |\n",
      "|8  |2013-07-25 00:00:00.0|2911 |PROCESSING     |\n",
      "|9  |2013-07-25 00:00:00.0|5657 |PENDING_PAYMENT|\n",
      "|10 |2013-07-25 00:00:00.0|5648 |PENDING_PAYMENT|\n",
      "|11 |2013-07-25 00:00:00.0|918  |PAYMENT_REVIEW |\n",
      "|12 |2013-07-25 00:00:00.0|1837 |CLOSED         |\n",
      "|13 |2013-07-25 00:00:00.0|9149 |PENDING_PAYMENT|\n",
      "|14 |2013-07-25 00:00:00.0|9842 |PROCESSING     |\n",
      "|15 |2013-07-25 00:00:00.0|2568 |COMPLETE       |\n",
      "|16 |2013-07-25 00:00:00.0|7276 |PENDING_PAYMENT|\n",
      "|17 |2013-07-25 00:00:00.0|2667 |COMPLETE       |\n",
      "|18 |2013-07-25 00:00:00.0|1205 |CLOSED         |\n",
      "|19 |2013-07-25 00:00:00.0|9488 |PENDING_PAYMENT|\n",
      "|20 |2013-07-25 00:00:00.0|9198 |PROCESSING     |\n",
      "+---+---------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preview 20 records\n",
    "orderDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#preview Schema\n",
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set column name without type cast\n",
    "orderDF = spark.read.csv(\"/user/pi/retail_db/orders\"). \\\n",
    "        toDF('order_id','order_date','customer_id','order_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set column name with type cast\n",
    "orderDF = spark.read.csv(\"/user/pi/retail_db/orders\",sep=',', \\\n",
    "                         schema='order_id int,order_Date string,customer_id int,order_status string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_Date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.printSchema()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------+---------------+\n",
      "|order_id|order_date           |customer_id|order_status   |\n",
      "+--------+---------------------+-----------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599      |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256        |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111      |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827       |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318      |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130       |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530       |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911       |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657       |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648       |PENDING_PAYMENT|\n",
      "+--------+---------------------+-----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tesxt file using spark.read.format\n",
    "orderDF = spark. \\\n",
    "            read. \\\n",
    "            format('csv'). \\\n",
    "            option('sep',','). \\\n",
    "            schema('order_id int,order_Date string,customer_id int,order_status string'). \\\n",
    "            load(\"/user/pi/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_Date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_Date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Different methods of type cast\n",
    "orderDF = spark.read.csv(\"/user/pi/retail_db/orders\").toDF('order_id','order_date','customer_id','order_status')\n",
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "orderDF_1=orderDF.select(orderDF.order_id.cast(\"int\"),orderDF.order_date,orderDF.customer_id.cast(IntegerType()),orderDF.order_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using with column - when we have to typecast specific columns only\n",
    "orderDF = spark.read.csv(\"/user/pi/retail_db/orders\").toDF('order_id','order_date','customer_id','order_status')\n",
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF_2=orderDF.withColumn('order_id',orderDF.order_id.cast(IntegerType())). \\\n",
    "       withColumn('customer_id',orderDF.customer_id.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF_2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REad fixed length data - spark.reaqd.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderDF = spark.read.text(\"/user/pi/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|value                                        |\n",
      "+---------------------------------------------+\n",
      "|1,2013-07-25 00:00:00.0,11599,CLOSED         |\n",
      "|2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT  |\n",
      "|3,2013-07-25 00:00:00.0,12111,COMPLETE       |\n",
      "|4,2013-07-25 00:00:00.0,8827,CLOSED          |\n",
      "|5,2013-07-25 00:00:00.0,11318,COMPLETE       |\n",
      "|6,2013-07-25 00:00:00.0,7130,COMPLETE        |\n",
      "|7,2013-07-25 00:00:00.0,4530,COMPLETE        |\n",
      "|8,2013-07-25 00:00:00.0,2911,PROCESSING      |\n",
      "|9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT |\n",
      "|10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT|\n",
      "|11,2013-07-25 00:00:00.0,918,PAYMENT_REVIEW  |\n",
      "|12,2013-07-25 00:00:00.0,1837,CLOSED         |\n",
      "|13,2013-07-25 00:00:00.0,9149,PENDING_PAYMENT|\n",
      "|14,2013-07-25 00:00:00.0,9842,PROCESSING     |\n",
      "|15,2013-07-25 00:00:00.0,2568,COMPLETE       |\n",
      "|16,2013-07-25 00:00:00.0,7276,PENDING_PAYMENT|\n",
      "|17,2013-07-25 00:00:00.0,2667,COMPLETE       |\n",
      "|18,2013-07-25 00:00:00.0,1205,CLOSED         |\n",
      "|19,2013-07-25 00:00:00.0,9488,PENDING_PAYMENT|\n",
      "|20,2013-07-25 00:00:00.0,9198,PROCESSING     |\n",
      "+---------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Frames from Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://192.168.1.109:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xb025ca10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If Hive and Spark are integrated, we can create data frames from data in Hive tables or run Spark SQL queries against it.</p>\n",
    "<ul>\n",
    "<li>We can use spark.read.table to read data from Hive tables into Data Frame</li>\n",
    "<li>We can prefix database name to table name while reading Hive tables into Data Frame</li>\n",
    "<li>We can also run Hive queries directly using spark.sql</li>\n",
    "<li>Both spark.read.table and spark.sql returns Data Frame</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       databaseName|\n",
      "+-------------------+\n",
      "|ameen_daily_revenue|\n",
      "|            default|\n",
      "|      retail_db_txt|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+\n",
      "|     database|      tableName|isTemporary|\n",
      "+-------------+---------------+-----------+\n",
      "|retail_db_txt|      customers|      false|\n",
      "|retail_db_txt|daily_revenue_2|      false|\n",
      "|retail_db_txt|    order_items|      false|\n",
      "|retail_db_txt|         orders|      false|\n",
      "+-------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use retail_db_txt\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read data from Hive table\n",
    "orderDF=spark.read.table(\"retail_db_txt.orders\")\n",
    "orderDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#using spark sql\n",
    "orderDF = spark.sql(\"select * from retail_db_txt.orders\")\n",
    "orderDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Frames using JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Spark also facilitate us to read data from relational databases over JDBC.</p>\n",
    "<ul>\n",
    "<li>We need to make sure jdbc jar file is registered using  <code>--packages</code>  or  <code>--jars</code>  and  <code>--driver-class-path</code>  while launching pyspark</li>\n",
    "<li>In Pycharm, we need to copy relevant jdbc jar file to SPARK_HOME/jars</li>\n",
    "<li>We can either use spark.read.format(‘jdbc’) with options or spark.read.jdbc with jdbc url, table name and other properties as dict to read data from remote relational databases.</li>\n",
    "<li>We can pass a table name or query to read data using JDBC into Data Frame</li>\n",
    "<li>While reading data, we can define number of partitions (using numPartitions), criteria to divide data into partitions (partitionColumn, lowerBound, upperBound)</li>\n",
    "<li>Partitioning can be done only on numeric fields</li>\n",
    "<li>If lowerBound and upperBound is specified, it will generate strides depending up on number of partitions and then process entire data. Here is the example\n",
    "<ul>\n",
    "<li>We are trying to read order_items data with 4 as numPartitions</li>\n",
    "<li>partitionColumn – order_item_order_id</li>\n",
    "<li>lowerBound – 10000</li>\n",
    "<li>upperBound – 20000</li>\n",
    "<li>order_item_order_id is in the range of 1 and 68883</li>\n",
    "<li>But as we define lowerBound as 10000 and upperBound as 20000, here will be strides – 1 to 12499, 12500 to 14999, 15000 to 17499, 17500 to maximum of order_item_order_id</li>\n",
    "<li>You can check the data in the output path mentioned</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a DB user in mysql with privileges\n",
    "$sudo mysql -u root -p\n",
    ":root\n",
    "\n",
    "CREATE USER 'mysql'@'localhost' IDENTIFIED BY 'mysql';\n",
    "GRANT ALL PRIVILEGES ON *.* TO 'mysql'@'localhost' WITH GRANT OPTION;\n",
    "CREATE USER 'mysql'@'%' IDENTIFIED BY 'mysql';\n",
    "GRANT ALL PRIVILEGES ON *.* TO 'mysql'@'%' WITH GRANT OPTION;\n",
    "FLUSH PRIVILEGES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "pyspark --master yarn --conf spark.ui.port=12121 --jars /home/pi/jars/mysql-connector-java.jar \\\\\n",
    "    --driver-class-path /home/pi/jars/mysql-connector-java.jar\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method jdbc in module pyspark.sql.readwriter:\n",
      "\n",
      "jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Construct a :class:`DataFrame` representing the database table named ``table``\n",
      "    accessible via JDBC URL ``url`` and connection ``properties``.\n",
      "    \n",
      "    Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      "    ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\n",
      "    is needed when ``column`` is specified.\n",
      "    \n",
      "    If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      "    \n",
      "    .. note:: Don't create too many partitions in parallel on a large cluster;\n",
      "        otherwise Spark might crash your external database systems.\n",
      "    \n",
      "    :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      "    :param table: the name of the table\n",
      "    :param column: the name of a column of numeric, date, or timestamp type\n",
      "                   that will be used for partitioning;\n",
      "                   if this parameter is specified, then ``numPartitions``, ``lowerBound``\n",
      "                   (inclusive), and ``upperBound`` (exclusive) will form partition strides\n",
      "                   for generated WHERE clause expressions used to split the column\n",
      "                   ``column`` evenly\n",
      "    :param lowerBound: the minimum value of ``column`` used to decide partition stride\n",
      "    :param upperBound: the maximum value of ``column`` used to decide partition stride\n",
      "    :param numPartitions: the number of partitions\n",
      "    :param predicates: a list of expressions suitable for inclusion in WHERE clauses;\n",
      "                       each one defines one partition of the :class:`DataFrame`\n",
      "    :param properties: a dictionary of JDBC database connection arguments. Normally at\n",
      "                       least properties \"user\" and \"password\" with their corresponding values.\n",
      "                       For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "    :return: a DataFrame\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to connect to jdbc\n",
    "help(spark.read.jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user spark.read.format\n",
    "order_items = spark.read.\\\n",
    "                format('jdbc'). \\\n",
    "                option('url','jdbc:mysql://raspberrypi1:3306'). \\\n",
    "                option('dbtable','retail_db.order_items'). \\\n",
    "                option('user','mysql'). \\\n",
    "                option('password','mysql'). \\\n",
    "                load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.jdbc\n",
    "order_items_jdbc = spark.read.jdbc('jdbc:mysql://raspberrypi1:3306','retail_db.order_items', \\\n",
    "                                  properties={'user':'mysql','password':'mysql'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items_jdbc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To define no:of threads for processing*\n",
    "    - numPartitions\n",
    "    - partitionColumn -- partition column - partition can be done only on numeric fields\n",
    "    - lowerBound\n",
    "    - upperBound\n",
    "\n",
    "<ul>\n",
    "<li>We are trying to read order_items data with 4 as numPartitions</li>\n",
    "<li>partitionColumn – order_item_order_id</li>\n",
    "<li>lowerBound – 10000</li>\n",
    "<li>upperBound – 20000</li>\n",
    "<li>order_item_order_id is in the range of 1 and 68883</li>\n",
    "<li>But as we define lowerBound as 10000 and upperBound as 20000, here will be strides – 1 to 12499, 12500 to 14999, 15000 to 17499, 17500 to maximum of order_item_order_id</li>\n",
    "<li>You can check the data in the output path mentioned</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user spark.read.format\n",
    "order_items = spark.read.\\\n",
    "                format('jdbc'). \\\n",
    "                option('url','jdbc:mysql://raspberrypi1:3306'). \\\n",
    "                option('dbtable','retail_db.order_items'). \\\n",
    "                option('user','mysql'). \\\n",
    "                option('password','mysql'). \\\n",
    "                option('partitionColumn','order_item_order_id'). \\\n",
    "                option('lowerBound',10000). \\\n",
    "                option('upperBound',20000). \\\n",
    "                option('numPartitions',4). \\\n",
    "                load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv file\n",
    "order_items.write.csv(\"/user/pi/retail_db_csv/order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 19:48:05,098 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   2 pi supergroup          0 2020-05-31 19:47 /user/pi/retail_db_csv/order_items/_SUCCESS\n",
      "-rw-r--r--   2 pi supergroup     936524 2020-05-31 19:47 /user/pi/retail_db_csv/order_items/part-00000-904cca22-2ea2-4103-903e-f7cdcdbdc0b4-c000.csv\n",
      "-rw-r--r--   2 pi supergroup     196316 2020-05-31 19:47 /user/pi/retail_db_csv/order_items/part-00001-904cca22-2ea2-4103-903e-f7cdcdbdc0b4-c000.csv\n",
      "-rw-r--r--   2 pi supergroup     192627 2020-05-31 19:47 /user/pi/retail_db_csv/order_items/part-00002-904cca22-2ea2-4103-903e-f7cdcdbdc0b4-c000.csv\n",
      "-rw-r--r--   2 pi supergroup    4083413 2020-05-31 19:47 /user/pi/retail_db_csv/order_items/part-00003-904cca22-2ea2-4103-903e-f7cdcdbdc0b4-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/pi/retail_db_csv/order_items/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Even though we have mentioned lower bound as 10000, and numPartition as 4 , and upperBound as 20000 , first it will take the diffrence between upper and lower and divide by 4.So each junk will have 2500 , but for the first partition it will consider from 0 to 10000+2500 and for the last from 17500 till the max order id.This is the reason why the last file have higher size*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Operations - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let us see overview about Data Frame Operations. It is one of the 2 ways we can process Data Frames.</p>\n",
    "<ul>\n",
    "<li>Selection or Projection – select</li>\n",
    "<li>Filtering data – filter or where</li>\n",
    "<li>Joins – join (supports outer join as well)</li>\n",
    "<li>Aggregations – groupBy and agg with support of functions such as sum, avg, min, max etc</li>\n",
    "<li>Sorting – sort or orderBy</li>\n",
    "<li>Analytics Functions – aggregations, ranking and windowing functions</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can also use Spark SQL to process data in data frames.</p>\n",
    "<ul>\n",
    "<li>We can get list of tables by using  <code>spark.sql('show tables')</code>\n",
    "</li>\n",
    "<li>We can register data frame as temporary view  <code>df.createTempView(\"view_name\")</code>\n",
    "</li>\n",
    "<li>Output of show tables show the temporary tables as well</li>\n",
    "<li>Once temp view is created, we can use SQL style syntax and run queries against the tables/views</li>\n",
    "<li>Most of the hive queries will work out of the box</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Functions to manipulate data in Data Frame fields or columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let us quickly look into some of the functions available in Data Frames.</p>\n",
    "<ul>\n",
    "<li>Main package for functions pyspark.sql.functions</li>\n",
    "<li>We can import by saying  <code>from pyspark.sql import functions as sf</code>\n",
    "</li>\n",
    "<li>You will see many functions which are similar to the functions in traditional databases.</li>\n",
    "<li>These can be categorized into\n",
    "<ul>\n",
    "<li>String manipulation</li>\n",
    "<li>Date manipulation</li>\n",
    "<li>Type casting</li>\n",
    "<li>Expressions such as case when</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>We will see some of the functions in action\n",
    "<ul>\n",
    "<li>substring</li>\n",
    "<li>lower, upper</li>\n",
    "<li>trim</li>\n",
    "<li>date_format</li>\n",
    "<li>trunc</li>\n",
    "<li>Type Casting</li>\n",
    "<li>case when</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            function|\n",
      "+--------------------+\n",
      "|                   !|\n",
      "|                   %|\n",
      "|                   &|\n",
      "|                   *|\n",
      "|                   +|\n",
      "|                   -|\n",
      "|                   /|\n",
      "|                   <|\n",
      "|                  <=|\n",
      "|                 <=>|\n",
      "|                   =|\n",
      "|                  ==|\n",
      "|                   >|\n",
      "|                  >=|\n",
      "|                   ^|\n",
      "|                 abs|\n",
      "|                acos|\n",
      "|          add_months|\n",
      "|           aggregate|\n",
      "|                 and|\n",
      "|approx_count_dist...|\n",
      "|   approx_percentile|\n",
      "|               array|\n",
      "|      array_contains|\n",
      "|      array_distinct|\n",
      "|        array_except|\n",
      "|     array_intersect|\n",
      "|          array_join|\n",
      "|           array_max|\n",
      "|           array_min|\n",
      "|      array_position|\n",
      "|        array_remove|\n",
      "|        array_repeat|\n",
      "|          array_sort|\n",
      "|         array_union|\n",
      "|      arrays_overlap|\n",
      "|          arrays_zip|\n",
      "|               ascii|\n",
      "|                asin|\n",
      "|         assert_true|\n",
      "|                atan|\n",
      "|               atan2|\n",
      "|                 avg|\n",
      "|              base64|\n",
      "|              bigint|\n",
      "|                 bin|\n",
      "|              binary|\n",
      "|          bit_length|\n",
      "|             boolean|\n",
      "|              bround|\n",
      "|         cardinality|\n",
      "|                cast|\n",
      "|                cbrt|\n",
      "|                ceil|\n",
      "|             ceiling|\n",
      "|                char|\n",
      "|         char_length|\n",
      "|    character_length|\n",
      "|                 chr|\n",
      "|            coalesce|\n",
      "|        collect_list|\n",
      "|         collect_set|\n",
      "|              concat|\n",
      "|           concat_ws|\n",
      "|                conv|\n",
      "|                corr|\n",
      "|                 cos|\n",
      "|                cosh|\n",
      "|                 cot|\n",
      "|               count|\n",
      "|    count_min_sketch|\n",
      "|           covar_pop|\n",
      "|          covar_samp|\n",
      "|               crc32|\n",
      "|                cube|\n",
      "|           cume_dist|\n",
      "|    current_database|\n",
      "|        current_date|\n",
      "|   current_timestamp|\n",
      "|                date|\n",
      "|            date_add|\n",
      "|         date_format|\n",
      "|            date_sub|\n",
      "|          date_trunc|\n",
      "|            datediff|\n",
      "|                 day|\n",
      "|          dayofmonth|\n",
      "|           dayofweek|\n",
      "|           dayofyear|\n",
      "|             decimal|\n",
      "|              decode|\n",
      "|             degrees|\n",
      "|          dense_rank|\n",
      "|              double|\n",
      "|                   e|\n",
      "|          element_at|\n",
      "|                 elt|\n",
      "|              encode|\n",
      "|              exists|\n",
      "|                 exp|\n",
      "|             explode|\n",
      "|       explode_outer|\n",
      "|               expm1|\n",
      "|           factorial|\n",
      "|              filter|\n",
      "|         find_in_set|\n",
      "|               first|\n",
      "|         first_value|\n",
      "|             flatten|\n",
      "|               float|\n",
      "|               floor|\n",
      "|       format_number|\n",
      "|       format_string|\n",
      "|           from_json|\n",
      "|       from_unixtime|\n",
      "|  from_utc_timestamp|\n",
      "|     get_json_object|\n",
      "|            greatest|\n",
      "|            grouping|\n",
      "|         grouping_id|\n",
      "|                hash|\n",
      "|                 hex|\n",
      "|                hour|\n",
      "|               hypot|\n",
      "|                  if|\n",
      "|              ifnull|\n",
      "|                  in|\n",
      "|             initcap|\n",
      "|              inline|\n",
      "|        inline_outer|\n",
      "|input_file_block_...|\n",
      "|input_file_block_...|\n",
      "|     input_file_name|\n",
      "|               instr|\n",
      "|                 int|\n",
      "|               isnan|\n",
      "|           isnotnull|\n",
      "|              isnull|\n",
      "|         java_method|\n",
      "|          json_tuple|\n",
      "|            kurtosis|\n",
      "|                 lag|\n",
      "|                last|\n",
      "|            last_day|\n",
      "|          last_value|\n",
      "|               lcase|\n",
      "|                lead|\n",
      "|               least|\n",
      "|                left|\n",
      "|              length|\n",
      "|         levenshtein|\n",
      "|                like|\n",
      "|                  ln|\n",
      "|              locate|\n",
      "|                 log|\n",
      "|               log10|\n",
      "|               log1p|\n",
      "|                log2|\n",
      "|               lower|\n",
      "|                lpad|\n",
      "|               ltrim|\n",
      "|                 map|\n",
      "|          map_concat|\n",
      "|     map_from_arrays|\n",
      "|    map_from_entries|\n",
      "|            map_keys|\n",
      "|          map_values|\n",
      "|                 max|\n",
      "|                 md5|\n",
      "|                mean|\n",
      "|                 min|\n",
      "|              minute|\n",
      "|                 mod|\n",
      "|monotonically_inc...|\n",
      "|               month|\n",
      "|      months_between|\n",
      "|        named_struct|\n",
      "|               nanvl|\n",
      "|            negative|\n",
      "|            next_day|\n",
      "|                 not|\n",
      "|                 now|\n",
      "|               ntile|\n",
      "|              nullif|\n",
      "|                 nvl|\n",
      "|                nvl2|\n",
      "|        octet_length|\n",
      "|                  or|\n",
      "|           parse_url|\n",
      "|        percent_rank|\n",
      "|          percentile|\n",
      "|   percentile_approx|\n",
      "|                  pi|\n",
      "|                pmod|\n",
      "|          posexplode|\n",
      "|    posexplode_outer|\n",
      "|            position|\n",
      "|            positive|\n",
      "|                 pow|\n",
      "|               power|\n",
      "|              printf|\n",
      "|             quarter|\n",
      "|             radians|\n",
      "|                rand|\n",
      "|               randn|\n",
      "|                rank|\n",
      "|             reflect|\n",
      "|      regexp_extract|\n",
      "|      regexp_replace|\n",
      "|              repeat|\n",
      "|             replace|\n",
      "|             reverse|\n",
      "|               right|\n",
      "|                rint|\n",
      "|               rlike|\n",
      "|              rollup|\n",
      "|               round|\n",
      "|          row_number|\n",
      "|                rpad|\n",
      "|               rtrim|\n",
      "|      schema_of_json|\n",
      "|              second|\n",
      "|           sentences|\n",
      "|            sequence|\n",
      "|                 sha|\n",
      "|                sha1|\n",
      "|                sha2|\n",
      "|           shiftleft|\n",
      "|          shiftright|\n",
      "|  shiftrightunsigned|\n",
      "|             shuffle|\n",
      "|                sign|\n",
      "|              signum|\n",
      "|                 sin|\n",
      "|                sinh|\n",
      "|                size|\n",
      "|            skewness|\n",
      "|               slice|\n",
      "|            smallint|\n",
      "|          sort_array|\n",
      "|             soundex|\n",
      "|               space|\n",
      "|  spark_partition_id|\n",
      "|               split|\n",
      "|                sqrt|\n",
      "|               stack|\n",
      "|                 std|\n",
      "|              stddev|\n",
      "|          stddev_pop|\n",
      "|         stddev_samp|\n",
      "|          str_to_map|\n",
      "|              string|\n",
      "|              struct|\n",
      "|              substr|\n",
      "|           substring|\n",
      "|     substring_index|\n",
      "|                 sum|\n",
      "|                 tan|\n",
      "|                tanh|\n",
      "|           timestamp|\n",
      "|             tinyint|\n",
      "|             to_date|\n",
      "|             to_json|\n",
      "|        to_timestamp|\n",
      "|   to_unix_timestamp|\n",
      "|    to_utc_timestamp|\n",
      "|           transform|\n",
      "|           translate|\n",
      "|                trim|\n",
      "|               trunc|\n",
      "|               ucase|\n",
      "|            unbase64|\n",
      "|               unhex|\n",
      "|      unix_timestamp|\n",
      "|               upper|\n",
      "|                uuid|\n",
      "|             var_pop|\n",
      "|            var_samp|\n",
      "|            variance|\n",
      "|             weekday|\n",
      "|          weekofyear|\n",
      "|                when|\n",
      "|              window|\n",
      "|               xpath|\n",
      "|       xpath_boolean|\n",
      "|        xpath_double|\n",
      "|         xpath_float|\n",
      "|           xpath_int|\n",
      "|          xpath_long|\n",
      "|        xpath_number|\n",
      "|         xpath_short|\n",
      "|        xpath_string|\n",
      "|                year|\n",
      "|            zip_with|\n",
      "|                   ||\n",
      "|                   ~|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show functions').show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available functions for row level manupulation\n",
    "|1|2|3|4|5|6|\n",
    "|-----|-----|-----|-----|-----|-----|\n",
    "|abs|element_at|posexplode_outer|ceil|least|sqrt|          \n",
    "|acos|encode|pow| coalesce|length|stddev|\n",
    "|add_months|exp|quarter|col|levenshtein|stddev_pop|\n",
    "|approxCountDistinct|explode|radians|  collect_list|lit|stddev_samp|\n",
    "|approx_count_distinct|explode_outer|rand|collect_set|locate|struct|\n",
    "|array|expm1|randn|column|log|substring|\n",
    "|array_contains|expr|rank|concat|log10|substring_index|\n",
    "|array_distinct|factorial|regexp_extract|concat_ws|log1p|sum|\n",
    "|array_except|first|regexp_replace|conv|log2|sumDistinct|\n",
    "|array_intersect|flatten|repeat|corr|lower|tan|\n",
    "|array_join|floor|reverse|cos|lpad|tanh|\n",
    "|array_max|format_number|rint|cosh|ltrim|toDegrees|\n",
    "|array_min|format_string|round|count|map|toRadians|\n",
    "|array_position|from_json|row_number|countDistinct|map_concat|to_date|\n",
    "|array_remove|from_unixtime|rpad|covar_pop|map_from_arrays|to_json|\n",
    "|array_repeat|from_utc_timestamp|rtrim|covar_samp|map_from_entries|to_timestamp|\n",
    "|array_sort|get_json_object|schema_of_json|crc32|map_keys|to_utc_timestamp|\n",
    "|array_union|greatest|second|cume_dist|map_values|translate|\n",
    "|arrays_overlap|grouping|sequence|currentRow|max|trim|\n",
    "|arrays_zip|grouping_id|sha1|current_date|md5|trunc|\n",
    "|asc|hash|sha2|current_timestamp|mean|typedLit|\n",
    "|asc_nulls_first|hex|shiftLeft|date_add|min|udf|\n",
    "|asc_nulls_last|hour|shiftRight|date_format|minute|unbase64|\n",
    "|ascii|hypot|shiftRightUnsigned|date_sub|monotonicallyIncreasingId|unboundedFollowing|\n",
    "|asin|initcap|shuffle|date_trunc|monotonically_increasing_id|unboundedPreceding|\n",
    "|atan|input_file_name|signum|datediff|month|unhex|\n",
    "|atan2|instr|sin|dayofmonth|months_between|unix_timestamp|\n",
    "|avg|isnan|sinh|dayofweek|nanvl|upper|\n",
    "|base64|isnull|size|dayofyear|negate|var_pop|\n",
    "|bin|json_tuple|skewness|decode|next_day|var_samp|\n",
    "|bitwiseNOT|kurtosis|slice|degrees|not|variance|\n",
    "|broadcast|lag|sort_array|dense_rank|ntile|weekofyear|\n",
    "|bround|last|soundex|desc|percent_rank|when|\n",
    "|callUDF|last_day|spark_partition_id|desc_nulls_first|pmod|window|\n",
    "|cbrt|lead|split|desc_nulls_last|posexplode|year|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
