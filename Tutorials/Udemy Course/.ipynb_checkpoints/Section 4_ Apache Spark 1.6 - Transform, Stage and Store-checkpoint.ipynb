{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Apache Core Spark 1.6 - Transform, Stage and Store"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initializing Spark job using pyspark\n",
    "    $pyspark --master yarn --conf spark.ui.port=12888\n",
    "    --> port can be 5 digit number less than 65535.\n",
    "    -->Port not required to mentioned is in local installation\n",
    "    -->pyspark will launch python console with spark modules and top of it ,\n",
    "        it will launch a webserivice called spark context,which is assigned to variable called sc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create RDD\n",
    "    -->RDD is extension to Python list\n",
    "    -->Resilient Distributed Dataset\n",
    "            ->In-Memory\n",
    "            ->Distributed\n",
    "            ->Resilient - Fault tolerant\n",
    "    -->To view Spark API's\n",
    "        $help(sc)\n",
    "    -->Common RDD creating APIs from HDFS file system\n",
    "        1)hadoopFile\n",
    "        2)newAPIHadoopFile\n",
    "        3)sequenceFile\n",
    "        4)textFile  -- most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-17 17:24:33,240 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 9 items\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/.git\n",
      "-rw-r--r--   2 pi supergroup        826 2020-05-11 12:05 /user/pi/retail_db/README.md\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/categories\n",
      "-rw-r--r--   2 pi supergroup   10303495 2020-05-11 12:06 /user/pi/retail_db/create_db.sql\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/customers\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/departments\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:05 /user/pi/retail_db/order_items\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:05 /user/pi/retail_db/orders\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/products\n"
     ]
    }
   ],
   "source": [
    "#Reading file from HDFS\n",
    "! hdfs dfs -ls /user/pi/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,957,1,299.98,299.98\n",
      "2,2,1073,1,199.99,199.99\n",
      "3,2,502,5,250.0,50.0\n",
      "4,2,403,1,129.99,129.99\n",
      "5,4,897,2,49.98,24.99\n",
      "6,4,365,5,299.95,59.99\n",
      "7,4,502,3,150.0,50.0\n",
      "8,4,1014,4,199.92,49.98\n",
      "9,5,957,1,299.98,299.98\n",
      "10,5,365,5,299.95,59.99\n"
     ]
    }
   ],
   "source": [
    "order_items = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "for order in order_items.take(10):\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "->2 Types of APIs available on top of RDD\n",
    "    --> Tranformation  --Transfomation are to do transformation to data\n",
    "    --> Action  --Action are to preview data and save to files\n",
    "->$help(orderItems)   -- the API available are transformations and Actions\n",
    "->DAG(Directed Acyclic Graph) and lazy evaluation\n",
    "    -->Tranformation code will not be executed directly ,information is captured into DAG \n",
    "    -->That will be executed when action is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-299.98\n",
      "4-699.85\n",
      "8-729.84\n",
      "9-599.96\n",
      "10-651.92\n",
      "12-1299.87\n",
      "14-549.94\n",
      "16-419.93\n",
      "17-694.84\n",
      "19-699.96\n"
     ]
    }
   ],
   "source": [
    "#Sample code to demostration\n",
    "#Order Revenue\n",
    "order_items = sc.textFile('/user/pi/retail_db/order_items')\n",
    "order_items_PairRDD = order_items.map(lambda order:(order.split(',')[1],float(order.split(',')[4])))\n",
    "revenue = order_items_PairRDD.reduceByKey(lambda x,y:x+y)\n",
    "for order,rev in revenue.take(10):\n",
    "    print(\"{}-{:.2f}\".format(order,rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[48] at RDD at PythonRDD.scala:53 []\\n |  /user/pi/retail_db/order_items MapPartitionsRDD[42] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  /user/pi/retail_db/order_items HadoopRDD[41] at textFile at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see DAG information\n",
    "order_items_PairRDD .toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', 299.98)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Previewing the data using Actions\n",
    "order_items_PairRDD.take(10) # display first 10 rows\n",
    "order_items_PairRDD.collect()  #convert RDD to python list\n",
    "order_items_PairRDD.count()  #to count number of records\n",
    "order_items_PairRDD.first() # to display firt row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RDD using data from collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a list\n",
    "numList=range(1,100)\n",
    "numListRDD = sc.parallelize(numList) #to convert list to RDD\n",
    "numListRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5452\r\n",
      "-rwxr--r-- 1 smbuser smbuser 5581078 May 11 06:20 part-00000\r\n"
     ]
    }
   ],
   "source": [
    "#Real world use case\n",
    "#Read data from local file system and convert to RDD\n",
    "! ls -l retail_db/order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['1,1,957,1,299.98,299.98', '2,2,1073,1,199.99,199.99', '3,2,502,5,250.0,50.0', '4,2,403,1,129.99,129.99', '5,4,897,2,49.98,24.99', '6,4,365,5,299.95,59.99', '7,4,502,3,150.0,50.0', '8,4,1014,4,199.92,49.98', '9,5,957,1,299.98,299.98', '10,5,365,5,299.95,59.99']\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "order_items = open(\"/home/pi/shared/retail_db/order_items/part-00000\").read().splitlines()\n",
    "print(type(order_items))\n",
    "order_itemsRDD = sc.parallelize(order_items)\n",
    "print(order_itemsRDD.take(10))\n",
    "print(type(order_itemsRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Different File Formats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--> DataFrame - Distributed collection with structure - extension to RDD\n",
    "-->APIs provided by sqlContext for DataFrames\n",
    "-->Supported File formats are\n",
    "        1)orc\n",
    "        2)json\n",
    "        3)parquet\n",
    "        4)avro\n",
    "        5)text\n",
    "-->To preview Data \n",
    "$show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://raspberrypi1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.1.109:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://192.168.1.109:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0xb0266ab0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext  #created in top of sc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "File formats are \n",
    "APIs are :\n",
    "    sqlContext.read.json(<HDFS-path>) or sqlContext.load(<HDFS-path>,\"json\")\n",
    "    sqlContext.read.orc(<HDFS-path>) or sqlContext.load(<HDFS-path>,\"orc\")\n",
    "    sqlContext.read.parquet(<HDFS-path>) or sqlContext.load(<HDFS-path>,\"parquet\")\n",
    "    sqlContext.read.text(<HDFS-path>) or sqlContext.load(<HDFS-path>,\"text\")\n",
    "    \n",
    "--both load and read API are same only differnce in parameters\n",
    "--data created of type spark DataFrame\n",
    "\n",
    "Previewing the data : show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|_corrupt_record|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "|              [|       null|      null|       null|      null|   null|\n",
      "|           null|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|           null|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|           null|        1.3|       0.2|        4.7|       3.2| setosa|\n",
      "|           null|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "|           null|        1.4|       0.2|        5.0|       3.6| setosa|\n",
      "|           null|        1.7|       0.4|        5.4|       3.9| setosa|\n",
      "|           null|        1.4|       0.3|        4.6|       3.4| setosa|\n",
      "|           null|        1.5|       0.2|        5.0|       3.4| setosa|\n",
      "|           null|        1.4|       0.2|        4.4|       2.9| setosa|\n",
      "|           null|        1.5|       0.1|        4.9|       3.1| setosa|\n",
      "|           null|        1.5|       0.2|        5.4|       3.7| setosa|\n",
      "|           null|        1.6|       0.2|        4.8|       3.4| setosa|\n",
      "|           null|        1.4|       0.1|        4.8|       3.0| setosa|\n",
      "|           null|        1.1|       0.1|        4.3|       3.0| setosa|\n",
      "|           null|        1.2|       0.2|        5.8|       4.0| setosa|\n",
      "|           null|        1.5|       0.4|        5.7|       4.4| setosa|\n",
      "|           null|        1.3|       0.4|        5.4|       3.9| setosa|\n",
      "|           null|        1.4|       0.3|        5.1|       3.5| setosa|\n",
      "|           null|        1.7|       0.3|        5.7|       3.8| setosa|\n",
      "+---------------+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read data from json file\n",
    "iris=sqlContext.read.json('/user/pi/iris.json')\n",
    "iris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLContext' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-5995ff9ec5e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miris\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/user/pi/iris.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SQLContext' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "iris=sqlContext.load('/user/pi/iris.json','json')\n",
    "iris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Level Transformations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-->String Manupulations\n",
    "-->Map Transformation\n",
    "-->FlaMap Transformation\n",
    "-->Filter Transformation\n",
    "-->Joins\n",
    "-->Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Manupulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 17:06:04,723 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 9 items\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/.git\n",
      "-rw-r--r--   2 pi supergroup        826 2020-05-11 12:05 /user/pi/retail_db/README.md\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/categories\n",
      "-rw-r--r--   2 pi supergroup   10303495 2020-05-11 12:06 /user/pi/retail_db/create_db.sql\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/customers\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/departments\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:05 /user/pi/retail_db/order_items\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:05 /user/pi/retail_db/orders\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-11 12:06 /user/pi/retail_db/products\n"
     ]
    }
   ],
   "source": [
    "#Action first() converts RDDs first fine to unicode text\n",
    "#read data from HDFS\n",
    "! hdfs dfs -ls /user/pi/retail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = sc.textFile(\"/user/pi/retail_db/orders\")\n",
    "s = orders.first()\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
      "1,2013-07-\n",
      "2013-07-25 00:00:00.0\n",
      "36\n",
      "['1', '2013-07-25 00:00:00.0', '11599', 'CLOSED']\n",
      "2013-07-25 00:00:00.0\n",
      "20130725\n"
     ]
    }
   ],
   "source": [
    "print(s)\n",
    "print(s[:10])\n",
    "print(s[2:23])\n",
    "print(len(s))\n",
    "print(s.split(\",\"))\n",
    "print(s.split(\",\")[1])\n",
    "print(int(s.split(\",\")[1][:10].replace(\"-\",\"\")))\n",
    "##convertion function int(),float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class str in module builtins:\n",
      "\n",
      "class str(object)\n",
      " |  str(object='') -> str\n",
      " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      " |  \n",
      " |  Create a new string object from the given object. If encoding or\n",
      " |  errors is specified, then the object must expose a data buffer\n",
      " |  that will be decoded using the given encoding and error handler.\n",
      " |  Otherwise, returns the result of object.__str__() (if defined)\n",
      " |  or repr(object).\n",
      " |  encoding defaults to sys.getdefaultencoding().\n",
      " |  errors defaults to 'strict'.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(self, format_spec, /)\n",
      " |      Return a formatted version of the string as described by format_spec.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __sizeof__(self, /)\n",
      " |      Return the size of the string in memory, in bytes.\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  capitalize(self, /)\n",
      " |      Return a capitalized version of the string.\n",
      " |      \n",
      " |      More specifically, make the first character have upper case and the rest lower\n",
      " |      case.\n",
      " |  \n",
      " |  casefold(self, /)\n",
      " |      Return a version of the string suitable for caseless comparisons.\n",
      " |  \n",
      " |  center(self, width, fillchar=' ', /)\n",
      " |      Return a centered string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  count(...)\n",
      " |      S.count(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the number of non-overlapping occurrences of substring sub in\n",
      " |      string S[start:end].  Optional arguments start and end are\n",
      " |      interpreted as in slice notation.\n",
      " |  \n",
      " |  encode(self, /, encoding='utf-8', errors='strict')\n",
      " |      Encode the string using the codec registered for encoding.\n",
      " |      \n",
      " |      encoding\n",
      " |        The encoding in which to encode the string.\n",
      " |      errors\n",
      " |        The error handling scheme to use for encoding errors.\n",
      " |        The default is 'strict' meaning that encoding errors raise a\n",
      " |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
      " |        'xmlcharrefreplace' as well as any other name registered with\n",
      " |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
      " |  \n",
      " |  endswith(...)\n",
      " |      S.endswith(suffix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S ends with the specified suffix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      suffix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  expandtabs(self, /, tabsize=8)\n",
      " |      Return a copy where all tab characters are expanded using spaces.\n",
      " |      \n",
      " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      " |  \n",
      " |  find(...)\n",
      " |      S.find(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  format(...)\n",
      " |      S.format(*args, **kwargs) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  format_map(...)\n",
      " |      S.format_map(mapping) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from mapping.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  index(...)\n",
      " |      S.index(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found, \n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  isalnum(self, /)\n",
      " |      Return True if the string is an alpha-numeric string, False otherwise.\n",
      " |      \n",
      " |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
      " |      there is at least one character in the string.\n",
      " |  \n",
      " |  isalpha(self, /)\n",
      " |      Return True if the string is an alphabetic string, False otherwise.\n",
      " |      \n",
      " |      A string is alphabetic if all characters in the string are alphabetic and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  isascii(self, /)\n",
      " |      Return True if all characters in the string are ASCII, False otherwise.\n",
      " |      \n",
      " |      ASCII characters have code points in the range U+0000-U+007F.\n",
      " |      Empty string is ASCII too.\n",
      " |  \n",
      " |  isdecimal(self, /)\n",
      " |      Return True if the string is a decimal string, False otherwise.\n",
      " |      \n",
      " |      A string is a decimal string if all characters in the string are decimal and\n",
      " |      there is at least one character in the string.\n",
      " |  \n",
      " |  isdigit(self, /)\n",
      " |      Return True if the string is a digit string, False otherwise.\n",
      " |      \n",
      " |      A string is a digit string if all characters in the string are digits and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  isidentifier(self, /)\n",
      " |      Return True if the string is a valid Python identifier, False otherwise.\n",
      " |      \n",
      " |      Use keyword.iskeyword() to test for reserved identifiers such as \"def\" and\n",
      " |      \"class\".\n",
      " |  \n",
      " |  islower(self, /)\n",
      " |      Return True if the string is a lowercase string, False otherwise.\n",
      " |      \n",
      " |      A string is lowercase if all cased characters in the string are lowercase and\n",
      " |      there is at least one cased character in the string.\n",
      " |  \n",
      " |  isnumeric(self, /)\n",
      " |      Return True if the string is a numeric string, False otherwise.\n",
      " |      \n",
      " |      A string is numeric if all characters in the string are numeric and there is at\n",
      " |      least one character in the string.\n",
      " |  \n",
      " |  isprintable(self, /)\n",
      " |      Return True if the string is printable, False otherwise.\n",
      " |      \n",
      " |      A string is printable if all of its characters are considered printable in\n",
      " |      repr() or if it is empty.\n",
      " |  \n",
      " |  isspace(self, /)\n",
      " |      Return True if the string is a whitespace string, False otherwise.\n",
      " |      \n",
      " |      A string is whitespace if all characters in the string are whitespace and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  istitle(self, /)\n",
      " |      Return True if the string is a title-cased string, False otherwise.\n",
      " |      \n",
      " |      In a title-cased string, upper- and title-case characters may only\n",
      " |      follow uncased characters and lowercase characters only cased ones.\n",
      " |  \n",
      " |  isupper(self, /)\n",
      " |      Return True if the string is an uppercase string, False otherwise.\n",
      " |      \n",
      " |      A string is uppercase if all cased characters in the string are uppercase and\n",
      " |      there is at least one cased character in the string.\n",
      " |  \n",
      " |  join(self, iterable, /)\n",
      " |      Concatenate any number of strings.\n",
      " |      \n",
      " |      The string whose method is called is inserted in between each given string.\n",
      " |      The result is returned as a new string.\n",
      " |      \n",
      " |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
      " |  \n",
      " |  ljust(self, width, fillchar=' ', /)\n",
      " |      Return a left-justified string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  lower(self, /)\n",
      " |      Return a copy of the string converted to lowercase.\n",
      " |  \n",
      " |  lstrip(self, chars=None, /)\n",
      " |      Return a copy of the string with leading whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  partition(self, sep, /)\n",
      " |      Partition the string into three parts using the given separator.\n",
      " |      \n",
      " |      This will search for the separator in the string.  If the separator is found,\n",
      " |      returns a 3-tuple containing the part before the separator, the separator\n",
      " |      itself, and the part after it.\n",
      " |      \n",
      " |      If the separator is not found, returns a 3-tuple containing the original string\n",
      " |      and two empty strings.\n",
      " |  \n",
      " |  replace(self, old, new, count=-1, /)\n",
      " |      Return a copy with all occurrences of substring old replaced by new.\n",
      " |      \n",
      " |        count\n",
      " |          Maximum number of occurrences to replace.\n",
      " |          -1 (the default value) means replace all occurrences.\n",
      " |      \n",
      " |      If the optional argument count is given, only the first count occurrences are\n",
      " |      replaced.\n",
      " |  \n",
      " |  rfind(...)\n",
      " |      S.rfind(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  rindex(...)\n",
      " |      S.rindex(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  rjust(self, width, fillchar=' ', /)\n",
      " |      Return a right-justified string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  rpartition(self, sep, /)\n",
      " |      Partition the string into three parts using the given separator.\n",
      " |      \n",
      " |      This will search for the separator in the string, starting at the end. If\n",
      " |      the separator is found, returns a 3-tuple containing the part before the\n",
      " |      separator, the separator itself, and the part after it.\n",
      " |      \n",
      " |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
      " |      and the original string.\n",
      " |  \n",
      " |  rsplit(self, /, sep=None, maxsplit=-1)\n",
      " |      Return a list of the words in the string, using sep as the delimiter string.\n",
      " |      \n",
      " |        sep\n",
      " |          The delimiter according which to split the string.\n",
      " |          None (the default value) means split according to any whitespace,\n",
      " |          and discard empty strings from the result.\n",
      " |        maxsplit\n",
      " |          Maximum number of splits to do.\n",
      " |          -1 (the default value) means no limit.\n",
      " |      \n",
      " |      Splits are done starting at the end of the string and working to the front.\n",
      " |  \n",
      " |  rstrip(self, chars=None, /)\n",
      " |      Return a copy of the string with trailing whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  split(self, /, sep=None, maxsplit=-1)\n",
      " |      Return a list of the words in the string, using sep as the delimiter string.\n",
      " |      \n",
      " |      sep\n",
      " |        The delimiter according which to split the string.\n",
      " |        None (the default value) means split according to any whitespace,\n",
      " |        and discard empty strings from the result.\n",
      " |      maxsplit\n",
      " |        Maximum number of splits to do.\n",
      " |        -1 (the default value) means no limit.\n",
      " |  \n",
      " |  splitlines(self, /, keepends=False)\n",
      " |      Return a list of the lines in the string, breaking at line boundaries.\n",
      " |      \n",
      " |      Line breaks are not included in the resulting list unless keepends is given and\n",
      " |      true.\n",
      " |  \n",
      " |  startswith(...)\n",
      " |      S.startswith(prefix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S starts with the specified prefix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      prefix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  strip(self, chars=None, /)\n",
      " |      Return a copy of the string with leading and trailing whitespace remove.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  swapcase(self, /)\n",
      " |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
      " |  \n",
      " |  title(self, /)\n",
      " |      Return a version of the string where each word is titlecased.\n",
      " |      \n",
      " |      More specifically, words start with uppercased characters and all remaining\n",
      " |      cased characters have lower case.\n",
      " |  \n",
      " |  translate(self, table, /)\n",
      " |      Replace each character in the string using the given translation table.\n",
      " |      \n",
      " |        table\n",
      " |          Translation table, which must be a mapping of Unicode ordinals to\n",
      " |          Unicode ordinals, strings, or None.\n",
      " |      \n",
      " |      The table must implement lookup/indexing via __getitem__, for instance a\n",
      " |      dictionary or list.  If this operation raises LookupError, the character is\n",
      " |      left untouched.  Characters mapped to None are deleted.\n",
      " |  \n",
      " |  upper(self, /)\n",
      " |      Return a copy of the string converted to uppercase.\n",
      " |  \n",
      " |  zfill(self, width, /)\n",
      " |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
      " |      \n",
      " |      The string is never truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  maketrans(x, y=None, z=None, /)\n",
      " |      Return a translation table usable for str.translate().\n",
      " |      \n",
      " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      " |      Character keys will be then converted to ordinals.\n",
      " |      If there are two arguments, they must be strings of equal length, and\n",
      " |      in the resulting dictionary, each character in x will be mapped to the\n",
      " |      character at the same position in y. If there is a third argument, it\n",
      " |      must be a string, whose characters will be mapped to None in the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,1,957,1,299.98,299.98',\n",
       " '2,2,1073,1,199.99,199.99',\n",
       " '3,2,502,5,250.0,50.0',\n",
       " '4,2,403,1,129.99,129.99',\n",
       " '5,4,897,2,49.98,24.99',\n",
       " '6,4,365,5,299.95,59.99',\n",
       " '7,4,502,3,150.0,50.0',\n",
       " '8,4,1014,4,199.92,49.98',\n",
       " '9,5,957,1,299.98,299.98',\n",
       " '10,5,365,5,299.95,59.99']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return a new distributed dataset formed by passing each element of the source through a function func.\n",
    "#create a tuple with orderid and revenue\n",
    "order_items = sc.textFile('/user/pi/retail_db/order_items')\n",
    "order_items.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('957', 299.98),\n",
       " ('1073', 199.99),\n",
       " ('502', 250.0),\n",
       " ('403', 129.99),\n",
       " ('897', 49.98),\n",
       " ('365', 299.95),\n",
       " ('502', 150.0),\n",
       " ('1014', 199.92),\n",
       " ('957', 299.98),\n",
       " ('365', 299.95)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderRev = order_items.map(lambda order:(order.split(\",\")[2],float(order.split(\",\")[4])))\n",
    "orderRev.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatMap() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'are', 'you'], ['where', 'are', 'you'], ['how', 'was', 'it']]\n",
      "['how', 'are', 'you', 'where', 'are', 'you', 'how', 'was', 'it']\n"
     ]
    }
   ],
   "source": [
    "#Map function it returns a single output for each elemnt in RDD but flatMap will flatten all elements to form a single collection.\n",
    "wordsList = [\"how are you\",\"where are you\",\"how was it\"]\n",
    "WordsRDD = sc.parallelize(wordsList)\n",
    "#Map\n",
    "WordsMap = WordsRDD.map(lambda line:line.split(\" \"))\n",
    "print(WordsMap.take(10))\n",
    "WordsFlatMap = WordsRDD.flatMap(lambda line:line.split(\" \"))\n",
    "print(WordsFlatMap.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how-2\n",
      "are-2\n",
      "you-2\n",
      "where-1\n",
      "was-1\n",
      "it-1\n"
     ]
    }
   ],
   "source": [
    "wordCnt = WordsFlatMap.countByValue()\n",
    "for word,cnt in wordCnt.items():\n",
    "    print('{}-{}'.format(word,cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter() Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,2013-07-25 00:00:00.0,11599,CLOSED', '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT', '3,2013-07-25 00:00:00.0,12111,COMPLETE', '4,2013-07-25 00:00:00.0,8827,CLOSED', '5,2013-07-25 00:00:00.0,11318,COMPLETE', '6,2013-07-25 00:00:00.0,7130,COMPLETE', '7,2013-07-25 00:00:00.0,4530,COMPLETE', '8,2013-07-25 00:00:00.0,2911,PROCESSING', '9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT', '10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT']\n",
      "2544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['25882,2014-01-01 00:00:00.0,4598,COMPLETE',\n",
       " '25888,2014-01-01 00:00:00.0,6735,COMPLETE',\n",
       " '25889,2014-01-01 00:00:00.0,10045,COMPLETE',\n",
       " '25891,2014-01-01 00:00:00.0,3037,CLOSED',\n",
       " '25895,2014-01-01 00:00:00.0,1044,COMPLETE',\n",
       " '25897,2014-01-01 00:00:00.0,6405,COMPLETE',\n",
       " '25898,2014-01-01 00:00:00.0,3950,COMPLETE',\n",
       " '25899,2014-01-01 00:00:00.0,8068,CLOSED',\n",
       " '25900,2014-01-01 00:00:00.0,2382,CLOSED',\n",
       " '25901,2014-01-01 00:00:00.0,3099,COMPLETE']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "#Used to Filter rows from RDD\n",
    "#Extract all closed and complered order of jan 2014\n",
    "orders = sc.textFile(\"/user/pi/retail_db/orders\")\n",
    "print(orders.take(10))\n",
    "orderFil = orders.filter(lambda order:order.split(\",\")[3] in ('CLOSED','COMPLETE') and order.split(\",\")[1][:7] == '2014-01')\n",
    "print(orderFil.count())\n",
    "orderFil.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.\n",
    "Data should be in (K, V) and (K, W) tuples and returns a dataset of (K, (V, W))\n",
    "Type of Joins are for 2 RDDs A and B are where A = (K, V) and B= (K, W) then result in format (K, (V, W))\n",
    "    --> A.join(B)\n",
    "    --> A.leftOuterJoin(B)\n",
    "    --> A.rightOuterJoin(B)\n",
    "    --> A.fullOuterJoin(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1014, 2888993.939999649), (502, 3147800.0), (1004, 6929653.499999708), (642, 27840.0), (572, 35191.19999999998), (44, 56330.61000000003), (564, 27210.0), (924, 14151.149999999976), (926, 14870.699999999972), (278, 36576.86999999999)]\n",
      "[(11599, 'CLOSED'), (256, 'PENDING_PAYMENT'), (12111, 'COMPLETE'), (8827, 'CLOSED'), (11318, 'COMPLETE'), (7130, 'COMPLETE'), (4530, 'COMPLETE'), (2911, 'PROCESSING'), (5657, 'PENDING_PAYMENT'), (5648, 'PENDING_PAYMENT')]\n"
     ]
    }
   ],
   "source": [
    "order_items = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "order_items_PairRDD = order_items.map(lambda order_item:(int(order_item.split(\",\")[2]),float(order_item.split(\",\")[4])))\n",
    "orderRevenue = order_items_PairRDD.reduceByKey(lambda x,y:x+y)\n",
    "print(orderRevenue.take(10))\n",
    "orders = sc.textFile(\"/user/pi/retail_db/orders\")\n",
    "orderStatus = orders.map(lambda order:(int(order.split(\",\")[2]),order.split(\",\")[3]))\n",
    "print(orderStatus.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(652, (7539.419999999991, 'PENDING_PAYMENT')), (652, (7539.419999999991, 'PROCESSING')), (652, (7539.419999999991, 'PENDING')), (652, (7539.419999999991, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE'))]\n",
      "[(652, (7539.419999999991, 'PENDING_PAYMENT')), (652, (7539.419999999991, 'PROCESSING')), (652, (7539.419999999991, 'PENDING')), (652, (7539.419999999991, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE'))]\n",
      "[(652, (7539.419999999991, 'PENDING_PAYMENT')), (652, (7539.419999999991, 'PROCESSING')), (652, (7539.419999999991, 'PENDING')), (652, (7539.419999999991, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE'))]\n",
      "[(652, (7539.419999999991, 'PENDING_PAYMENT')), (652, (7539.419999999991, 'PROCESSING')), (652, (7539.419999999991, 'PENDING')), (652, (7539.419999999991, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'PENDING_PAYMENT')), (728, (61490.0, 'COMPLETE')), (728, (61490.0, 'COMPLETE'))]\n"
     ]
    }
   ],
   "source": [
    "#Inner join\n",
    "OrderInner = orderRevenue.join(orderStatus)\n",
    "print(OrderInner.take(10))\n",
    "#left outer join\n",
    "Orderleft = orderRevenue.leftOuterJoin(orderStatus)0\n",
    "print(OrderInner.take(10))\n",
    "#right Outer join\n",
    "Orderright = orderRevenue.rightOuterJoin(orderStatus)\n",
    "print(OrderInner.take(10))\n",
    "#full outer join\n",
    "Orderfull = orderRevenue.fullOuterJoin(orderStatus)\n",
    "print(OrderInner.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tranformation/Action in which elemnts in a group are consolidate and return a single value.ex.sum,count,mean etc\n",
    "2 types:\n",
    "    -->Total Aggregations\n",
    "    -->Per group Aggregation\n",
    "Actions:\n",
    "    count()\n",
    "    reduce()\n",
    "    countByKey()\n",
    "     \n",
    "Transformation:   \n",
    "    reduceByKey()\n",
    "    groupByKey()\n",
    "    aggregateByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,957,1,299.98,299.98\n",
      "2,2,1073,1,199.99,199.99\n",
      "3,2,502,5,250.0,50.0\n",
      "4,2,403,1,129.99,129.99\n",
      "5,4,897,2,49.98,24.99\n",
      "6,4,365,5,299.95,59.99\n",
      "7,4,502,3,150.0,50.0\n",
      "8,4,1014,4,199.92,49.98\n",
      "9,5,957,1,299.98,299.98\n",
      "10,5,365,5,299.95,59.99\n"
     ]
    }
   ],
   "source": [
    "order_items = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "for order_item in order_items.take(10):print(order_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action: count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172198\n"
     ]
    }
   ],
   "source": [
    "#Aggregation - total\n",
    "total_rec = order_items.count()\n",
    "print(total_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action: reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,2,1073,1,199.99,199.99\n",
      "3,2,502,5,250.0,50.0\n",
      "4,2,403,1,129.99,129.99\n",
      "Total Revnue from Order id : 2 is  579.98\n",
      "Total Revnue from Order id : 2 is  579.98\n"
     ]
    }
   ],
   "source": [
    "#Aggregation - total - Get revenue for given order_id\n",
    "from operator import add\n",
    "order_item2=order_items.filter(lambda oi:int(oi.split(\",\")[1])==2)\n",
    "for order_item in order_item2.take(10):print(order_item)\n",
    "#method1\n",
    "order2_rev = order_item2.map(lambda oi:float(oi.split(\",\")[4])).reduce(add)\n",
    "print('Total Revnue from Order id : 2 is ',order2_rev)\n",
    "#method2\n",
    "order2_rev = order_item2.map(lambda oi:float(oi.split(\",\")[4])).reduce(lambda x,y:x+y)\n",
    "print('Total Revnue from Order id : 2 is ',order2_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,2,1073,1,199.99,199.99\n",
      "3,2,502,5,250.0,50.0\n",
      "4,2,403,1,129.99,129.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4,2,403,1,129.99,129.99'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aggregation - total - Get order item details  which has minimum  order item subtotal for given order id\n",
    "order_item2=order_items.filter(lambda oi:int(oi.split(\",\")[1])==2)\n",
    "for order_item in order_item2.take(10):print(order_item)\n",
    "order_item2.reduce(lambda x,y:x if float(x.split(\",\")[4]) < float(x.split(\",\")[4]) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action: countByKey()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input should be pair RDD\n",
    "Input RDD of type (K,V) and returns hash map of (K,int) pair with count of each key(python dictionary)\n",
    "Primaryly used to previewing the data as this is an Action and further processing is difficult as the result is not RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2013-07-25 00:00:00.0,11599,CLOSED\n",
      "2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT\n",
      "3,2013-07-25 00:00:00.0,12111,COMPLETE\n",
      "4,2013-07-25 00:00:00.0,8827,CLOSED\n",
      "5,2013-07-25 00:00:00.0,11318,COMPLETE\n",
      "6,2013-07-25 00:00:00.0,7130,COMPLETE\n",
      "7,2013-07-25 00:00:00.0,4530,COMPLETE\n",
      "8,2013-07-25 00:00:00.0,2911,PROCESSING\n",
      "9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT\n",
      "10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT\n"
     ]
    }
   ],
   "source": [
    "#Get Count by status\n",
    "orders = sc.textFile(\"/user/pi/retail_db/orders\")\n",
    "for order in orders.take(10):print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CLOSED', 1),\n",
       " ('PENDING_PAYMENT', 1),\n",
       " ('COMPLETE', 1),\n",
       " ('CLOSED', 1),\n",
       " ('COMPLETE', 1),\n",
       " ('COMPLETE', 1),\n",
       " ('COMPLETE', 1),\n",
       " ('PROCESSING', 1),\n",
       " ('PENDING_PAYMENT', 1),\n",
       " ('PENDING_PAYMENT', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersPairRdd = orders.map(lambda order:(order.split(\",\")[3],1))\n",
    "ordersPairRdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'CLOSED': 7556,\n",
       "             'PENDING_PAYMENT': 15030,\n",
       "             'COMPLETE': 22899,\n",
       "             'PROCESSING': 8275,\n",
       "             'PAYMENT_REVIEW': 729,\n",
       "             'PENDING': 7610,\n",
       "             'ON_HOLD': 3798,\n",
       "             'CANCELED': 1428,\n",
       "             'SUSPECTED_FRAUD': 1558})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersPairRdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method countByKey in module pyspark.rdd:\n",
      "\n",
      "countByKey() method of pyspark.rdd.PipelinedRDD instance\n",
      "    Count the number of elements for each key, and return the result to the\n",
      "    master as a dictionary.\n",
      "    \n",
      "    >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      "    >>> sorted(rdd.countByKey().items())\n",
      "    [('a', 2), ('b', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ordersPairRdd.countByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.countByKey().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combiners"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reduceByKey() and aggregateByKey() has better performance than groupByKey() becasue groupByKey() not uses combiner and run as single threaded."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "groupByKey()\n",
    "reduceByKey()  -- if the combiner and reducer logic is same and it takes only on function as input\n",
    "aggregateByKey() -- if the combiber and reducer logic is different.It takes 2 functions one for each combiner and reducer\n",
    "\n",
    "Operation:\n",
    "    to sum 1 to 100\n",
    "--in groupByKey() --Not use combiner and reducer logic\n",
    "        sum(1+2+3+....+1000)\n",
    "--in reduceByKey() --User Combiner and reducer logic but support only single function ,ex:sum\n",
    "        SUM(sum(1 to 250) + sum(251 to 500) + sum(501 to 750) + sum(751 to 1000))\n",
    "        --inner 4 sum is called combiner\n",
    "        --other sum is called reducer\n",
    "--in aggreagte by key -- user different function for Combiner and reducer logic.ex:avg\n",
    "        DIV(sum(1 to 250) + sum(251 to 500) + sum(501 to 750) + sum(751 to 1000))/TOTAL COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,957,1,299.98,299.98\n",
      "2,2,1073,1,199.99,199.99\n",
      "3,2,502,5,250.0,50.0\n",
      "4,2,403,1,129.99,129.99\n",
      "5,4,897,2,49.98,24.99\n",
      "6,4,365,5,299.95,59.99\n",
      "7,4,502,3,150.0,50.0\n",
      "8,4,1014,4,199.92,49.98\n",
      "9,5,957,1,299.98,299.98\n",
      "10,5,365,5,299.95,59.99\n",
      "[(1, 299.98), (2, 199.99), (2, 250.0), (2, 129.99), (4, 49.98), (4, 299.95), (4, 150.0), (4, 199.92), (5, 299.98), (5, 299.95)]\n",
      "35186-249.96\n",
      "35188-79.98\n",
      "35190-879.91\n",
      "35192-120.0\n",
      "35194-100.0\n",
      "35196-39.99\n",
      "35198-939.91\n",
      "35200-199.99\n",
      "35206-879.79\n",
      "35208-889.94\n"
     ]
    }
   ],
   "source": [
    "#grouByKey() is least used since it not uses combiner\n",
    "#Get revenue for each order id\n",
    "order_items = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "for order_item in order_items.take(10):print(order_item)\n",
    "orderItemsPairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "print(orderItemsPairRDD.take(10))\n",
    "orderRevGrpBy = orderItemsPairRDD.groupByKey()\n",
    "orderRev = orderRevGrpBy.map(lambda oi:(oi[0],round(sum(oi[1]),2)))\n",
    "for order,rev in orderRev.take(10):\n",
    "    print('{}-{}'.format(order,rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3,2,502,5,250.0,50.0',\n",
       "  '2,2,1073,1,199.99,199.99',\n",
       "  '4,2,403,1,129.99,129.99'],\n",
       " ['6,4,365,5,299.95,59.99',\n",
       "  '8,4,1014,4,199.92,49.98',\n",
       "  '7,4,502,3,150.0,50.0',\n",
       "  '5,4,897,2,49.98,24.99'],\n",
       " ['18,8,365,5,299.95,59.99',\n",
       "  '19,8,1014,4,199.92,49.98',\n",
       "  '17,8,365,3,179.97,59.99',\n",
       "  '20,8,502,1,50.0,50.0'],\n",
       " ['24,10,1073,1,199.99,199.99',\n",
       "  '28,10,1073,1,199.99,199.99',\n",
       "  '26,10,403,1,129.99,129.99',\n",
       "  '25,10,1014,2,99.96,49.98',\n",
       "  '27,10,917,1,21.99,21.99'],\n",
       " ['37,12,191,5,499.95,99.99',\n",
       "  '34,12,957,1,299.98,299.98',\n",
       "  '38,12,502,5,250.0,50.0',\n",
       "  '36,12,1014,3,149.94,49.98',\n",
       "  '35,12,134,4,100.0,25.0'],\n",
       " ['40,14,1004,1,399.98,399.98',\n",
       "  '41,14,1014,2,99.96,49.98',\n",
       "  '42,14,502,1,50.0,50.0'],\n",
       " ['49,16,365,5,299.95,59.99', '48,16,365,2,119.98,59.99'],\n",
       " ['55,18,1073,1,199.99,199.99',\n",
       "  '57,18,403,1,129.99,129.99',\n",
       "  '56,18,365,2,119.98,59.99'],\n",
       " ['63,20,365,5,299.95,59.99',\n",
       "  '60,20,502,5,250.0,50.0',\n",
       "  '61,20,1014,4,199.92,49.98',\n",
       "  '62,20,403,1,129.99,129.99'],\n",
       " ['71,24,502,5,250.0,50.0',\n",
       "  '72,24,1073,1,199.99,199.99',\n",
       "  '73,24,1073,1,199.99,199.99',\n",
       "  '69,24,403,1,129.99,129.99',\n",
       "  '70,24,502,1,50.0,50.0']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get order item details in descending order by revenue\n",
    "order_items = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "orderItemsPairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),oi))\n",
    "orderItemsPairRDDGP = orderItemsPairRDD.groupByKey()\n",
    "orderItemsSrt = orderItemsPairRDDGP.map(lambda oi:sorted(oi[1],key = lambda lst:float(lst.split(\",\")[4]),reverse=True))\n",
    "orderItemsSrt.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87888,35186,403,1,129.99,129.99',\n",
       " '87889,35186,627,3,119.97,39.99',\n",
       " '87892,35188,627,2,79.98,39.99',\n",
       " '87897,35190,1004,1,399.98,399.98',\n",
       " '87894,35190,502,4,200.0,50.0',\n",
       " '87895,35190,403,1,129.99,129.99',\n",
       " '87896,35190,1014,2,99.96,49.98',\n",
       " '87898,35190,1014,1,49.98,49.98',\n",
       " '87899,35192,642,4,120.0,30.0',\n",
       " '87903,35194,502,2,100.0,50.0']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderItemsSrt = orderItemsPairRDDGP.flatMap(lambda oi:sorted(oi[1],key = lambda lst:float(lst.split(\",\")[4]),reverse=True))\n",
    "orderItemsSrt.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function sorted in module builtins:\n",
      "\n",
      "sorted(iterable, /, *, key=None, reverse=False)\n",
      "    Return a new list containing all items from the iterable in ascending order.\n",
      "    \n",
      "    A custom key function can be supplied to customize the sort order, and the\n",
      "    reverse flag can be set to request the result in descending order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduceByKey()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "takes key value pair (K,V) and return another key value pair (K,M) where M is the output of function against each key aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,1,957,1,299.98,299.98',\n",
       " '2,2,1073,1,199.99,199.99',\n",
       " '3,2,502,5,250.0,50.0',\n",
       " '4,2,403,1,129.99,129.99',\n",
       " '5,4,897,2,49.98,24.99',\n",
       " '6,4,365,5,299.95,59.99',\n",
       " '7,4,502,3,150.0,50.0',\n",
       " '8,4,1014,4,199.92,49.98',\n",
       " '9,5,957,1,299.98,299.98',\n",
       " '10,5,365,5,299.95,59.99']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order items\n",
    "order_items = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "order_items.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35186, 249.96), (35188, 79.98), (35190, 879.91), (35192, 120.0), (35194, 100.0), (35196, 39.99), (35198, 939.91), (35200, 199.99), (35206, 879.79), (35208, 889.94)]\n",
      "[(2, 579.98), (4, 699.85), (8, 729.84), (10, 651.92), (12, 1299.87), (14, 549.94), (16, 419.93), (18, 449.96), (20, 879.86), (24, 829.97)]\n"
     ]
    }
   ],
   "source": [
    "#Get revenue for each order id\n",
    "order_item_pairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "order_item_rev = order_item_pairRDD.reduceByKey(lambda x,y:x+y)\n",
    "order_item_rev_rnd = order_item_rev.map(lambda tp:(tp[0],round(tp[1],2)))\n",
    "print(order_item_rev_rnd.take(10))\n",
    "\n",
    "#method2\n",
    "from operator import add\n",
    "order_item_rev = order_item_pairRDD.reduceByKey(add)\n",
    "order_item_rev_rnd = order_item_rev.map(lambda tp:(tp[0],round(tp[1],2)))\n",
    "print(order_item_rev_rnd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35186, 119.97), (35188, 79.98), (35190, 49.98), (35192, 120.0), (35194, 100.0), (35196, 39.99), (35198, 50.0), (35200, 199.99), (35206, 74.97), (35208, 100.0)]\n"
     ]
    }
   ],
   "source": [
    "#Get min revenue for each order id\n",
    "order_item_pairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "order_item_rev = order_item_pairRDD.reduceByKey(lambda x,y:x if x<y else y)\n",
    "print(order_item_rev.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, '4,2,403,1,129.99,129.99'), (4, '5,4,897,2,49.98,24.99'), (8, '20,8,502,1,50.0,50.0'), (10, '27,10,917,1,21.99,21.99'), (12, '35,12,134,4,100.0,25.0'), (14, '42,14,502,1,50.0,50.0'), (16, '48,16,365,2,119.98,59.99'), (18, '56,18,365,2,119.98,59.99'), (20, '62,20,403,1,129.99,129.99'), (24, '70,24,502,1,50.0,50.0')]\n"
     ]
    }
   ],
   "source": [
    "#Get order item details with minimum subtotal for each order\n",
    "order_item_pairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),oi))\n",
    "order_item_rev = order_item_pairRDD.reduceByKey(lambda x,y:x if float(x.split(\",\")[4]) < float(y.split(\",\")[4]) else y)\n",
    "print(order_item_rev.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregateByKey()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Used when Combiner logic and reducer logic are different\n",
    "Combiner Logic - for intermediate value\n",
    "Reducer Logic  - for final value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 299.98),\n",
       " (2, 199.99),\n",
       " (2, 250.0),\n",
       " (2, 129.99),\n",
       " (4, 49.98),\n",
       " (4, 299.95),\n",
       " (4, 150.0),\n",
       " (4, 199.92),\n",
       " (5, 299.98),\n",
       " (5, 299.95)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Revenue and count of itmes for each order_id\n",
    "order_item_pairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "order_item_pairRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (579.98, 3)),\n",
       " (4, (699.85, 4)),\n",
       " (8, (729.8399999999999, 4)),\n",
       " (10, (651.9200000000001, 5)),\n",
       " (12, (1299.8700000000001, 5)),\n",
       " (14, (549.94, 3)),\n",
       " (16, (419.93, 2)),\n",
       " (18, (449.96000000000004, 3)),\n",
       " (20, (879.8599999999999, 4)),\n",
       " (24, (829.97, 5))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need output in format (2,(579.98,3))\n",
    "#aggregateByKey()\n",
    "#param1 - initialize output type variable\n",
    "#param2 - x data type is (0.0,0) and y data type is 0.0 (revenue) - funtion for combiner\n",
    "#param3 - data type of both x and y is of (0.0,0)\n",
    "oiAgg = order_item_pairRDD.aggregateByKey((0.0,0),\\\n",
    "                                         lambda x,y:(x[0]+y,x[1]+1),\\\n",
    "                                         lambda x,y:(x[0]+y[0],x[1],y[1])\\\n",
    "                                         )\n",
    "oiAgg.take(10)                                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (579.98, 3)),\n",
       " (4, (699.85, 4)),\n",
       " (8, (729.8399999999999, 4)),\n",
       " (10, (651.9200000000001, 5)),\n",
       " (12, (1299.8700000000001, 5)),\n",
       " (14, (549.94, 3)),\n",
       " (16, (419.93, 2)),\n",
       " (18, (449.96000000000004, 3)),\n",
       " (20, (879.8599999999999, 4)),\n",
       " (24, (829.97, 5))]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same output using reduceByKey()\n",
    "order_item_pairRDD = order_items.map(lambda oi:(int(oi.split(\",\")[1]),(float(oi.split(\",\")[4]),1)))\n",
    "rev_cnt = order_item_pairRDD.reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "rev_cnt.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortByKey()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\n",
    "\n",
    "This regex is matching a comma , only if it is outside double quotes by counting even number of quotes after literal ,.\n",
    "\n",
    "Explanation:\n",
    "\n",
    ", -> match literal comma\n",
    "(?=...) -> positive lookahead\n",
    "[^\"]*\" -> match anything before a \" followed by a literal \"\n",
    "[^\"]*\"[^\"]*\" -> match a pair of above \n",
    "(?:[^\"]*\"[^\"]*\")* -> Match 0 or more of pairs (0, 2, 4, 6 sets)\n",
    "[^\"]*$ -> Followed by any non-quote till end of string\n",
    "Example Input:\n",
    "\n",
    "\"Field1,Field2\",\"Field3\",\"item1,item2,item3\"\n",
    "First it will match , before \"Field3\" because lookahead: (?=(?:[^\"]*\"[^\"]*\")*[^\"]*$) is making sure there are 4 double quotes after this comma.\n",
    "Second it will match , after \"Field3\" because lookahead: (?=(?:[^\"]*\"[^\"]*\")*[^\"]*$) is making sure there are 2 double quotes after this comma.\n",
    "It is not matching comma between Field1 and Field2 because # of quotes after that are odd in numbers and hence lookahead (?=(?:[^\"]*\"[^\"]*\")*[^\"]*$) will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical\n",
      "66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "1048,47,\"Spalding Beast 60\"\" Glass Portable Basketball \",,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop\n",
      "60,4,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "197,10,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "488,22,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "694,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n",
      "695,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n"
     ]
    }
   ],
   "source": [
    "#sort data by product price descending\n",
    "import re\n",
    "\n",
    "class Utils():\n",
    "    COMMA_DELIMITER = re.compile(''',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)''')\n",
    "\n",
    "products = sc.textFile(\"/user/pi/retail_db/products\")\n",
    "productsPairRDD=products.map(lambda product:(float(Utils.COMMA_DELIMITER.split(product)[4]),product))\n",
    "productsSorted=productsPairRDD.sortByKey(ascending=False).map(lambda x:x[1])\n",
    "for products in productsSorted.take(10):\n",
    "    print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16,2,Riddell Youth 360 Custom Football Helmet,,299.99,http://images.acmesports.sports/Riddell+Youth+360+Custom+Football+Helmet',\n",
       " '11,2,Fitness Gear 300 lb Olympic Weight Set,,209.99,http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set',\n",
       " '5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet',\n",
       " '14,2,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy',\n",
       " \"12,2,Under Armour Men's Highlight MC Alter Ego Fla,,139.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Alter+Ego+Flash+Football...\",\n",
       " \"23,2,Under Armour Men's Highlight MC Alter Ego Hul,,139.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Alter+Ego+Hulk+Football...\",\n",
       " \"6,2,Jordan Men's VI Retro TD Football Cleat,,134.99,http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat\",\n",
       " \"2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\",\n",
       " \"8,2,Nike Men's Vapor Carbon Elite TD Football Cle,,129.99,http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat\",\n",
       " \"10,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\"]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort data by CategoryId and then price descending\n",
    "products = sc.textFile(\"/user/pi/retail_db/products\")\n",
    "productsPairRDD = products.map(lambda p:((int(Utils.COMMA_DELIMITER.split(p)[1]),-float(Utils.COMMA_DELIMITER.split(p)[4])),p))\n",
    "productsPairRDD.sortByKey().map(lambda x:x[1]).take(10)\n",
    "#sortByKey can sort both key in key tuple in ascending order or both in descending order.\n",
    "#Inorder to sort one in ascending and other in decending ,negative the value for decending key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Ranking using sortByKey and take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n",
      "695,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n",
      "60,4,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "197,10,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "488,22,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "709,32,Top Flite Women's Aero Adjustable Driver - Gr,,99.99,http://images.acmesports.sports/Top+Flite+Women%27s+Aero+Adjustable+Driver+-+Grey\n",
      "710,32,Top Flite Women's Aero Adjustable Driver - Pi,,99.99,http://images.acmesports.sports/Top+Flite+Women%27s+Aero+Adjustable+Driver+-+Pink\n",
      "719,33,Nike Lunar Cypress Golf Shoes,,99.99,http://images.acmesports.sports/Nike+Lunar+Cypress+Golf+Shoes\n",
      "733,33,Nike Women's Lunar Empress Golf Shoes,,99.99,http://images.acmesports.sports/Nike+Women%27s+Lunar+Empress+Golf+Shoes\n",
      "751,34,Ogio Sport Golf Shoes,,99.99,http://images.acmesports.sports/Ogio+Sport+Golf+Shoes\n"
     ]
    }
   ],
   "source": [
    "#sort data by product price descending\n",
    "import re\n",
    "class Utils():\n",
    "    COMMA_DELIMITER = re.compile(''',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)''')\n",
    "    \n",
    "products = sc.textFile(\"/user/pi/retail_db/products\")\n",
    "prodctsMap = products.map(lambda product:(Utils.COMMA_DELIMITER.split(product)[4],product))\n",
    "prodctsMap_Sorted = prodctsMap.sortByKey(ascending=False).map(lambda pd:pd[1])\n",
    "for p in prodctsMap_Sorted.take(10):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global using takeOrdered or top\n",
    "#--takeOrdered() - sort data in ascending order and first n elements\n",
    "#--top() - sort data in descending order and first n elements\n",
    "#--Can be applied on RDD directly,no pairRDD is required\n",
    "#--Both these functions are Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38,3,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "388,18,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "414,19,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "517,24,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "547,25,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "934,42,Callaway X Hot Driver,,0.0,http://images.acmesports.sports/Callaway+X+Hot+Driver\n",
      "1284,57,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "624,29,adidas Batting Helmet Hardware Kit,,4.99,http://images.acmesports.sports/adidas+Batting+Helmet+Hardware+Kit\n",
      "815,37,Zero Friction Practice Golf Balls - 12 Pack,,4.99,http://images.acmesports.sports/Zero+Friction+Practice+Golf+Balls+-+12+Pack\n",
      "336,15,\"Nike Swoosh Headband - 2\"\"\",,5.0,http://images.acmesports.sports/Nike+Swoosh+Headband+-+2%22\n"
     ]
    }
   ],
   "source": [
    "#order products by revenue asceding - using takeOrdered()\n",
    "products = sc.textFile(\"/user/pi/retail_db/products\")\n",
    "productsBottom10 = products.takeOrdered(10,lambda p:float(Utils.COMMA_DELIMITER.split(p)[4]))\n",
    "for product in productsBottom10:\n",
    "    print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical\n",
      "66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "1048,47,\"Spalding Beast 60\"\" Glass Portable Basketball \",,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop\n",
      "60,4,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "197,10,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "488,22,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "694,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n",
      "695,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n"
     ]
    }
   ],
   "source": [
    "#order products by revenue descending - using top()\n",
    "productsTop10 = products.top(10,lambda p:float(Utils.COMMA_DELIMITER.split(p)[4]))\n",
    "for product in productsTop10:\n",
    "    print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38,3,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "388,18,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "414,19,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "517,24,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "547,25,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "934,42,Callaway X Hot Driver,,0.0,http://images.acmesports.sports/Callaway+X+Hot+Driver\n",
      "1284,57,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat\n",
      "624,29,adidas Batting Helmet Hardware Kit,,4.99,http://images.acmesports.sports/adidas+Batting+Helmet+Hardware+Kit\n",
      "815,37,Zero Friction Practice Golf Balls - 12 Pack,,4.99,http://images.acmesports.sports/Zero+Friction+Practice+Golf+Balls+-+12+Pack\n",
      "336,15,\"Nike Swoosh Headband - 2\"\"\",,5.0,http://images.acmesports.sports/Nike+Swoosh+Headband+-+2%22\n"
     ]
    }
   ],
   "source": [
    "#order products by revenue asceding - using top()\n",
    "productsBottom10 = products.top(10,lambda p:-float(Utils.COMMA_DELIMITER.split(p)[4]))\n",
    "for product in productsBottom10:\n",
    "    print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical\n",
      "66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
      "1048,47,\"Spalding Beast 60\"\" Glass Portable Basketball \",,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop\n",
      "60,4,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "197,10,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "488,22,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical\n",
      "694,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n",
      "695,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...\n"
     ]
    }
   ],
   "source": [
    "#order products by revenue descending - using takeOrdered()\n",
    "productsTop10 = products.takeOrdered(10,lambda p:-float(Utils.COMMA_DELIMITER.split(p)[4]))\n",
    "for product in productsTop10:\n",
    "    print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(productsTop10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank By Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top N products by price per category - Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7,2,Schutt Youth Recruit Hybrid Custom Football H,,99.99,http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014',\n",
       " \"3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\",\n",
       " \"4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\",\n",
       " '31,3,Nike+ Fuelband SE,,99.0,http://images.acmesports.sports/Nike%2B+Fuelband+SE',\n",
       " \"26,3,Nike Men's USA White Home Stadium Soccer Jers,,90.0,http://images.acmesports.sports/Nike+Men%27s+USA+White+Home+Stadium+Soccer+Jersey\",\n",
       " \"29,3,Nike Men's USA Away Stadium Replica Soccer Je,,90.0,http://images.acmesports.sports/Nike+Men%27s+USA+Away+Stadium+Replica+Soccer+Jersey\",\n",
       " '60,4,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical',\n",
       " '56,4,Fitbit Flex Wireless Activity & Sleep Wristba,,99.95,http://images.acmesports.sports/Fitbit+Flex+Wireless+Activity+%26+Sleep+Wristband',\n",
       " '54,4,Nike+ Fuelband SE,,99.0,http://images.acmesports.sports/Nike%2B+Fuelband+SE',\n",
       " '77,5,Schutt Youth Recruit Hybrid Custom Football H,,99.99,http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Python Collection\n",
    "products = sc.textFile(\"/user/pi/retail_db/products\")\n",
    "productMap = products.map(lambda p:(Utils.COMMA_DELIMITER.split(p)[1],p))\n",
    "productMapGp=productMap.groupByKey()\n",
    "productCat=productMapGp.flatMap(lambda p:sorted(p[1],key=lambda l:Utils.COMMA_DELIMITER.split(l)[4],reverse=True)[:3])\n",
    "productCat.top(10,lambda p:-int(Utils.COMMA_DELIMITER.split(p)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top N priced products for per catagory - Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16,2,Riddell Youth 360 Custom Football Helmet,,299.99,http://images.acmesports.sports/Riddell+Youth+360+Custom+Football+Helmet',\n",
       " '11,2,Fitness Gear 300 lb Olympic Weight Set,,209.99,http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set',\n",
       " '5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet',\n",
       " '14,2,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy',\n",
       " '66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill',\n",
       " '60,4,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical',\n",
       " '71,4,Diamondback Adult Response XE Mountain Bike 2,,349.98,http://images.acmesports.sports/Diamondback+Adult+Response+XE+Mountain+Bike+2014',\n",
       " '117,6,YETI Tundra 65 Chest Cooler,,399.99,http://images.acmesports.sports/YETI+Tundra+65+Chest+Cooler',\n",
       " '106,6,Teeter Hang Ups NXT-S Inversion Table,,299.99,http://images.acmesports.sports/Teeter+Hang+Ups+NXT-S+Inversion+Table',\n",
       " '100,6,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy',\n",
       " '162,8,YETI Tundra 65 Chest Cooler,,399.99,http://images.acmesports.sports/YETI+Tundra+65+Chest+Cooler',\n",
       " '153,8,Teeter Hang Ups NXT-S Inversion Table,,299.99,http://images.acmesports.sports/Teeter+Hang+Ups+NXT-S+Inversion+Table',\n",
       " '148,8,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy',\n",
       " '208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical',\n",
       " '199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill',\n",
       " '197,10,SOLE E25 Elliptical,,999.99,http://images.acmesports.sports/SOLE+E25+Elliptical',\n",
       " '262,12,Fitness Gear Pro Utility Bench,,179.99,http://images.acmesports.sports/Fitness+Gear+Pro+Utility+Bench',\n",
       " \"255,12,ASICS Women's GEL-Noosa Tri 9 Running Shoe,,139.99,http://images.acmesports.sports/ASICS+Women%27s+GEL-Noosa+Tri+9+Running+Shoe\",\n",
       " \"260,12,ASICS Women's GEL-Nimbus 15 Running Shoe,,119.99,http://images.acmesports.sports/ASICS+Women%27s+GEL-Nimbus+15+Running+Shoe\",\n",
       " '860,38,Bushnell Pro X7 Jolt Slope Rangefinder,,599.99,http://images.acmesports.sports/Bushnell+Pro+X7+Jolt+Slope+Rangefinder']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import takewhile\n",
    "\n",
    "def getTopNProducts(products,topN):\n",
    "    productsSorted = sorted(products[1],key=lambda p:float(Utils.COMMA_DELIMITER.split(p)[4]),reverse=True)\n",
    "    TopN_prices = sorted(set(map(lambda p:float(Utils.COMMA_DELIMITER.split(p)[4]),productsSorted)),reverse=True)[:topN]\n",
    "    return takewhile(lambda l:float(Utils.COMMA_DELIMITER.split(l)[4]) in TopN_prices,productsSorted)\n",
    "\n",
    "products = sc.textFile(\"/user/pi/retail_db/products\")\n",
    "prod_catMap = products.map(lambda p:(int(Utils.COMMA_DELIMITER.split(p)[1]),p))\n",
    "prod_catMap_GP=prod_catMap.groupByKey()\n",
    "prod_catMap_GP_TopN=prod_catMap_GP.flatMap(lambda l:getTopNProducts(l,3))\n",
    "prod_catMap_GP_TopN.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data\n",
    "#Requires 2 datasets of same structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['154733,61900,1073,1,199.99,199.99',\n",
       " '154734,61900,365,5,299.95,59.99',\n",
       " '154743,61904,724,5,500.0,100.0',\n",
       " '154756,61908,403,1,129.99,129.99',\n",
       " '154769,61912,773,1,249.99,249.99',\n",
       " '154778,61916,666,1,109.99,109.99',\n",
       " '154779,61916,1004,1,399.98,399.98',\n",
       " '154780,61916,1014,3,149.94,49.98',\n",
       " '154781,61916,1073,1,199.99,199.99',\n",
       " '154786,61920,957,1,299.98,299.98']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract OrderItesm from 2013-12 and 2014-01\n",
    "#order_items dataset has no date .So need to join with orders and get data\n",
    "orders = sc.textFile(\"/user/pi/retail_db/orders\")\n",
    "orderItems = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "orders201312 = orders.filter(lambda o:o.split(\",\")[1][:7]=='2013-12').map(lambda o:(int(o.split(',')[0]),o))\n",
    "orders201401 = orders.filter(lambda o:o.split(\",\")[1][:7]=='2014-01').map(lambda o:(int(o.split(',')[0]),o))\n",
    "orderItemsMap = orderItems.map(lambda oi:(int(oi.split(\",\")[1]),oi))\n",
    "orderItems201312 = orders201312.join(orderItemsMap).map(lambda oi:oi[1][1])\n",
    "orderItems201401 = orders201401.join(orderItemsMap).map(lambda oi:oi[1][1])\n",
    "orderItems201401.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14729\n",
      "14666\n"
     ]
    }
   ],
   "source": [
    "print(orderItems201312.count())\n",
    "print(orderItems201401.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union and distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14729\n",
      "14666\n",
      "29395\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#get allproduct ids in 2013-12 and 2014-01\n",
    "productIds201312 = orderItems201312.map(lambda p:int(p.split(\",\")[2]))\n",
    "productIds201401 = orderItems201401.map(lambda p:int(p.split(\",\")[2]))\n",
    "print(productIds201312.count())\n",
    "print(productIds201401.count())\n",
    "allProducts = productIds201312.union(productIds201401)\n",
    "print(allProducts.count())\n",
    "allProducts = productIds201312.union(productIds201401).distinct()\n",
    "print(allProducts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersect and Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intersect by default gives distinct values\n",
    "#Product Ids sold in both 2013-12 and 2014-01\n",
    "commonProducts = productIds201312.intersection(productIds201401)\n",
    "commonProducts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#product ids sold in 2013-12 and not in 2014-01\n",
    "products201312only = productIds201312.subtract(productIds201401).distinct()\n",
    "products201312only.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#product ids sold in 2014-01 and not in 2013-12\n",
    "products201401only = productIds201401.subtract(productIds201312).distinct()\n",
    "products201401only.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#product ids sold only in 2014-01 and only in 2013-12\n",
    "productsonly = products201312only.union(products201401only)\n",
    "productsonly.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data into HDFS - text file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get revenue per order and save to text file\n",
    "from operator import add\n",
    "orderItems = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "order_rev = orderItems.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "orer_revenue = order_rev.reduceByKey(add)\n",
    "orer_revenue.map(lambda ore:str(ore[0])+'\\t'+str(ore[1])).saveAsTextFile('/user/pi/retail_db/order_items_by_revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 17:37:30,321 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   2 pi supergroup          0 2020-05-20 17:36 /user/pi/retail_db/order_items_by_revenue/_SUCCESS\n",
      "-rw-r--r--   2 pi supergroup     454338 2020-05-20 17:36 /user/pi/retail_db/order_items_by_revenue/part-00000\n",
      "-rw-r--r--   2 pi supergroup     454679 2020-05-20 17:36 /user/pi/retail_db/order_items_by_revenue/part-00001\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/pi/retail_db/order_items_by_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t579.98\n",
      "4\t699.85\n",
      "8\t729.8399999999999\n",
      "10\t651.9200000000001\n",
      "12\t1299.8700000000001\n",
      "14\t549.94\n",
      "16\t419.93\n",
      "18\t449.96000000000004\n",
      "20\t879.8599999999999\n",
      "24\t829.97\n"
     ]
    }
   ],
   "source": [
    "order_items_by_revenue=sc.textFile('/user/pi/retail_db/order_items_by_revenue')\n",
    "for i in order_items_by_revenue.take(10):print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Saving data into HDFS - text file format with compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n",
      "<!--\r\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
      "  you may not use this file except in compliance with the License.\r\n",
      "  You may obtain a copy of the License at\r\n",
      "\r\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\r\n",
      "\r\n",
      "  Unless required by applicable law or agreed to in writing, software\r\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
      "  See the License for the specific language governing permissions and\r\n",
      "  limitations under the License. See accompanying LICENSE file.\r\n",
      "-->\r\n",
      "\r\n",
      "<!-- Put site-specific property overrides in this file. -->\r\n",
      "\r\n",
      "<configuration>\r\n",
      "<property>\r\n",
      "<name>fs.default.name</name>\r\n",
      "<value>hdfs://raspberrypi1:9000</value>\r\n",
      "</property>\r\n",
      "<property>\r\n",
      "<name>io.compression.codecs</name>\r\n",
      "<value>org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec</value>\r\n",
      "</property>\r\n",
      "</configuration>\r\n"
     ]
    }
   ],
   "source": [
    "#to verify which all compression are configured on cluster open core-site.html\n",
    "! cat /opt/hadoop/etc/hadoop/core-site.xml\n",
    "#check for property io.compression.codecs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Hadoop supports the following compression types and codecs:\n",
    "\n",
    "gzip  org.apache.hadoop.io.compress.GzipCodec\n",
    "bzip2  org.apache.hadoop.io.compress.BZip2Codec\n",
    "LZO  com.hadoop.compression.lzo.LzopCodec\n",
    "Snappy  org.apache.hadoop.io.compress.SnappyCodec\n",
    "Deflate  org.apache.hadoop.io.compress.DeflateCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method saveAsTextFile in module pyspark.rdd:\n",
      "\n",
      "saveAsTextFile(path, compressionCodecClass=None) method of pyspark.rdd.PipelinedRDD instance\n",
      "    Save this RDD as a text file, using string representations of elements.\n",
      "    \n",
      "    @param path: path to text file\n",
      "    @param compressionCodecClass: (None by default) string i.e.\n",
      "        \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      "    \n",
      "    >>> tempFile = NamedTemporaryFile(delete=True)\n",
      "    >>> tempFile.close()\n",
      "    >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      "    >>> from fileinput import input\n",
      "    >>> from glob import glob\n",
      "    >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      "    '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      "    \n",
      "    Empty lines are tolerated when saving to text files.\n",
      "    \n",
      "    >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      "    >>> tempFile2.close()\n",
      "    >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      "    >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      "    '\\n\\n\\nbar\\nfoo\\n'\n",
      "    \n",
      "    Using compressionCodecClass\n",
      "    \n",
      "    >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      "    >>> tempFile3.close()\n",
      "    >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      "    >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      "    >>> from fileinput import input, hook_compressed\n",
      "    >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      "    >>> b''.join(result).decode('utf-8')\n",
      "    'bar\\nfoo\\n'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "orderItems = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "order_rev = orderItems.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "order_revenue = order_rev.reduceByKey(add)\n",
    "help(order_revenue.saveAsTextFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4926.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1013)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1013)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1013)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1012)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:970)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:968)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:968)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:968)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1562)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1550)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1550)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1550)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:558)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 332.0 failed 4 times, most recent failure: Lost task 1.3 in stage 332.0 (TID 718, 192.168.1.111, executor 2): java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support.\n\tat org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:65)\n\tat org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:134)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150)\n\tat org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131)\n\tat org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:100)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:137)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n\t... 48 more\nCaused by: java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support.\n\tat org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:65)\n\tat org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:134)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150)\n\tat org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131)\n\tat org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:100)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:137)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-20e377e37a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m order_revenue.map(lambda ore:str(ore[0])+'\\t'+str(ore[1])).saveAsTextFile('/user/pi/retail_db/order_items_by_revenue_compressed',\\\n\u001b[0;32m----> 2\u001b[0;31m                         compressionCodecClass=\"org.apache.hadoop.io.compress.SnappyCodec\")\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompressionCodecClass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mcompressionCodec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressionCodecClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4926.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1013)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1013)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1013)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1012)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:970)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:968)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$2.apply(PairRDDFunctions.scala:968)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:968)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1562)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1550)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1550)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1550)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:558)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 332.0 failed 4 times, most recent failure: Lost task 1.3 in stage 332.0 (TID 718, 192.168.1.111, executor 2): java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support.\n\tat org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:65)\n\tat org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:134)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150)\n\tat org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131)\n\tat org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:100)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:137)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\n\t... 48 more\nCaused by: java.lang.RuntimeException: native snappy library not available: this version of libhadoop was built without snappy support.\n\tat org.apache.hadoop.io.compress.SnappyCodec.checkNativeCodeLoaded(SnappyCodec.java:65)\n\tat org.apache.hadoop.io.compress.SnappyCodec.getCompressorType(SnappyCodec.java:134)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150)\n\tat org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131)\n\tat org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:100)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:137)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:230)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:120)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "order_revenue.map(lambda ore:str(ore[0])+'\\t'+str(ore[1])).saveAsTextFile('/user/pi/retail_db/order_items_by_revenue_compressed',\\\n",
    "                        compressionCodecClass=\"org.apache.hadoop.io.compress.SnappyCodec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data into HDFS using Data Frames - json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Supported File Formats \n",
    "    orc\n",
    "    json\n",
    "    parquest\n",
    "    avro(with databricks plugin)\n",
    "\n",
    "Steps to save into different file formats\n",
    "    Make sure data is represented as Data Frame\n",
    "    Use write or save API to save Data Frame into different file formats\n",
    "    User compression algorithom if required"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.write.json(<path>)\n",
    "df.save(<path>,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 579.98),\n",
       " (4, 699.85),\n",
       " (8, 729.84),\n",
       " (10, 651.92),\n",
       " (12, 1299.87),\n",
       " (14, 549.94),\n",
       " (16, 419.93),\n",
       " (18, 449.96),\n",
       " (20, 879.86),\n",
       " (24, 829.97)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "orderItems = sc.textFile(\"/user/pi/retail_db/order_items\")\n",
    "order_rev = orderItems.map(lambda oi:(int(oi.split(\",\")[1]),float(oi.split(\",\")[4])))\n",
    "order_revenue = order_rev.reduceByKey(add).map(lambda o:(o[0],round(float(o[1]),2)))\n",
    "order_revenue.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method toDF in module pyspark.sql.session:\n",
      "\n",
      "toDF(schema=None, sampleRatio=None) method of pyspark.rdd.PipelinedRDD instance\n",
      "    Converts current :class:`RDD` into a :class:`DataFrame`\n",
      "    \n",
      "    This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      "    \n",
      "    :param schema: a :class:`pyspark.sql.types.StructType` or list of names of columns\n",
      "    :param samplingRatio: the sample ratio of rows used for inferring\n",
      "    :return: a DataFrame\n",
      "    \n",
      "    >>> rdd.toDF().collect()\n",
      "    [Row(name=u'Alice', age=1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(order_revenue.toDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|order_id|Revenue|\n",
      "+--------+-------+\n",
      "|       2| 579.98|\n",
      "|       4| 699.85|\n",
      "|       8| 729.84|\n",
      "|      10| 651.92|\n",
      "|      12|1299.87|\n",
      "|      14| 549.94|\n",
      "|      16| 419.93|\n",
      "|      18| 449.96|\n",
      "|      20| 879.86|\n",
      "|      24| 829.97|\n",
      "|      28| 1159.9|\n",
      "|      30|  100.0|\n",
      "|      34| 299.98|\n",
      "|      36| 799.96|\n",
      "|      38| 359.96|\n",
      "|      42| 739.92|\n",
      "|      44| 399.98|\n",
      "|      46| 229.95|\n",
      "|      48|  99.96|\n",
      "|      50| 429.97|\n",
      "+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#save as json file\n",
    "#convert to DF\n",
    "order_revenueDF = order_revenue.toDF(schema=[\"order_id\",\"Revenue\"])\n",
    "order_revenueDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_revenueDF.write.json('/user/pi/retail_db/order_items_by_rev_json')\n",
    "#order_revenueDF.save('/user/pi/retail_db/order_items_by_rev_json',\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|Revenue|order_id|\n",
      "+-------+--------+\n",
      "| 299.98|       1|\n",
      "|1129.86|       5|\n",
      "| 579.92|       7|\n",
      "| 599.96|       9|\n",
      "| 919.79|      11|\n",
      "| 127.96|      13|\n",
      "| 925.91|      15|\n",
      "| 694.84|      17|\n",
      "| 699.96|      19|\n",
      "| 372.91|      21|\n",
      "| 299.98|      23|\n",
      "| 399.98|      25|\n",
      "| 749.97|      27|\n",
      "|1109.85|      29|\n",
      "| 499.95|      31|\n",
      "| 659.89|      33|\n",
      "| 129.99|      35|\n",
      "| 159.95|      37|\n",
      "| 199.99|      39|\n",
      "| 327.88|      41|\n",
      "+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_revenueDF = sqlContext.read.json('/user/pi/retail_db/order_items_by_rev_json')\n",
    "order_revenueDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(299.98, 1),\n",
       " (1129.86, 5),\n",
       " (579.92, 7),\n",
       " (599.96, 9),\n",
       " (919.79, 11),\n",
       " (127.96, 13),\n",
       " (925.91, 15),\n",
       " (694.84, 17),\n",
       " (699.96, 19),\n",
       " (372.91, 21)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_revenueDF.rdd.map(lambda x:(x[0],x[1])).take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
