{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6: Apache Spark 1.6 - Data Analysis - Spark SQL or HiveQL using Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different interfaces to run SQL - Hive, Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from command line\n",
    "***\n",
    "spark-sql --master yarn --conf spark.ui.port=12567\n",
    "***\n",
    "- in sparl sql prompt list databases\n",
    "    * $show databases; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same queries can be run using hive prompt\n",
    "    * $hive\n",
    "- hive will be compiled to map reduce framework\n",
    "- if its spark-sql will be compiled to spark framework\n",
    "- to compile need to read metadata - called hive metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create database and tables of text file format - orders and order_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create database *retail_db*\n",
    "* Create tables *orders* and *order_items*\n",
    "* Load data into tables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$hive\n",
    ">create database retail_db;\n",
    ">use retail_db\n",
    ">show tables;\n",
    "\n",
    "#to run hadoop commands from hive prompt\n",
    ">dfs -ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#to find the path on which database is created.\n",
    "hive-site.xml\n",
    "check proprty\n",
    "<property>\n",
    "<name>hive.metastore.warehouse.dir</name>\n",
    "<value>/apps/hive/warehouse</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Hive Session ID = 589e3a17-0278-4111-9f5d-a85dc9b0fd54\n",
      "\n",
      "Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "Hive Session ID = 0d5f858e-45db-4f03-8788-8316abde41e0\n",
      "Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "hive> "
     ]
    }
   ],
   "source": [
    "! hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* launch hive in command prompt\n",
    "***\n",
    "hive\n",
    "***\n",
    ">set hive.metastore.warehouse.dir;\n",
    "***\n",
    "- above command shows the path of *.db creation\n",
    "- hive.metastore.warehouse.dir=/user/hive/warehouse  --hdfs location\n",
    "- Create database\n",
    "***\n",
    ">create database reatil_db_txt\n",
    "***\n",
    ">use retail_db_txt\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-23 10:47:36,932 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - pi supergroup          0 2020-05-23 10:44 /user/hive/warehouse/retail_db_txt.db\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#to execute hdfs command from hive prompt\n",
    "hive> dfs -ls /user/hive/warehouse;\n",
    "Found 1 items\n",
    "drwxr-xr-x   - pi supergroup          0 2020-05-23 10:44 /user/hive/warehouse/retail_db_txt.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tables orders and order_items\n",
    "* Orders\n",
    "    - order_id int(11)\n",
    "    - order_date datetime\n",
    "    - order_custermer_id int(11)\n",
    "    - order_status varchar(45)\n",
    "\n",
    "* Order_Items\n",
    "    - order_item_id int(11)\n",
    "    - order_id int(11)\n",
    "    - product_id int(11)\n",
    "    - quantity tinyint(4)\n",
    "    - subtotal float\n",
    "    - product_price float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive> create table orders (\n",
    "    > order_id int,\n",
    "    > order_date string,\n",
    "    > customer_id int,\n",
    "    > status string\n",
    "    > ) row format delimited fields terminated by ','\n",
    "    > stored as textfile;\n",
    "OK\n",
    "Time taken: 1.759 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive> show tables;\n",
    "OK\n",
    "orders\n",
    "Time taken: 0.164 seconds, Fetched: 1 row(s)\n",
    "hive> select * from orders;\n",
    "OK\n",
    "Time taken: 5.366 seconds\n",
    "hive> select * from orders limit 10;\n",
    "OK\n",
    "Time taken: 0.534 seconds\n",
    "hive>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table\n",
    "#from hdfs - without truncate load table\n",
    "hive> load data inpath '/user/pi/retail_db/orders' into table orders;\n",
    "#from hdfs with truncate load\n",
    "hive> load data inpath '/user/pi/retail_db/orders' overwrite into table orders;\n",
    "#from local - without truncate load table\n",
    "hive> load data local inpath '/user/pi/retail_db/orders' into table orders;\n",
    "#from local with truncate load\n",
    "hive> load data local inpath '/user/pi/retail_db/orders' overwrite into table orders;\n",
    "hive> dfs -ls /user/hive/warehouse/retail_db_txt.db/orders/;\n",
    "Found 1 items\n",
    "-rw-r--r--   2 pi supergroup    3068827 2020-05-11 12:05 /user/hive/warehouse/retail_db_txt.db/orders/part-00000\n",
    "hive> select * from orders limit 10;\n",
    "OK\n",
    "1       2013-07-25 00:00:00.0   11599   CLOSED\n",
    "2       2013-07-25 00:00:00.0   256     PENDING_PAYMENT\n",
    "3       2013-07-25 00:00:00.0   12111   COMPLETE\n",
    "4       2013-07-25 00:00:00.0   8827    CLOSED\n",
    "5       2013-07-25 00:00:00.0   11318   COMPLETE\n",
    "6       2013-07-25 00:00:00.0   7130    COMPLETE\n",
    "7       2013-07-25 00:00:00.0   4530    COMPLETE\n",
    "8       2013-07-25 00:00:00.0   2911    PROCESSING\n",
    "9       2013-07-25 00:00:00.0   5657    PENDING_PAYMENT\n",
    "10      2013-07-25 00:00:00.0   5648    PENDING_PAYMENT\n",
    "Time taken: 0.485 seconds, Fetched: 10 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create order_items table\n",
    "hive> create table order_items(\n",
    "    >     order_item_id int,\n",
    "    >     order_id int,\n",
    "    >     product_id int,\n",
    "    >     quantity int,\n",
    "    >     subtotal float,\n",
    "    >     product_price float)\n",
    "    >     row format delimited fields terminated by \",\"\n",
    "    >     stored as textfile;\n",
    "OK\n",
    "Time taken: 3.555 seconds\n",
    "hive> load data inpath \"/user/pi/retail_db/order_items\" into table order_items;\n",
    "Loading data to table default.order_items\n",
    "OK\n",
    "Time taken: 1.754 seconds\n",
    "hive> select * From order_items limit 10;\n",
    "OK\n",
    "1       1       957     1       299.98  299.98\n",
    "2       2       1073    1       199.99  199.99\n",
    "3       2       502     5       250.0   50.0\n",
    "4       2       403     1       129.99  129.99\n",
    "5       4       897     2       49.98   24.99\n",
    "6       4       365     5       299.95  59.99\n",
    "7       4       502     3       150.0   50.0\n",
    "8       4       1014    4       199.92  49.98\n",
    "9       5       957     1       299.98  299.98\n",
    "10      5       365     5       299.95  59.99\n",
    "Time taken: 4.983 seconds, Fetched: 10 row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create database and tables of ORC file format - orders and order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As our source data in text format ,we need to run insert command to convert data to ORC and store into tables in new Database\n",
    "#Other tan text file format other formats like ORC,parquet,avro,RCFILE stores metadata along with data.\n",
    "#Hence no need to mention delimiter explicitly while creating table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive> describe formated order_items;\n",
    "FAILED: SemanticException [Error 10001]: Table not found formated\n",
    "hive> describe formatted order_items;\n",
    "OK\n",
    "# col_name              data_type               comment\n",
    "order_item_id           int\n",
    "order_id                int\n",
    "product_id              int\n",
    "quantity                int\n",
    "subtotal                float\n",
    "product_price           float\n",
    "\n",
    "# Detailed Table Information\n",
    "Database:               retail_db_txt\n",
    "OwnerType:              USER\n",
    "Owner:                  pi\n",
    "CreateTime:             Sat May 23 11:58:49 BST 2020\n",
    "LastAccessTime:         UNKNOWN\n",
    "Retention:              0\n",
    "Location:               hdfs://raspberrypi1:9000/user/hive/warehouse/retail_db_txt.db/order_items\n",
    "Table Type:             MANAGED_TABLE\n",
    "Table Parameters:\n",
    "        COLUMN_STATS_ACCURATE   {\\\"BASIC_STATS\\\":\\\"true\\\",\\\"COLUMN_STATS\\\":{\\\"order_id\\\":\\\"true\\\",\\\"order_item_id\\\":\\\"true\\\",\\\"product_id\\\":\\\"true\\\",\\\"product_price\\\":\\\"true\\\",\\uantity\\\":\\\"true\\\",\\\"subtotal\\\":\\\"true\\\"}}\n",
    "        bucketing_version       2\n",
    "        numFiles                0\n",
    "        numRows                 0\n",
    "        rawDataSize             0\n",
    "        totalSize               0\n",
    "        transient_lastDdlTime   1590232014\n",
    "\n",
    "# Storage Information\n",
    "SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n",
    "InputFormat:            org.apache.hadoop.mapred.TextInputFormat\n",
    "OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n",
    "Compressed:             No\n",
    "Num Buckets:            -1\n",
    "Bucket Columns:         []\n",
    "Sort Columns:           []\n",
    "Storage Desc Params:\n",
    "        field.delim             ,\n",
    "        serialization.format    ,\n",
    "Time taken: 0.292 seconds, Fetched: 37 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hive> create database retail_db_orc;\n",
    "hive> use retail_db_orc;\n",
    "OK\n",
    "Time taken: 0.121 seconds\n",
    "hive> create table orders (\n",
    "    > order_id int,\n",
    "    > order_date string,\n",
    "    > customer_id int,\n",
    "    > status string\n",
    "    > ) stored as orc;\n",
    "OK\n",
    "Time taken: 0.253 seconds\n",
    "hive> create table order_items(\n",
    "    >     order_item_id int,\n",
    "    >     order_id int,\n",
    "    >     product_id int,\n",
    "    >     quantity int,\n",
    "    >     subtotal float,\n",
    "    >     product_price float) stored as orc;\n",
    "OK\n",
    "Time taken: 0.289 seconds\n",
    "#since the input file in text format we can user insert to insert to table in orc format seleting from text db \"retail_db_txt\"\n",
    "hive> insert into orders select * from retail_db_txt.orders;\n",
    "hive> select * from orders limit 5;\n",
    "OK\n",
    "1       2013-07-25 00:00:00.0   11599   CLOSED\n",
    "2       2013-07-25 00:00:00.0   256     PENDING_PAYMENT\n",
    "3       2013-07-25 00:00:00.0   12111   COMPLETE\n",
    "4       2013-07-25 00:00:00.0   8827    CLOSED\n",
    "5       2013-07-25 00:00:00.0   11318   COMPLETE\n",
    "Time taken: 0.513 seconds, Fetched: 5 row(s)\n",
    "hive> select * from order_items limit 10;\n",
    "OK\n",
    "1       1       957     1       299.98  299.98\n",
    "2       2       1073    1       199.99  199.99\n",
    "3       2       502     5       250.0   50.0\n",
    "4       2       403     1       129.99  129.99\n",
    "5       4       897     2       49.98   24.99\n",
    "6       4       365     5       299.95  59.99\n",
    "7       4       502     3       150.0   50.0\n",
    "8       4       1014    4       199.92  49.98\n",
    "9       5       957     1       299.98  299.98\n",
    "10      5       365     5       299.95  59.99\n",
    "Time taken: 0.461 seconds, Fetched: 10 row(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL/Hive Commands using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hive queries can be executed from pyspark using sqlContext.sql()\n",
    "#result willbe form dataframe\n",
    "#to display show()\n",
    "#or convert to collection using collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0xb021cc30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> sqlContext.sql(\"show databases\").show()\n",
    "+-------------+\n",
    "| databaseName|\n",
    "+-------------+\n",
    "|      default|\n",
    "|retail_db_txt|\n",
    "+-------------+\n",
    ">>> sqlContext.sql(\"use retail_db_txt\")\n",
    "DataFrame[]\n",
    ">>> sqlContext.sql(\"create table orders ( \\\n",
    "... order_id int,\\\n",
    "... order_date string,\\\n",
    "... customer_id int,\\\n",
    "... status string\\\n",
    "... ) row format delimited fields terminated by ','\\\n",
    "...     stored as textfile\")\n",
    "\n",
    "2020-05-23 13:47:35,731 WARN metastore.HiveMetaStore: Location: file:/home/pi/spark-warehouse/retail_db_txt.db/orders specified for non-external table:orders\n",
    "DataFrame[]\n",
    ">>> sqlContext.sql(\"load data inpath '/user/pi/retail_db/orders' into table orders\")\n",
    "DataFrame[]\n",
    ">>> sqlContext.sql(\"select * from orders limit 10\").show()\n",
    "+--------+--------------------+-----------+---------------+\n",
    "|order_id|          order_date|customer_id|         status|\n",
    "+--------+--------------------+-----------+---------------+\n",
    "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
    "+--------+--------------------+-----------+---------------+\n",
    ">>> for rec in sqlContext.sql(\"select * from orders limit 10\").collect():\n",
    "...     print(rec)\n",
    "Row(order_id=1, order_date='2013-07-25 00:00:00.0', customer_id=11599, status='CLOSED')\n",
    "Row(order_id=2, order_date='2013-07-25 00:00:00.0', customer_id=256, status='PENDING_PAYMENT')\n",
    "Row(order_id=3, order_date='2013-07-25 00:00:00.0', customer_id=12111, status='COMPLETE')\n",
    "Row(order_id=4, order_date='2013-07-25 00:00:00.0', customer_id=8827, status='CLOSED')\n",
    "Row(order_id=5, order_date='2013-07-25 00:00:00.0', customer_id=11318, status='COMPLETE')\n",
    "Row(order_id=6, order_date='2013-07-25 00:00:00.0', customer_id=7130, status='COMPLETE')\n",
    "Row(order_id=7, order_date='2013-07-25 00:00:00.0', customer_id=4530, status='COMPLETE')\n",
    "Row(order_id=8, order_date='2013-07-25 00:00:00.0', customer_id=2911, status='PROCESSING')\n",
    "Row(order_id=9, order_date='2013-07-25 00:00:00.0', customer_id=5657, status='PENDING_PAYMENT')\n",
    "Row(order_id=10, order_date='2013-07-25 00:00:00.0', customer_id=5648, status='PENDING_PAYMENT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Other concepts\n",
    "    - Filtering (horizontal and vertical)\n",
    "    - Functions\n",
    "    - Row level Transformations\n",
    "    - Joins\n",
    "    - Aggregation\n",
    "    - Sorting\n",
    "    - Set Operations\n",
    "    - Analystics Function\n",
    "    - Windowing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions - Getting Started"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#to get list of functions\n",
    "#connect hive\n",
    ">show functions;\n",
    "#to get syntax\n",
    ">describe function <fn_name>\n",
    ">describe function sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hive> show functions;\n",
    "***\n",
    "OK| !| !=| $sum0| %| &| *| +| -| /| <| <=| <=>| <>| =| ==| >| >=| ^| abs| acos| add_months| aes_decrypt| aes_encrypt| and| array| array_contains| ascii| asin| assert_true| assert_true_oom| atan| avg| base64| between| bin| bloom_filter| bround| cardinality_violation| case| cbrt| ceil| ceiling| char_length| character_length| chr| coalesce| collect_list| collect_set| compute_stats| concat| concat_ws| context_ngrams| conv| corr| cos| count| covar_pop| covar_samp| crc32| create_union| cume_dist| current_authorizer| current_database| current_date| current_groups| current_timestamp| current_user| date_add| date_format| date_sub| datediff| day| dayofmonth| dayofweek| decode| degrees| dense_rank| div| e| elt| encode| enforce_constraint| exp| explode| extract_union| factorial| field| find_in_set| first_value| floor| floor_day| floor_hour| floor_minute| floor_month| floor_quarter| floor_second| floor_week| floor_year| format_number| from_unixtime| from_utc_timestamp| get_json_object| get_splits| greatest| grouping| hash| hex| histogram_numeric| hour| if| in| in_bloom_filter| in_file| index| initcap| inline| instr| internal_interval| isfalse| isnotfalse| isnotnull| isnottrue| isnull| istrue| java_method| json_tuple| lag| last_day| last_value| lcase| lead| least| length| levenshtein| like| likeall| likeany| ln| locate| log| log10| log2| logged_in_user| lower| lpad| ltrim| map| map_keys| map_values| mask| mask_first_n| mask_hash| mask_last_n| mask_show_first_n| mask_show_last_n| matchpath| max| md5| min| minute| mod| month| months_between| murmur_hash| named_struct| negative| next_day| ngrams| noop| noopstreaming| noopwithmap| noopwithmapstreaming| not| ntile| nullif| nvl| octet_length| or| parse_url| parse_url_tuple| percent_rank| percentile| percentile_approx| pi| pmod| posexplode| positive| pow| power| printf| quarter| radians| rand| rank| reflect| reflect2| regexp| regexp_extract| regexp_replace| regr_avgx| regr_avgy| regr_count| regr_intercept| regr_r2| regr_slope| regr_sxx| regr_sxy| regr_syy| repeat| replace| replicate_rows| restrict_information_schema| reverse| rlike| round| row_number| rpad| rtrim| second| sentences| sha| sha1| sha2| shiftleft| shiftright| shiftrightunsigned| sign| sin| size| sort_array| sort_array_by| soundex| space| split| sq_count_check| sqrt| stack| std| stddev| stddev_pop| stddev_samp| str_to_map| struct| substr| substring| substring_index| sum| tan| to_date| to_epoch_milli| to_unix_timestamp| to_utc_timestamp| translate| trim| trunc| ucase| udftoboolean| udftobyte| udftodouble| udftofloat| udftointeger| udftolong| udftoshort| udftostring| unbase64| unhex| unix_timestamp| upper| uuid| var_pop| var_samp| variance| version| weekofyear| when| width_bucket| windowingtablefunction| xpath| xpath_boolean| xpath_double| xpath_float| xpath_int| xpath_long| xpath_number| xpath_short| xpath_string| year| || ~| \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get syntax\n",
    "hive> describe function length;\n",
    "OK\n",
    "length(str | binary) - Returns the length of str or number of bytes in binary data\n",
    "Time taken: 0.109 seconds, Fetched: 1 row(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"drop database retail_db_txt\")\n",
    "sqlContext.sql(\"create database retail_db_txt\")\n",
    "sqlContext.sql(\"use retail_db_txt\")\n",
    "sqlContext.sql(\"create table orders ( \\\n",
    "  order_id int,\\\n",
    "  order_date string,\\\n",
    "  customer_id int,\\\n",
    "  status string\\\n",
    "  ) row format delimited fields terminated by ','\\\n",
    "      stored as textfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"load data local inpath '/home/pi/shared/retail_db/orders' into table orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from orders limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|         status|count(1)|\n",
      "+---------------+--------+\n",
      "|PENDING_PAYMENT|   30060|\n",
      "|       COMPLETE|   45798|\n",
      "|        ON_HOLD|    7596|\n",
      "| PAYMENT_REVIEW|    1458|\n",
      "|     PROCESSING|   16550|\n",
      "|         CLOSED|   15112|\n",
      "|SUSPECTED_FRAUD|    3116|\n",
      "|        PENDING|   15220|\n",
      "|       CANCELED|    2856|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select status,count(*) from orders group by status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"create table order_items(\\\n",
    "        order_item_id int,\\\n",
    "        order_id int,\\\n",
    "        product_id int,\\\n",
    "        quantity int,\\\n",
    "        subtotal float,\\\n",
    "        product_price float) row format delimited fields terminated by ','\\\n",
    "        stored as textfile\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"load data local inpath '/home/pi/shared/retail_db/order_items' into table order_items\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|product_price|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|            1|       1|       957|       1|  299.98|       299.98|\n",
      "|            2|       2|      1073|       1|  199.99|       199.99|\n",
      "|            3|       2|       502|       5|   250.0|         50.0|\n",
      "|            4|       2|       403|       1|  129.99|       129.99|\n",
      "|            5|       4|       897|       2|   49.98|        24.99|\n",
      "|            6|       4|       365|       5|  299.95|        59.99|\n",
      "|            7|       4|       502|       3|   150.0|         50.0|\n",
      "|            8|       4|      1014|       4|  199.92|        49.98|\n",
      "|            9|       5|       957|       1|  299.98|       299.98|\n",
      "|           10|       5|       365|       5|  299.95|        59.99|\n",
      "|           11|       5|      1014|       2|   99.96|        49.98|\n",
      "|           12|       5|       957|       1|  299.98|       299.98|\n",
      "|           13|       5|       403|       1|  129.99|       129.99|\n",
      "|           14|       7|      1073|       1|  199.99|       199.99|\n",
      "|           15|       7|       957|       1|  299.98|       299.98|\n",
      "|           16|       7|       926|       5|   79.95|        15.99|\n",
      "|           17|       8|       365|       3|  179.97|        59.99|\n",
      "|           18|       8|       365|       5|  299.95|        59.99|\n",
      "|           19|       8|      1014|       4|  199.92|        49.98|\n",
      "|           20|       8|       502|       1|    50.0|         50.0|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from order_items\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions - String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"create table customers ( \\\n",
    "customer_id int,\\\n",
    "customer_fname varchar(45),\\\n",
    "customer_lname varchar(45),\\\n",
    "customer_email varchar(45),\\\n",
    "customer_password varchar(45),\\\n",
    "customer_street varchar(255),\\\n",
    "customer_city varchar(45),\\\n",
    "customer_state varchar(45),\\\n",
    "customer_zipcode varchar(45)) row format delimited fields terminated by ','\\\n",
    "stored as textfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('load data local inpath \"/home/pi/shared/retail_db/customers\" into table customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|           00725|\n",
      "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|\"10 Crystal River...|       Caguas|            PR|           00725|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from customers limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Important string functions\n",
    "- substr or substring\n",
    "- instr\n",
    "- like\n",
    "- rlike\n",
    "- length\n",
    "- lcase or lower\n",
    "- ucase or upper\n",
    "- trim,ltrim,rtrim\n",
    "- lpad/rpad\n",
    "- cast\n",
    "- split/index\n",
    "- initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|substr|\n",
      "+------+\n",
      "|   how|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select substr('hello how are you',7,3) substr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|instr|\n",
      "+-----+\n",
      "|   11|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select instr('hello how are you','are') instr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|hello how are you LIKE %how%|\n",
      "+----------------------------+\n",
      "|                        true|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select 'hello how are you' like '%how%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|0501986734r RLIKE [^0-9]{2}|\n",
      "+---------------------------+\n",
      "|                      false|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select '0501986734r' rlike '[^0-9]{2}'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|CAST(0501986734 AS INT)|\n",
      "+-----------------------+\n",
      "|              501986734|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select cast('0501986734' as int)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(split(hello how are you,  )=['hello', 'how', 'are', 'you'])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select split('hello how are you',' ')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(split(hello how are you,  )=['hello', 'how', 'are', 'you'])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select split('hello how are you',' ') \").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       function_desc|\n",
      "+--------------------+\n",
      "|Function: index n...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"describe function index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Undefined function: 'index'. This function is neither a registered temporary function nor a permanent function registered in the database 'retail_db_txt'.; line 1 pos 7\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.AnalysisException: Undefined function: 'index'. This function is neither a registered temporary function nor a permanent function registered in the database 'retail_db_txt'.; line 1 pos 7\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1291)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15$$anonfun$applyOrElse$49.apply(Analyzer.scala:1291)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1290)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1282)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:258)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:257)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:83)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:83)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:83)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse(AnalysisHelper.scala:129)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressions$1.applyOrElse(AnalysisHelper.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveExpressions(AnalysisHelper.scala:128)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressions(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1282)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1279)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-d47b328be67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select index(split('1,2,3,4',','),2)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Undefined function: 'index'. This function is neither a registered temporary function nor a permanent function registered in the database 'retail_db_txt'.; line 1 pos 7\""
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select index(split('1,2,3,4',','),2)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions - Date Manipulation\n",
    "***\n",
    "\t- current_date\n",
    "\t- current_timestamp\n",
    "\t- date_add\n",
    "\t- date_sub\n",
    "\t- date_format\n",
    "\t- datediff\n",
    "\t- day\n",
    "\t- dayofmonth\n",
    "\t- to_date\n",
    "\t- to_unix_timestamp\n",
    "\t- to_utc_timestamp\n",
    "\t- from_unixtime\n",
    "\t- from_utc_timestamp\n",
    "\t- minute\n",
    "\t- month\n",
    "\t- months_between\n",
    "\t- next_day\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2020-05-23|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select current_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| current_timestamp()|\n",
      "+--------------------+\n",
      "|2020-05-23 19:47:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select current_timestamp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|date_format(current_timestamp(), y)|\n",
      "+-----------------------------------+\n",
      "|                               2020|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select date_format(current_timestamp,'y')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|date_add(CAST(current_timestamp() AS DATE), 10)|\n",
      "+-----------------------------------------------+\n",
      "|                                     2020-06-02|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select date_add(current_timestamp,10)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(function_desc='Function: date_sub'),\n",
       " Row(function_desc='Class: org.apache.spark.sql.catalyst.expressions.DateSub'),\n",
       " Row(function_desc='Usage: date_sub(start_date, num_days) - Returns the date that is `num_days` before `start_date`.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"describe function date_sub\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(function_desc='Function: next_day'),\n",
       " Row(function_desc='Class: org.apache.spark.sql.catalyst.expressions.NextDay'),\n",
       " Row(function_desc='Usage: next_day(start_date, day_of_week) - Returns the first date which is later than `start_date` and named as indicated.')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"describe function next_day\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|dayofmonth(current_date())|\n",
      "+--------------------------+\n",
      "|                        23|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select day(current_date)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|minute(current_timestamp())|\n",
      "+---------------------------+\n",
      "|                         53|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select minute(current_timestamp)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|month(CAST(current_timestamp() AS DATE))|\n",
      "+----------------------------------------+\n",
      "|                                       5|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select month(current_timestamp)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|to_unix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss)|\n",
      "+-----------------------------------------------------------+\n",
      "|                                                 1590260058|\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select to_unix_timestamp(current_timestamp)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|from_unixtime(CAST(1590260058 AS BIGINT), yyyy-MM-dd HH:mm:ss)|\n",
      "+--------------------------------------------------------------+\n",
      "|                                           2020-05-23 19:54:18|\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select from_unixtime(1590260058)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Functions - Aggregate Functions in brief\n",
    "***\n",
    "    - count\n",
    "    - avg\n",
    "    - min\n",
    "    - max\n",
    "    - sum\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|total_records| st|\n",
      "+-------------+---+\n",
      "|       206649|  9|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(*) total_records,count(distinct status) st from orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions - case and nvl\n",
    "***\n",
    "    - case\n",
    "    - nvl\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from orders limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|   status|count(1)|\n",
      "+---------+--------+\n",
      "|COMPLETED|   91365|\n",
      "|  PENDING|  115284|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select case status when 'CLOSED' then 'COMPLETED'\\\n",
    "                                   when 'COMPLETE' then 'COMPLETED'\\\n",
    "                                   else 'PENDING' END status,count(*) from orders group by case status when 'CLOSED' then 'COMPLETED'\\\n",
    "                                   when 'COMPLETE' then 'COMPLETED'\\\n",
    "                                   else 'PENDING' END\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(function_desc='Function: case'),\n",
       " Row(function_desc='Usage: CASE expr1 WHEN expr2 THEN expr3 [WHEN expr4 THEN expr5]* [ELSE expr6] END - When `expr1` = `expr2`, returns `expr3`; when `expr1` = `expr4`, return `expr5`; else return `expr6`.')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"describe function case\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|   status|count(1)|\n",
      "+---------+--------+\n",
      "|COMPLETED|   91365|\n",
      "|  PENDING|  115284|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select case  when status in ('CLOSED','COMPLETE') then 'COMPLETED'\\\n",
    "                                   else 'PENDING' END status,count(*) from orders group by case  when status in ('CLOSED','COMPLETE') then 'COMPLETED'\\\n",
    "                                   else 'PENDING' END\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|nvl(NULL, 'Missing')|\n",
      "+--------------------+\n",
      "|             Missing|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select nvl(null,'Missing')\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row level transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| databaseName|\n",
      "+-------------+\n",
      "|      default|\n",
      "|retail_db_txt|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   mon|\n",
      "+------+\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "|201307|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select cast(concat(substr(order_date,1,4),substr(order_date,6,2)) as int) mon From orders limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   mon|\n",
      "+------+\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "|201300|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select cast(date_format(order_date,'yyyymm') as int) mon From orders limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining data between multiple tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
      "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
      "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|           00725|\n",
      "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
      "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|\"10 Crystal River...|       Caguas|            PR|           00725|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from customers limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from orders limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* types of joins are\n",
    "    - join\n",
    "    - left outer join\n",
    "    - right outer join\n",
    "    - full outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+--------+--------------------+-----------+---------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|order_id|          order_date|customer_id|         status|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+--------+--------------------+-----------+---------------+\n",
      "|      11599|          Mary|        Malone|     XXXXXXXXX|        XXXXXXXXX|8708 Indian Horse...|      Hickory|            NC|           28601|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|        256|         David|     Rodriguez|     XXXXXXXXX|        XXXXXXXXX|7605 Tawny Horse ...|      Chicago|            IL|           60625|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|      12111|         Amber|        Franco|     XXXXXXXXX|        XXXXXXXXX|8766 Clear Prairi...|   Santa Cruz|            CA|           95060|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       8827|         Brian|        Wilson|     XXXXXXXXX|        XXXXXXXXX|   8396 High Corners|  San Antonio|            TX|           78240|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|      11318|          Mary|         Henry|     XXXXXXXXX|        XXXXXXXXX|3047 Silent Ember...|       Caguas|            PR|           00725|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       7130|         Alice|         Smith|     XXXXXXXXX|        XXXXXXXXX|      8852 Iron Port|     Brooklyn|            NY|           11237|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       4530|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|1073 Green Leaf G...|        Miami|            FL|           33161|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       2911|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|9166 Golden Necta...|       Caguas|            PR|           00725|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       5657|          Mary|         James|     XXXXXXXXX|        XXXXXXXXX|  1389 Dusty Circuit|     Lakewood|            OH|           44107|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|       5648|        Joshua|         Smith|     XXXXXXXXX|        XXXXXXXXX|864 Iron Spring S...|      Memphis|            TN|           38111|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select c.*,o.* from customers c,orders o where c.customer_id=o.customer_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+--------+--------------------+-----------+---------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|order_id|          order_date|customer_id|         status|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+--------+--------------------+-----------+---------------+\n",
      "|      11599|          Mary|        Malone|     XXXXXXXXX|        XXXXXXXXX|8708 Indian Horse...|      Hickory|            NC|           28601|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|        256|         David|     Rodriguez|     XXXXXXXXX|        XXXXXXXXX|7605 Tawny Horse ...|      Chicago|            IL|           60625|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|      12111|         Amber|        Franco|     XXXXXXXXX|        XXXXXXXXX|8766 Clear Prairi...|   Santa Cruz|            CA|           95060|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       8827|         Brian|        Wilson|     XXXXXXXXX|        XXXXXXXXX|   8396 High Corners|  San Antonio|            TX|           78240|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|      11318|          Mary|         Henry|     XXXXXXXXX|        XXXXXXXXX|3047 Silent Ember...|       Caguas|            PR|           00725|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       7130|         Alice|         Smith|     XXXXXXXXX|        XXXXXXXXX|      8852 Iron Port|     Brooklyn|            NY|           11237|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       4530|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|1073 Green Leaf G...|        Miami|            FL|           33161|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       2911|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|9166 Golden Necta...|       Caguas|            PR|           00725|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       5657|          Mary|         James|     XXXXXXXXX|        XXXXXXXXX|  1389 Dusty Circuit|     Lakewood|            OH|           44107|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|       5648|        Joshua|         Smith|     XXXXXXXXX|        XXXXXXXXX|864 Iron Spring S...|      Memphis|            TN|           38111|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+--------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select c.*,o.* from customers c join orders o on c.customer_id=o.customer_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   68913|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select count(*) from customers c left outer join orders o on c.customer_id=o.customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      30|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#customer who didnt place orders yet\n",
    "sqlContext.sql(\"select count(*) from customers c left outer join orders o on c.customer_id=o.customer_id where o.customer_id is null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by and aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|product_price|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|            1|       1|       957|       1|  299.98|       299.98|\n",
      "|            2|       2|      1073|       1|  199.99|       199.99|\n",
      "|            3|       2|       502|       5|   250.0|         50.0|\n",
      "|            4|       2|       403|       1|  129.99|       129.99|\n",
      "|            5|       4|       897|       2|   49.98|        24.99|\n",
      "|            6|       4|       365|       5|  299.95|        59.99|\n",
      "|            7|       4|       502|       3|   150.0|         50.0|\n",
      "|            8|       4|      1014|       4|  199.92|        49.98|\n",
      "|            9|       5|       957|       1|  299.98|       299.98|\n",
      "|           10|       5|       365|       5|  299.95|        59.99|\n",
      "|           11|       5|      1014|       2|   99.96|        49.98|\n",
      "|           12|       5|       957|       1|  299.98|       299.98|\n",
      "|           13|       5|       403|       1|  129.99|       129.99|\n",
      "|           14|       7|      1073|       1|  199.99|       199.99|\n",
      "|           15|       7|       957|       1|  299.98|       299.98|\n",
      "|           16|       7|       926|       5|   79.95|        15.99|\n",
      "|           17|       8|       365|       3|  179.97|        59.99|\n",
      "|           18|       8|       365|       5|  299.95|        59.99|\n",
      "|           19|       8|      1014|       4|  199.92|        49.98|\n",
      "|           20|       8|       502|       1|    50.0|         50.0|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#daily revenue for complete and closed orders\n",
    "sqlContext.sql(\"select * from order_items\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|order_date| revenue|\n",
      "+----------+--------+\n",
      "|2013-09-09|49372.42|\n",
      "|2013-09-19|51600.34|\n",
      "|2014-06-03| 29006.2|\n",
      "|2013-09-12|42801.83|\n",
      "|2014-01-24|33855.51|\n",
      "|2014-02-16|55914.86|\n",
      "|2014-06-11|37022.75|\n",
      "|2013-11-18|44898.19|\n",
      "|2014-02-18|45412.09|\n",
      "|2013-08-14|42043.45|\n",
      "|2013-10-05|43319.77|\n",
      "|2014-07-04|25467.71|\n",
      "|2014-07-06|16451.76|\n",
      "|2013-09-18|49899.08|\n",
      "|2013-09-20|29575.36|\n",
      "|2013-09-25|61042.05|\n",
      "|2014-06-13|52380.18|\n",
      "|2013-11-23|55605.42|\n",
      "|2013-09-14|60928.16|\n",
      "|2014-02-24|38936.61|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select to_date(order_date) order_date,round(sum(subtotal),2) revenue \\\n",
    "from orders o join order_items oi \\\n",
    "on o.order_id=oi.order_id \\\n",
    "where o.status in ('CLOSED','COMPLETE')\\\n",
    "group by to_date(order_date)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|order_date| revenue|\n",
      "+----------+--------+\n",
      "|2013-07-25|31547.23|\n",
      "|2013-07-26|54713.23|\n",
      "|2013-07-27|48411.48|\n",
      "|2013-07-28|35672.03|\n",
      "|2013-07-29| 54579.7|\n",
      "|2013-07-30|49329.29|\n",
      "|2013-07-31|59212.49|\n",
      "|2013-08-01|49160.08|\n",
      "|2013-08-02|50688.58|\n",
      "|2013-08-03|43416.74|\n",
      "|2013-08-04|35093.01|\n",
      "|2013-08-05|34025.27|\n",
      "|2013-08-06|57843.89|\n",
      "|2013-08-07|45525.59|\n",
      "|2013-08-08|33549.47|\n",
      "|2013-08-09|29225.16|\n",
      "|2013-08-10|46435.04|\n",
      "|2013-08-11| 31155.5|\n",
      "|2013-08-12|59014.74|\n",
      "|2013-08-13|17956.88|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select to_date(order_date) order_date,round(sum(subtotal),2) revenue \\\n",
    "from orders o join order_items oi \\\n",
    "on o.order_id=oi.order_id \\\n",
    "where o.status in ('CLOSED','COMPLETE')\\\n",
    "group by to_date(order_date)\\\n",
    "order by order_date,revenue desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|order_date| revenue|\n",
      "+----------+--------+\n",
      "|2013-09-09|49372.42|\n",
      "|2013-09-19|51600.34|\n",
      "|2014-06-03| 29006.2|\n",
      "|2013-09-12|42801.83|\n",
      "|2014-01-24|33855.51|\n",
      "|2014-02-16|55914.86|\n",
      "|2014-06-11|37022.75|\n",
      "|2013-11-18|44898.19|\n",
      "|2014-02-18|45412.09|\n",
      "|2013-08-14|42043.45|\n",
      "|2013-10-05|43319.77|\n",
      "|2014-07-04|25467.71|\n",
      "|2014-07-06|16451.76|\n",
      "|2013-09-18|49899.08|\n",
      "|2013-09-20|29575.36|\n",
      "|2013-09-25|61042.05|\n",
      "|2014-06-13|52380.18|\n",
      "|2013-11-23|55605.42|\n",
      "|2013-09-14|60928.16|\n",
      "|2014-02-24|38936.61|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#distribute by  - sort by \n",
    "#in above query if order by order_date is not taht important ,but within in order_date need to sort by revenue\n",
    "sqlContext.sql(\"select to_date(order_date) order_date,round(sum(subtotal),2) revenue \\\n",
    "from orders o join order_items oi \\\n",
    "on o.order_id=oi.order_id \\\n",
    "where o.status in ('CLOSED','COMPLETE')\\\n",
    "group by to_date(order_date)\\\n",
    "distribute by order_date sort by order_date,revenue desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set operations - union and union all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* union\n",
    "* union all\n",
    "    - both same logic in oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics functions - aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+-------------+--------+----------+--------+--------+-------------+\n",
      "|order_id|          order_date|customer_id|         status|order_item_id|order_id|product_id|quantity|subtotal|product_price|\n",
      "+--------+--------------------+-----------+---------------+-------------+--------+----------+--------+--------+-------------+\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|            2|       2|      1073|       1|  199.99|       199.99|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|            3|       2|       502|       5|   250.0|         50.0|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|            4|       2|       403|       1|  129.99|       129.99|\n",
      "+--------+--------------------+-----------+---------------+-------------+--------+----------+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from orders o join order_items oi on o.order_id=oi.order_id where oi.order_id=2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+----------------+\n",
      "|order_id|order_date|subtotal|percentage_total|\n",
      "+--------+----------+--------+----------------+\n",
      "|       2|2013-07-25|  199.99|           34.48|\n",
      "|       2|2013-07-25|   250.0|            43.1|\n",
      "|       2|2013-07-25|  129.99|           22.41|\n",
      "+--------+----------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select o.order_id,to_date(o.order_date) order_date,oi.subtotal, \\\n",
    "round((subtotal/(sum(subtotal) over (partition by o.order_id)))*100,2) percentage_total \\\n",
    "from orders o join order_items oi on o.order_id=oi.order_id where oi.order_id=2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+----------------+\n",
      "|order_id|order_date|subtotal|percentage_total|\n",
      "+--------+----------+--------+----------------+\n",
      "|    1580|2013-08-02|  299.95|           100.0|\n",
      "|    2366|2013-08-06|  299.97|           100.0|\n",
      "|    3749|2013-08-15|  143.97|           100.0|\n",
      "|    3794|2013-08-16|  299.95|           100.0|\n",
      "|    4101|2013-08-18|  129.99|           100.0|\n",
      "|    4519|2013-08-21|   79.98|           100.0|\n",
      "|    4818|2013-08-23|  399.98|           100.0|\n",
      "|    5518|2013-08-27|  199.95|           100.0|\n",
      "|    6397|2013-09-03|  399.98|           100.0|\n",
      "|    6620|2013-09-05|  399.98|           100.0|\n",
      "|    7554|2013-09-09|  199.99|           100.0|\n",
      "|    7880|2013-09-11|   99.96|           100.0|\n",
      "|    9900|2013-09-25|   39.99|           100.0|\n",
      "|   10817|2013-09-29|   100.0|           100.0|\n",
      "|   11141|2013-10-02|  399.98|           100.0|\n",
      "|   12940|2013-10-12|  149.94|           100.0|\n",
      "|   14570|2013-10-25|   39.99|           100.0|\n",
      "|   15619|2013-11-01|   150.0|           100.0|\n",
      "|   16503|2013-11-05|  399.98|           100.0|\n",
      "|   17420|2013-11-10|   63.96|           100.0|\n",
      "+--------+----------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#in hive subquery always to be aliased\n",
    "#get those order have percentate total as 100\n",
    "sqlContext.sql(\"select * from (select o.order_id,to_date(o.order_date) order_date,oi.subtotal, \\\n",
    "round((subtotal/(sum(subtotal) over (partition by o.order_id)))*100,2) percentage_total \\\n",
    "from orders o join order_items oi on o.order_id=oi.order_id) q where percentage_total=100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics functions - ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+----------------+-----------+-----------------+---------------+------------------+----------+\n",
      "|order_id|order_date|subtotal|percentage_total|rnk_revenue|dense_rnk_revenue|pct_rnk_revenue|rn_orderid_revenue|rn_revenue|\n",
      "+--------+----------+--------+----------------+-----------+-----------------+---------------+------------------+----------+\n",
      "|       5|2013-07-25|  299.98|           26.55|          1|                1|            0.0|                 1|         9|\n",
      "|       5|2013-07-25|  299.98|           26.55|          1|                1|            0.0|                 2|        10|\n",
      "|       5|2013-07-25|  299.95|           26.55|          3|                2|            0.5|                 3|        11|\n",
      "|       5|2013-07-25|  129.99|            11.5|          4|                3|           0.75|                 4|        12|\n",
      "|       5|2013-07-25|   99.96|            8.85|          5|                4|            1.0|                 5|        13|\n",
      "+--------+----------+--------+----------------+-----------+-----------------+---------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from (select o.order_id,to_date(o.order_date) order_date,oi.subtotal, \\\n",
    "round((subtotal/(sum(subtotal) over (partition by o.order_id)))*100,2) percentage_total, \\\n",
    "rank() over (partition by o.order_id order by oi.subtotal desc) rnk_revenue, \\\n",
    "dense_rank() over (partition by o.order_id order by oi.subtotal desc) dense_rnk_revenue, \\\n",
    "percent_rank() over (partition by o.order_id order by oi.subtotal desc) pct_rnk_revenue, \\\n",
    "row_number() over (partition by o.order_id order by oi.subtotal desc) rn_orderid_revenue, \\\n",
    "row_number() over (order by o.order_id) rn_revenue \\\n",
    "from orders o join order_items oi on o.order_id=oi.order_id) q where order_id=5\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LEAD\n",
    "- LAG\n",
    "- FIRST_VALUE\n",
    "- LAST_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+---------+-------+-----------+-----------+------------+------+-----------+----------+-----+------+\n",
      "|order_id|    ord_dt|subtotal|per_total|rnk_rev|dns_rnk_rev|pct_rnk_rev|rn_ordid_rev|rn_rev|lead_subtot|lag_subtot|f_val| l_val|\n",
      "+--------+----------+--------+---------+-------+-----------+-----------+------------+------+-----------+----------+-----+------+\n",
      "|       5|2013-07-25|  299.98|    26.55|      1|          1|        0.0|           1|     9|     299.98|    299.95|99.96|299.98|\n",
      "|       5|2013-07-25|  299.98|    26.55|      1|          1|        0.0|           2|    12|       null|    299.98|99.96|299.98|\n",
      "|       5|2013-07-25|  299.95|    26.55|      3|          2|        0.5|           3|    10|     299.98|    129.99|99.96|299.95|\n",
      "|       5|2013-07-25|  129.99|     11.5|      4|          3|       0.75|           4|    13|     299.95|     99.96|99.96|129.99|\n",
      "|       5|2013-07-25|   99.96|     8.85|      5|          4|        1.0|           5|    11|     129.99|      null|99.96| 99.96|\n",
      "+--------+----------+--------+---------+-------+-----------+-----------+------------+------+-----------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from (select o.order_id,to_date(o.order_date) ord_dt,oi.subtotal, \\\n",
    "round((subtotal/(sum(subtotal) over (partition by o.order_id)))*100,2) per_total, \\\n",
    "rank() over (partition by o.order_id order by oi.subtotal desc) rnk_rev, \\\n",
    "dense_rank() over (partition by o.order_id order by oi.subtotal desc) dns_rnk_rev, \\\n",
    "percent_rank() over (partition by o.order_id order by oi.subtotal desc) pct_rnk_rev, \\\n",
    "row_number() over (partition by o.order_id order by oi.subtotal desc) rn_ordid_rev, \\\n",
    "row_number() over (order by o.order_id) rn_rev, \\\n",
    "lead(oi.subtotal) over (partition by o.order_id order by oi.subtotal) lead_subtot, \\\n",
    "lag(oi.subtotal) over (partition by o.order_id order by oi.subtotal) lag_subtot, \\\n",
    "first_value(oi.subtotal) over (partition by o.order_id order by oi.subtotal) f_val, \\\n",
    "last_value(oi.subtotal) over (partition by o.order_id order by oi.subtotal) l_val \\\n",
    "from orders o join order_items oi on o.order_id=oi.order_id) q where order_id=5 \\\n",
    "order by subtotal desc\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Frames and register as temp tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get daily revenue by product considering completed and closed orders.\n",
    "\t- PRODUCTS have to be read from local file system.DataFrame need to be created\n",
    "\t- Join Orders,ORDER_ITEMS\n",
    "\t- Filter on ORDER_STATUS\n",
    "* Data need to be sorted by ascending order by date and thendescending order by revenue computed for each product for each day\n",
    "\t- Sort data by order_date in ascending order and then daily revenue per product in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| databaseName|\n",
      "+-------------+\n",
      "|      default|\n",
      "|retail_db_txt|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"use retail_db_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to print Dataframe model\n",
    "sqlContext.sql(\"select * from orders\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from HDFS to RDD and then convert to Dataframe and then to temptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2013-07-25 00:00:00.0,11599,CLOSED',\n",
       " '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT',\n",
       " '3,2013-07-25 00:00:00.0,12111,COMPLETE',\n",
       " '4,2013-07-25 00:00:00.0,8827,CLOSED',\n",
       " '5,2013-07-25 00:00:00.0,11318,COMPLETE',\n",
       " '6,2013-07-25 00:00:00.0,7130,COMPLETE',\n",
       " '7,2013-07-25 00:00:00.0,4530,COMPLETE',\n",
       " '8,2013-07-25 00:00:00.0,2911,PROCESSING',\n",
       " '9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT',\n",
       " '10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data from HDFS\n",
    "orders=sc.textFile(\"/user/pi/retail_db/orders\")\n",
    "orders.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to RDD\n",
    "from pyspark.sql import Row\n",
    "ordersDF = orders.map(lambda o:Row(order_id=int(o.split(\",\")[0]),order_date=o.split(',')[1].split(\" \")[0], \\\n",
    "                                   customer_id=int(o.split(\",\")[2]),order_status=o.split(\",\")[3])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+---------------+\n",
      "|customer_id|order_date|order_id|   order_status|\n",
      "+-----------+----------+--------+---------------+\n",
      "|      11599|2013-07-25|       1|         CLOSED|\n",
      "|        256|2013-07-25|       2|PENDING_PAYMENT|\n",
      "|      12111|2013-07-25|       3|       COMPLETE|\n",
      "|       8827|2013-07-25|       4|         CLOSED|\n",
      "|      11318|2013-07-25|       5|       COMPLETE|\n",
      "|       7130|2013-07-25|       6|       COMPLETE|\n",
      "|       4530|2013-07-25|       7|       COMPLETE|\n",
      "|       2911|2013-07-25|       8|     PROCESSING|\n",
      "|       5657|2013-07-25|       9|PENDING_PAYMENT|\n",
      "|       5648|2013-07-25|      10|PENDING_PAYMENT|\n",
      "|        918|2013-07-25|      11| PAYMENT_REVIEW|\n",
      "|       1837|2013-07-25|      12|         CLOSED|\n",
      "|       9149|2013-07-25|      13|PENDING_PAYMENT|\n",
      "|       9842|2013-07-25|      14|     PROCESSING|\n",
      "|       2568|2013-07-25|      15|       COMPLETE|\n",
      "|       7276|2013-07-25|      16|PENDING_PAYMENT|\n",
      "|       2667|2013-07-25|      17|       COMPLETE|\n",
      "|       1205|2013-07-25|      18|         CLOSED|\n",
      "|       9488|2013-07-25|      19|PENDING_PAYMENT|\n",
      "|       9198|2013-07-25|      20|     PROCESSING|\n",
      "+-----------+----------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create temp table\n",
    "ordersDF.registerTempTable(\"ordersDF_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+---------------+\n",
      "|customer_id|order_date|order_id|   order_status|\n",
      "+-----------+----------+--------+---------------+\n",
      "|      11599|2013-07-25|       1|         CLOSED|\n",
      "|        256|2013-07-25|       2|PENDING_PAYMENT|\n",
      "|      12111|2013-07-25|       3|       COMPLETE|\n",
      "|       8827|2013-07-25|       4|         CLOSED|\n",
      "|      11318|2013-07-25|       5|       COMPLETE|\n",
      "|       7130|2013-07-25|       6|       COMPLETE|\n",
      "|       4530|2013-07-25|       7|       COMPLETE|\n",
      "|       2911|2013-07-25|       8|     PROCESSING|\n",
      "|       5657|2013-07-25|       9|PENDING_PAYMENT|\n",
      "|       5648|2013-07-25|      10|PENDING_PAYMENT|\n",
      "|        918|2013-07-25|      11| PAYMENT_REVIEW|\n",
      "|       1837|2013-07-25|      12|         CLOSED|\n",
      "|       9149|2013-07-25|      13|PENDING_PAYMENT|\n",
      "|       9842|2013-07-25|      14|     PROCESSING|\n",
      "|       2568|2013-07-25|      15|       COMPLETE|\n",
      "|       7276|2013-07-25|      16|PENDING_PAYMENT|\n",
      "|       2667|2013-07-25|      17|       COMPLETE|\n",
      "|       1205|2013-07-25|      18|         CLOSED|\n",
      "|       9488|2013-07-25|      19|PENDING_PAYMENT|\n",
      "|       9198|2013-07-25|      20|     PROCESSING|\n",
      "+-----------+----------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from ordersDF_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to set no:of tasks for a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\",\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get daily revenue by product considering completed and closed orders.\n",
    "\t- PRODUCTS have to be read from local file system.DataFrame need to be created\n",
    "\t- Join Orders,ORDER_ITEMS\n",
    "\t- Filter on ORDER_STATUS\n",
    "* Data need to be sorted by ascending order by date and thendescending order by revenue computed for each product for each day\n",
    "\t- Sort data by order_date in ascending order and then daily revenue per product in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy',\n",
       " \"2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\",\n",
       " \"3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\",\n",
       " \"4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat\",\n",
       " '5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet',\n",
       " \"6,2,Jordan Men's VI Retro TD Football Cleat,,134.99,http://images.acmesports.sports/Jordan+Men%27s+VI+Retro+TD+Football+Cleat\",\n",
       " '7,2,Schutt Youth Recruit Hybrid Custom Football H,,99.99,http://images.acmesports.sports/Schutt+Youth+Recruit+Hybrid+Custom+Football+Helmet+2014',\n",
       " \"8,2,Nike Men's Vapor Carbon Elite TD Football Cle,,129.99,http://images.acmesports.sports/Nike+Men%27s+Vapor+Carbon+Elite+TD+Football+Cleat\",\n",
       " '9,2,Nike Adult Vapor Jet 3.0 Receiver Gloves,,50.0,http://images.acmesports.sports/Nike+Adult+Vapor+Jet+3.0+Receiver+Gloves',\n",
       " \"10,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read product data from local file system and convert to table\n",
    "productsRaw = open(\"/home/pi/shared/retail_db/products/part-00000\").read().splitlines()\n",
    "productsRDD = sc.parallelize(productsRaw)\n",
    "productsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert RDD to DF and then to table\n",
    "from pyspark.sql import Row\n",
    "productsDF=productsRDD.map(lambda p:Row(product_id=int(p.split(\",\")[0]),product_name=p.split(\",\")[2])).toDF()\n",
    "productsDF.registerTempTable(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|product_id|        product_name|\n",
      "+----------+--------------------+\n",
      "|         1|Quest Q64 10 FT. ...|\n",
      "|         2|Under Armour Men'...|\n",
      "|         3|Under Armour Men'...|\n",
      "|         4|Under Armour Men'...|\n",
      "|         5|Riddell Youth Rev...|\n",
      "|         6|Jordan Men's VI R...|\n",
      "|         7|Schutt Youth Recr...|\n",
      "|         8|Nike Men's Vapor ...|\n",
      "|         9|Nike Adult Vapor ...|\n",
      "|        10|Under Armour Men'...|\n",
      "|        11|Fitness Gear 300 ...|\n",
      "|        12|Under Armour Men'...|\n",
      "|        13|Under Armour Men'...|\n",
      "|        14|Quik Shade Summit...|\n",
      "|        15|Under Armour Kids...|\n",
      "|        16|Riddell Youth 360...|\n",
      "|        17|Under Armour Men'...|\n",
      "|        18|Reebok Men's Full...|\n",
      "|        19|Nike Men's Finger...|\n",
      "|        20|Under Armour Men'...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read products\n",
    "sqlContext.sql(\"select * from products\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read order\n",
    "sqlContext.sql(\"use retail_db_txt\")\n",
    "sqlContext.sql(\"select * from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|order_item_id|order_id|product_id|quantity|subtotal|product_price|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "|            1|       1|       957|       1|  299.98|       299.98|\n",
      "|            2|       2|      1073|       1|  199.99|       199.99|\n",
      "|            3|       2|       502|       5|   250.0|         50.0|\n",
      "|            4|       2|       403|       1|  129.99|       129.99|\n",
      "|            5|       4|       897|       2|   49.98|        24.99|\n",
      "|            6|       4|       365|       5|  299.95|        59.99|\n",
      "|            7|       4|       502|       3|   150.0|         50.0|\n",
      "|            8|       4|      1014|       4|  199.92|        49.98|\n",
      "|            9|       5|       957|       1|  299.98|       299.98|\n",
      "|           10|       5|       365|       5|  299.95|        59.99|\n",
      "|           11|       5|      1014|       2|   99.96|        49.98|\n",
      "|           12|       5|       957|       1|  299.98|       299.98|\n",
      "|           13|       5|       403|       1|  129.99|       129.99|\n",
      "|           14|       7|      1073|       1|  199.99|       199.99|\n",
      "|           15|       7|       957|       1|  299.98|       299.98|\n",
      "|           16|       7|       926|       5|   79.95|        15.99|\n",
      "|           17|       8|       365|       3|  179.97|        59.99|\n",
      "|           18|       8|       365|       5|  299.95|        59.99|\n",
      "|           19|       8|      1014|       4|  199.92|        49.98|\n",
      "|           20|       8|       502|       1|    50.0|         50.0|\n",
      "+-------------+--------+----------+--------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from order_items\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|order_date|        product_name|revenue|\n",
      "+----------+--------------------+-------+\n",
      "|2013-07-25|Field & Stream Sp...|5599.72|\n",
      "|2013-07-25|Nike Men's Free 5...|5099.49|\n",
      "|2013-07-25|Diamondback Women...| 4499.7|\n",
      "|2013-07-25|Perfect Fitness P...|3359.44|\n",
      "|2013-07-25|Pelican Sunstream...|2999.85|\n",
      "|2013-07-25|O'Brien Men's Neo...|2798.88|\n",
      "|2013-07-25|Nike Men's CJ Eli...|1949.85|\n",
      "|2013-07-25|Nike Men's Dri-FI...| 1650.0|\n",
      "|2013-07-25|Under Armour Girl...|1079.73|\n",
      "|2013-07-25|Bowflex SelectTec...| 599.99|\n",
      "|2013-07-25|Elevation Trainin...| 319.96|\n",
      "|2013-07-25|Titleist Pro V1 H...| 207.96|\n",
      "|2013-07-25|Nike Men's Kobe I...| 199.99|\n",
      "|2013-07-25|Cleveland Golf Wo...| 119.99|\n",
      "|2013-07-25|TYR Boys' Team Di...| 119.97|\n",
      "|2013-07-25|Merrell Men's All...| 109.99|\n",
      "|2013-07-25|LIJA Women's Butt...|  108.0|\n",
      "|2013-07-25|Nike Women's Lege...|  100.0|\n",
      "|2013-07-25|Team Golf Tenness...|  99.96|\n",
      "|2013-07-25|Bridgestone e6 St...|  95.97|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select to_date(o.order_date) order_date,p.product_name,round(sum(oi.subtotal),2) revenue \\\n",
    "from orders o join order_items oi on (o.order_id=oi.order_id) \\\n",
    "              join products p on (p.product_id=oi.product_id) \\\n",
    "where o.status in ('COMPLETE','CLOSED') \\\n",
    "group by to_date(o.order_date),p.product_name \\\n",
    "order by order_date,revenue desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+-----------+\n",
      "|     database|     tableName|isTemporary|\n",
      "+-------------+--------------+-----------+\n",
      "|retail_db_txt|     customers|      false|\n",
      "|retail_db_txt|   order_items|      false|\n",
      "|retail_db_txt|        orders|      false|\n",
      "|             |ordersdf_table|       true|\n",
      "|             |      products|       true|\n",
      "+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Spark Application - Saving Data Frame to Hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Hive and store the output to hive database\n",
    "\t- Get order_date,product_name,daily_revenue_per_product and save into Hive table using ORC file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"create database ameen_daily_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"create table daily_revenue (order_date string,product_name string,revenue float) stored as orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[order_date: string, product_name: string, revenue: float]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from ameen_daily_revenue.daily_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyRevenue_df=sqlContext.sql(\"select to_date(o.order_date) order_date,p.product_name,round(sum(oi.subtotal),2) revenue \\\n",
    "from orders o join order_items oi on (o.order_id=oi.order_id) \\\n",
    "              join products p on (p.product_id=oi.product_id) \\\n",
    "where o.status in ('COMPLETE','CLOSED') \\\n",
    "group by to_date(o.order_date),p.product_name \\\n",
    "order by order_date,revenue desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyRevenue_df.write.insertInto(\"ameen_daily_revenue.daily_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|order_date|        product_name|revenue|\n",
      "+----------+--------------------+-------+\n",
      "|2014-01-21|Mio ALPHA Heart R...|  199.0|\n",
      "|2014-01-21|Nike Men's Comfor...| 179.96|\n",
      "|2014-01-21|Titleist Pro V1x ...| 155.97|\n",
      "|2014-01-21|Glove It Women's ...| 153.93|\n",
      "|2014-01-21|Nike Men's Deutsc...|  150.0|\n",
      "|2014-01-21|Merrell Women's G...| 129.99|\n",
      "|2014-01-21|LIJA Women's Mid-...|  100.0|\n",
      "|2014-01-21|Team Golf Pittsbu...|  99.96|\n",
      "|2014-01-21|Titleist Pro V1x ...|  95.98|\n",
      "|2014-01-21|Bridgestone e6 St...|  95.97|\n",
      "|2014-01-21|Glove It Imperial...|  63.96|\n",
      "|2014-01-21|Nike Women's Temp...|   60.0|\n",
      "|2014-01-21|Bag Boy Beverage ...|  49.98|\n",
      "|2014-01-21|Team Golf New Eng...|  49.98|\n",
      "|2014-01-21|Glove It Urban Br...|  47.97|\n",
      "|2014-01-21|Bridgestone e6 St...|  31.99|\n",
      "|2014-01-21|Nike Dri-FIT Crew...|   22.0|\n",
      "|2014-01-22|Field & Stream Sp...|11999.4|\n",
      "|2014-01-22|Diamondback Women...|6599.56|\n",
      "|2014-01-22|Nike Men's Dri-FI...| 6000.0|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from ameen_daily_revenue.daily_revenue\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method2\n",
    "sqlContext.sql(\"create table ameen_daily_revenue.daily_revenue_2 (order_date string,product_name string,revenue float) stored as orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"insert into ameen_daily_revenue.daily_revenue_2 select to_date(o.order_date) order_date,p.product_name,round(sum(oi.subtotal),2) revenue \\\n",
    "from orders o join order_items oi on (o.order_id=oi.order_id) \\\n",
    "              join products p on (p.product_id=oi.product_id) \\\n",
    "where o.status in ('COMPLETE','CLOSED') \\\n",
    "group by to_date(o.order_date),p.product_name \\\n",
    "order by order_date,revenue desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|order_date|        product_name|revenue|\n",
      "+----------+--------------------+-------+\n",
      "|2013-07-25|Field & Stream Sp...|5599.72|\n",
      "|2013-07-25|Nike Men's Free 5...|5099.49|\n",
      "|2013-07-25|Diamondback Women...| 4499.7|\n",
      "|2013-07-25|Perfect Fitness P...|3359.44|\n",
      "|2013-07-25|Pelican Sunstream...|2999.85|\n",
      "|2013-07-25|O'Brien Men's Neo...|2798.88|\n",
      "|2013-07-25|Nike Men's CJ Eli...|1949.85|\n",
      "|2013-07-25|Nike Men's Dri-FI...| 1650.0|\n",
      "|2013-07-25|Under Armour Girl...|1079.73|\n",
      "|2013-07-25|Bowflex SelectTec...| 599.99|\n",
      "|2013-07-25|Elevation Trainin...| 319.96|\n",
      "|2013-07-25|Titleist Pro V1 H...| 207.96|\n",
      "|2013-07-25|Nike Men's Kobe I...| 199.99|\n",
      "|2013-07-25|Cleveland Golf Wo...| 119.99|\n",
      "|2013-07-25|TYR Boys' Team Di...| 119.97|\n",
      "|2013-07-25|Merrell Men's All...| 109.99|\n",
      "|2013-07-25|LIJA Women's Butt...|  108.0|\n",
      "|2013-07-25|Nike Women's Lege...|  100.0|\n",
      "|2013-07-25|Team Golf Tenness...|  99.96|\n",
      "|2013-07-25|Bridgestone e6 St...|  95.97|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from ameen_daily_revenue.daily_revenue_2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - show() --preview data\n",
    "    - write.insertInto() -- to write to hive table\n",
    "    - select\n",
    "    - filter\n",
    "    - join\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|order_date|        product_name|revenue|\n",
      "+----------+--------------------+-------+\n",
      "|2013-07-25|Field & Stream Sp...|5599.72|\n",
      "|2013-07-25|Nike Men's Free 5...|5099.49|\n",
      "|2013-07-25|Diamondback Women...| 4499.7|\n",
      "|2013-07-25|Perfect Fitness P...|3359.44|\n",
      "|2013-07-25|Pelican Sunstream...|2999.85|\n",
      "|2013-07-25|O'Brien Men's Neo...|2798.88|\n",
      "|2013-07-25|Nike Men's CJ Eli...|1949.85|\n",
      "|2013-07-25|Nike Men's Dri-FI...| 1650.0|\n",
      "|2013-07-25|Under Armour Girl...|1079.73|\n",
      "|2013-07-25|Bowflex SelectTec...| 599.99|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyRevenue_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyRevenue_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyRevenue_df.write.saveAsTable(\"ameen_daily_revenue.daily_revenue_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|order_date|        product_name|revenue|\n",
      "+----------+--------------------+-------+\n",
      "|2013-07-25|Field & Stream Sp...|5599.72|\n",
      "|2013-07-25|Nike Men's Free 5...|5099.49|\n",
      "|2013-07-25|Diamondback Women...| 4499.7|\n",
      "|2013-07-25|Perfect Fitness P...|3359.44|\n",
      "|2013-07-25|Pelican Sunstream...|2999.85|\n",
      "|2013-07-25|O'Brien Men's Neo...|2798.88|\n",
      "|2013-07-25|Nike Men's CJ Eli...|1949.85|\n",
      "|2013-07-25|Nike Men's Dri-FI...| 1650.0|\n",
      "|2013-07-25|Under Armour Girl...|1079.73|\n",
      "|2013-07-25|Bowflex SelectTec...| 599.99|\n",
      "|2013-07-25|Elevation Trainin...| 319.96|\n",
      "|2013-07-25|Titleist Pro V1 H...| 207.96|\n",
      "|2013-07-25|Nike Men's Kobe I...| 199.99|\n",
      "|2013-07-25|Cleveland Golf Wo...| 119.99|\n",
      "|2013-07-25|TYR Boys' Team Di...| 119.97|\n",
      "|2013-07-25|Merrell Men's All...| 109.99|\n",
      "|2013-07-25|LIJA Women's Butt...|  108.0|\n",
      "|2013-07-25|Nike Women's Lege...|  100.0|\n",
      "|2013-07-25|Team Golf Tenness...|  99.96|\n",
      "|2013-07-25|Bridgestone e6 St...|  95.97|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from ameen_daily_revenue.daily_revenue_3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|order_date|revenue|\n",
      "+----------+-------+\n",
      "|2013-07-25|5599.72|\n",
      "|2013-07-25|5099.49|\n",
      "|2013-07-25| 4499.7|\n",
      "|2013-07-25|3359.44|\n",
      "|2013-07-25|2999.85|\n",
      "|2013-07-25|2798.88|\n",
      "|2013-07-25|1949.85|\n",
      "|2013-07-25| 1650.0|\n",
      "|2013-07-25|1079.73|\n",
      "|2013-07-25| 599.99|\n",
      "|2013-07-25| 319.96|\n",
      "|2013-07-25| 207.96|\n",
      "|2013-07-25| 199.99|\n",
      "|2013-07-25| 119.99|\n",
      "|2013-07-25| 119.97|\n",
      "|2013-07-25| 109.99|\n",
      "|2013-07-25|  108.0|\n",
      "|2013-07-25|  100.0|\n",
      "|2013-07-25|  99.96|\n",
      "|2013-07-25|  95.97|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyRevenue_df.select('order_date','revenue').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|order_date|        product_name|revenue|\n",
      "+----------+--------------------+-------+\n",
      "|2013-07-25|Field & Stream Sp...|5599.72|\n",
      "|2013-07-25|Nike Men's Free 5...|5099.49|\n",
      "|2013-07-25|Diamondback Women...| 4499.7|\n",
      "|2013-07-25|Perfect Fitness P...|3359.44|\n",
      "|2013-07-25|Pelican Sunstream...|2999.85|\n",
      "|2013-07-25|O'Brien Men's Neo...|2798.88|\n",
      "|2013-07-25|Nike Men's CJ Eli...|1949.85|\n",
      "|2013-07-25|Nike Men's Dri-FI...| 1650.0|\n",
      "|2013-07-25|Under Armour Girl...|1079.73|\n",
      "|2013-07-25|Bowflex SelectTec...| 599.99|\n",
      "|2013-07-25|Elevation Trainin...| 319.96|\n",
      "|2013-07-25|Titleist Pro V1 H...| 207.96|\n",
      "|2013-07-25|Nike Men's Kobe I...| 199.99|\n",
      "|2013-07-25|Cleveland Golf Wo...| 119.99|\n",
      "|2013-07-25|TYR Boys' Team Di...| 119.97|\n",
      "|2013-07-25|Merrell Men's All...| 109.99|\n",
      "|2013-07-25|LIJA Women's Butt...|  108.0|\n",
      "|2013-07-25|Nike Women's Lege...|  100.0|\n",
      "|2013-07-25|Team Golf Tenness...|  99.96|\n",
      "|2013-07-25|Bridgestone e6 St...|  95.97|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyRevenue_df.filter(dailyRevenue_df['order_date']=='2013-07-25').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.json()\n",
    "#df.write.save(fn,type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(builtins.object)\n",
      " |  DataFrame(jdf, sql_ctx)\n",
      " |  \n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy.agg()``).\n",
      " |      \n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      :param alias: string, an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[http://dx.doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      :param col: str, list.\n",
      " |        Can be a single column name, or a list of names for multiple columns.\n",
      " |      :param probabilities: a list of quantile probabilities\n",
      " |        Each number must belong to [0, 1].\n",
      " |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      :param relativeError:  The relative target precision to achieve\n",
      " |        (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |        could be very expensive. Note that values greater than 1 are\n",
      " |        accepted but give the same result as 1.\n",
      " |      :return:  the approximate quantiles at the given probabilities. If\n",
      " |        the input `col` is a string, the output is a list of floats. If the\n",
      " |        input `col` is a list or tuple of strings, the output is also a\n",
      " |        list, but each element in it is a list of floats, i.e., the output\n",
      " |        is a list of list of floats.\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added support for multiple columns.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (C{MEMORY_AND_DISK}).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this DataFrame, which is especially useful in iterative algorithms where the\n",
      " |      plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with L{SparkContext.setCheckpointDir()}.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      :param numPartitions: int, to specify the target number of partitions\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      :param colName: string, column name specified as a regex.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      :param method: The correlation method. Currently only supports \"pearson\"\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this DataFrame.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      :param other: Right side of the cartesian product.\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      :param col2: The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      :param cols: a string name of the column to drop, or a\n",
      " |          :class:`Column` to drop, or a list of string name of the columns to drop.\n",
      " |      \n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      :param how: 'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      :param thresh: int, default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |      \n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  explain(self, extended=False)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      :param value: int, long, float, string, bool or dict.\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, long, float, boolean, or string.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      :param condition: a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      :param cols: list of columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      :param n: int, default 1. Number of rows to return.\n",
      " |      :return: If n is greater than 1, return a list of :class:`Row`.\n",
      " |          If n is 1, return a single Row.\n",
      " |      \n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      :param name: A name of the hint.\n",
      " |      :param parameters: Optional parameters.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this dataframe and other\n",
      " |      dataframe while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL.\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      :param other: Right side of the join\n",
      " |      :param on: a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``full_outer``, ``left``, ``left_outer``, ``right``, ``right_outer``,\n",
      " |          ``left_semi``, and ``left_anti``.\n",
      " |      \n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()\n",
      " |      [Row(name=None, height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this DataFrame, which is especially useful in iterative\n",
      " |      algorithms where the plan may grow exponentially. Local checkpoints are stored in the\n",
      " |      executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this :class:`DataFrame` immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, False, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      :param weights: list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      :param seed: The seed for sampling.\n",
      " |      \n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      1\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      3\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      .. versionchanged:: 1.6\n",
      " |         Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |         optional if partitioning columns are specified.\n",
      " |      \n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      :param to_replace: bool, int, long, float, string, list or dict.\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      :param value: bool, int, long, float, string, list or None.\n",
      " |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      :param withReplacement: Sample with replacement or not (default ``False``).\n",
      " |      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      :param seed: Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      4\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      4\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      :param col: column that defines strata\n",
      " |      :param fractions:\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      :param seed: random seed\n",
      " |      :return: a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    5|\n",
      " |      |  1|    9|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: list of column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      :param n: Number of rows to show.\n",
      " |      :param truncate: If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      :param vertical: If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default ``True``).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting\n",
      " |          :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See also describe for basic statistics.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      :param cols: list of new column names (string)\n",
      " |      \n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toLocalIterator(self)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting Pandas's :class:`DataFrame` is\n",
      " |          expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n",
      " |      \n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use :func:`union` instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. note:: `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      :param colName: string, name of the new column.\n",
      " |      :param col: a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      :param existing: string, name of the existing column to rename.\n",
      " |      :param new: string, new name of the column.\n",
      " |      \n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      :param eventTime: the name of the column that contains the event time of the row.\n",
      " |      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns true if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      :return: :class:`DataFrameWriter`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamWriter`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dailyRevenue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"order_date\":\"2013-07-25\",\"product_name\":\"Field & Stream Sportsman 16 Gun Fire Safe\",\"revenue\":5599.72}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Nike Men\\'s Free 5.0+ Running Shoe\",\"revenue\":5099.49}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Diamondback Women\\'s Serene Classic Comfort Bi\",\"revenue\":4499.7}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Perfect Fitness Perfect Rip Deck\",\"revenue\":3359.44}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Pelican Sunstream 100 Kayak\",\"revenue\":2999.85}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"O\\'Brien Men\\'s Neoprene Life Vest\",\"revenue\":2798.88}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Nike Men\\'s CJ Elite 2 TD Football Cleat\",\"revenue\":1949.85}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Nike Men\\'s Dri-FIT Victory Golf Polo\",\"revenue\":1650.0}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Under Armour Girls\\' Toddler Spine Surge Runni\",\"revenue\":1079.73}',\n",
       " '{\"order_date\":\"2013-07-25\",\"product_name\":\"Bowflex SelectTech 1090 Dumbbells\",\"revenue\":599.99}']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailyRevenue_df.toJSON().take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
